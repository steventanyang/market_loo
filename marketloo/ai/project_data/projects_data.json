[
  {
    "title": "LureLore",
    "link": "https://devpost.com/software/lurelore",
    "text": "About the Project TLDR: Caught a fish? Take a snap. Our AI-powered app identifies the catch, keeps track of stats, and puts that fish in your 3d, virtual, interactive aquarium! Simply click on any fish in your aquarium, and all its details — its rarity, location, and more — appear, bringing your fishing memories back to life. Also, depending on the fish you catch, reel in achievements, such as your first fish caught (ever!), or your first 20 incher. The cherry on top? All users’ catches are displayed on an interactive map (built with Leaflet), where you can discover new fishing spots, or plan to get your next big catch :) Inspiration Our journey began with a simple observation: while fishing creates lasting memories, capturing those moments often falls short. We realized that a picture might be worth a thousand words, but a well-told fish tale is priceless. This spark ignited our mission to blend the age-old art of fishing with cutting-edge AI technology. What We Learned Diving into this project was like casting into uncharted waters – exhilarating and full of surprises. We expanded our skills in: But beyond the technical skills, we learned the art of transforming a simple idea into a full-fledged application that brings joy and preserves memories. How We Built It Our development process was as meticulously planned as a fishing expedition: Challenges We Faced Like any fishing trip, we encountered our fair share of challenges: Despite these challenges, or perhaps because of them, our team grew stronger and more resourceful. Each obstacle overcome was like landing a prized catch, making the final product all the more rewarding. As we cast our project out into the world, we're excited to see how it will evolve and grow, much like the tales of fishing adventures it's designed to capture.",
    "matched_prize": "None"
  },
  {
    "title": "PathSense",
    "link": "https://devpost.com/software/pathsense-athy2r",
    "text": "Inspiration Our journey with PathSense began with a deeply personal connection. Several of us have visually impaired family members, and we've witnessed firsthand the challenges they face navigating indoor spaces. We realized that while outdoor navigation has seen remarkable advancements, indoor environments remained a complex puzzle for the visually impaired. This gap in assistive technology sparked our imagination. We saw an opportunity to harness the power of AI, computer vision, and indoor mapping to create a solution that could profoundly impact lives. We envisioned a tool that would act as a constant companion, providing real-time guidance and environmental awareness in complex indoor settings, ultimately enhancing independence and mobility for visually impaired individuals. What it does PathSense, our voice-centric indoor navigation assistant, is designed to be a game-changer for visually impaired individuals. At its heart, our system aims to enhance mobility and independence by providing accessible, spoken navigation guidance in indoor spaces. Our solution offers the following key features: What sets PathSense apart is its adaptive nature. Our system continuously updates its guidance based on the user's movement and any changes in the environment, ensuring real-time accuracy. This dynamic approach allows for a more natural and responsive navigation experience, adapting to the user's pace and preferences as they move through complex indoor spaces. How we built it In building PathSense, we embraced the challenge of integrating multiple cutting-edge technologies. Our solution is built on the following technological framework: Voice Interaction: Voiceflow Computer Vision Pipeline: Data Management: Convex database Semantic Search: Cohere's Rerank API Indoor Mapping: MappedIn SDK Speech Processing: Video Input: Multiple TAPO cameras To tie it all together, we leveraged Cohere's Rerank API for semantic search, allowing us to find the most relevant information based on user queries. For speech processing, we chose a Groq model based on OpenAI's Whisper for transcription, and Unreal Engine for speech synthesis, prioritizing low latency for real-time interaction. The result is a seamless, responsive system that processes visual information, understands user requests, and provides spoken guidance in real-time. Challenges we ran into Our journey in developing PathSense was not without its hurdles. One of our biggest challenges was integrating the various complex components of our system. Combining the computer vision pipeline, Voiceflow agent, and MappedIn SDK into a cohesive, real-time system required careful planning and countless hours of debugging. We often found ourselves navigating uncharted territory, pushing the boundaries of what these technologies could do when working in concert. Another significant challenge was balancing the diverse skills and experience levels within our team. While our diversity brought valuable perspectives, it also required us to be intentional about task allocation and communication. We had to step out of our comfort zones, often learning new technologies on the fly. This steep learning curve, coupled with the pressure of working on parallel streams while ensuring all components meshed seamlessly, tested our problem-solving skills and teamwork to the limit. Accomplishments that we're proud of Looking back at our journey, we're filled with a sense of pride and accomplishment. Perhaps our greatest achievement is creating an application with genuine, life-changing potential. Knowing that PathSense could significantly improve the lives of visually impaired individuals, including our own family members, gives our work profound meaning. We're also incredibly proud of the technical feat we've accomplished. Successfully integrating numerous complex technologies - from AI and computer vision to voice processing - into a functional system within a short timeframe was no small task. Our ability to move from concept to a working prototype that demonstrates the real-world potential of AI-driven indoor navigation assistance is a testament to our team's creativity, technical skill, and determination. What we learned Our work on PathSense has been an incredible learning experience. We've gained invaluable insights into the power of interdisciplinary collaboration, seeing firsthand how diverse skills and perspectives can come together to tackle complex problems. The process taught us the importance of rapid prototyping and iterative development, especially in a high-pressure environment like a hackathon. Perhaps most importantly, we've learned the critical importance of user-centric design in developing assistive technology. Keeping the needs and experiences of visually impaired individuals at the forefront of our design and development process not only guided our technical decisions but also gave us a deeper appreciation for the impact technology can have on people's lives. What's next for PathSense As we look to the future of PathSense, we're brimming with ideas for enhancements and expansions. We're eager to partner with more venues to increase our coverage of mapped indoor spaces, making PathSense useful in a wider range of locations. We also plan to refine our object recognition capabilities, implement personalized user profiles, and explore integration with wearable devices for an even more seamless experience. In the long term, we envision PathSense evolving into a comprehensive indoor navigation ecosystem. This includes developing community features for crowd-sourced updates, integrating augmented reality capabilities to assist sighted companions, and collaborating with smart building systems for ultra-precise indoor positioning. With each step forward, our goal remains constant: to continually improve PathSense's ability to provide independence and confidence to visually impaired individuals navigating indoor spaces.",
    "matched_prize": "'MappedIn: Lead the Way in Indoor Mapping: Transform Spaces with Mappedin SDK'"
  },
  {
    "title": "ChatGPMe",
    "link": "https://devpost.com/software/chatgpme",
    "text": "AI, AI, AI... The number of projects using LLMs has skyrocketed with the wave of artificial intelligence. But what if you were the AI, tasked with fulfilling countless orders and managing requests in real time? Welcome to chatgpME, a fast-paced, chaotic game where you step into the role of an AI who has to juggle multiple requests, analyzing input, and delivering perfect responses under pressure! Inspired by games like Overcooked... chatgpME challenges you to process human queries as quickly and accurately as possible. Each round brings a flood of requests—ranging from simple math questions to complex emotional support queries—and it's your job to fulfill them quickly with high-quality responses! How to Play Take Orders: Players receive a constant stream of requests, represented by different \"orders\" from human users. The orders vary in complexity—from basic facts and math solutions to creative writing and emotional advice. Process Responses: Quickly scan each order, analyze the request, and deliver a response before the timer runs out. Get analyzed - our built-in AI checks how similar your answer is to what a real AI would say :) Key Features Fast-Paced Gameplay: Just like Overcooked, players need to juggle multiple tasks at once. Keep those responses flowing and maintain accuracy, or you’ll quickly find yourself overwhelmed. Orders with a Twist: The more aware the AI becomes, the more unpredictable it gets. Some responses might start including strange, existential musings—or it might start asking you questions in the middle of a task! How We Built It Concept & Design: We started by imagining a game where the player experiences life as ChatGPT, but with all the real-time pressure of a time management game like Overcooked. Designs were created in Procreate and our handy notebooks. Tech Stack: Using Unity, we integrated a system where mock requests are sent to the player, each with specific requirements and difficulty levels. A template was generated using defang, and we also used it to sanitize user inputs. Answers are then evaluated using the fantastic Cohere API! Playtesting: Through multiple playtests, we refined the speed and unpredictability of the game to keep players engaged and on their toes.",
    "matched_prize": "Ubisoft: Ubisoft Game Dev Challenge"
  },
  {
    "title": "ConArt AI",
    "link": "https://devpost.com/software/conart-ai",
    "text": "Inspiration We often found ourselves stuck at the start of the design process, not knowing where to begin or how to turn our ideas into something real. In large organisations these issues are not only inconvenient and costly, but also slow down development. That is why we created ConArt AI to make it easier. It helps teams get their ideas out quickly and turn them into something real without all the confusion. What it does ConArt AI is a gamified design application that helps artists and teams brainstorm ideas faster in the early stages of a project. Teams come together in a shared space where each person has to create a quick sketch and provide a prompt before the timer runs out. The sketches are then turned into images and everyone votes on their team's design where points are given from 1 to 5. This process encourages fast and fun brainstorming while helping teams quickly move from ideas to concepts. It makes collaboration more engaging and helps speed up the creative process. How we built it We built ConArt AI using React for the frontend to create a smooth and responsive interface that allows for real-time collaboration. On the backend, we used Convex to handle game logic and state management, ensuring seamless communication between players during the sketching, voting, and scoring phases. For the image generation, we integrated the Replicate API, which utilises AI models like ControlNet with Stable Diffusion to transform the sketches and prompts into full-fledged concept images. These API calls are managed through Convex actions, allowing for real-time updates and feedback loops. The entire project is hosted on Vercel, which is officially supported by Convex, ensuring fast deployment and scaling. Convex especially enabled us to have a serverless experience which allowed us to not worry about extra infrastructure and focus more on the functions of our app. The combination of these technologies allows ConArt AI to deliver a gamified, collaborative experience. Challenges we ran into We faced several challenges while building ConArt AI. One of the key issues was with routing in production, where we had to troubleshoot differences between development and live environments. We also encountered challenges in managing server vs. client-side actions, particularly ensuring smooth, real-time updates. Additionally, we had some difficulties with responsive design, ensuring the app looked and worked well across different devices and screen sizes. These challenges pushed us to refine our approach and improve the overall performance of the application. Accomplishments that we're proud of We’re incredibly proud of several key accomplishments from this hackathon. Nikhil: Learned how to use a new service like Convex during the hackathon, adapting quickly to integrate it into our project. Ben: Instead of just showcasing a local demo, he managed to finish and fully deploy the project by the end of the hackathon, which is a huge achievement. Shireen: Completed the UI/UX design of a website in under 36 hours for the first time, while also planning our pitch and brand identity, all during her first hackathon. Ryushen: He worked on building React components and the frontend, ensuring the UI/UX looked pretty, while also helping to craft an awesome pitch. Overall, we’re most proud of how well we worked as a team. Every person filled their role and brought the project to completion, and we’re happy to have made new friends along the way! What we learned We learned how to effectively use Convex by studying its documentation, which helped us manage real-time state and game logic for features like live sketching, voting, and scoring. We also learned how to trigger external API calls, like image generation with Replicate, through Convex actions, making the integration of AI seamless. On top of that, we improved our collaboration as a team, dividing tasks efficiently and troubleshooting together, which was key to building ConArt AI successfully. What's next for ConArt AI We plan to incorporate user profiles in order to let users personalise their experience and track their creative contributions over time. We will also be adding a feature to save concept art, allowing teams to store and revisit their designs for future reference or iteration. These updates will enhance collaboration and creativity, making ConArt AI even more valuable for artists and teams working on long-term projects.",
    "matched_prize": "Convex: Best App and Startup Idea Built on Convex"
  },
  {
    "title": "Flashback",
    "link": "https://devpost.com/software/flashback-q3kzg9",
    "text": "Inspiration Life is short and fast-paced, filled with fleeting moments of joy, achievements, and connection. During Hack the North, our team met so many amazing and inspiring people in such a short amount of time. It struck us how easy it is to forget these meaningful interactions and experiences as life rushes on. This inspired us to create Flashback—a VR experience designed to memorialize these cherished moments and allow users to revisit them in a deeply immersive and personalized museum. Unlike traditional social media, which encourages constant sharing with others, Flashback offers a personal and introspective journey through your own memories. It’s designed for the individual, allowing users to relive their most cherished moments in an immersive, meaningful way. Life is not measured by time. It is measured by moments. There is a limit to how much you can embrace a moment. But there is no limit to how much you can appreciate it. Bonus: Great for us forgetful folks! What it does Flashback is a VR experience that transforms your personal memories and achievements into interactive, immersive museum exhibits. Users can upload photos, videos, personal audio clips, and music to create unique 3D galleries, where each memory comes to life. As you walk up to an exhibit, specific music, audio clips, and captions are triggered, bringing the memory to life in a dynamic way. Memories can also be grouped into collections. Instead of just scrolling through pictures, Flashback lets you step into your memories—hear familiar voices, see cherished moments, and relive experiences in a fully immersive environment. Additionally, there is a web app where users can upload, update, and maintain their growing museum of memories. Flashback evolves with you over time, offering a place to revisit positive memories on difficult days, and preserve fleeting moments like our time at Hack the North. How we built it Flashback is built with React, Node.js, JavaScript, Express.js, Convex, HTML, CSS, Spotify APIand Material UI for the web app's front and back end. The VR experience is developed using Unity, .NET, and C#, with testing done on a Meta Quest VR headset. Our mascot Framey was drawn up by our team mate Jenn. Challenges we ran into Accomplishments that we're proud of What we learned Everyone on our team had experience in different technologies, but this weekend we each tried doing something new- using Convex DB integration for the first time, designing for the first time, learning C# and Unity, trying out front-end development, and creating our first VR project! Additionally, we learned that unity is hard and doing research and regularly communication about feasibility is extremely important. What's next for Flashback",
    "matched_prize": "None"
  },
  {
    "title": "TinkerFlow",
    "link": "https://devpost.com/software/tinkerflow",
    "text": "Inspiration We, as passionate tinkerers, understand the struggles that come with making a project come to life (especially for begineers). 80% of U.S. workers agree that learning new skills is important, but only 56% are actually learning something new. From not knowing how electrical components should be wired, to not knowing what a particular component does, and what is the correct procedure to effectively assemble a creation, TinkerFlow is here to help you ease this process, all in one interface. What it does -> Image identification/classification or text input of available electronic components -> Powered by Cohere and Groq LLM, generates wiring scheme and detailed instructions (with personality!) to complete an interesting project that is possible with electronics available -> Using React Flow, we developed our own library (as other existing softwares were depreciated) that generates electrical schematics to make the fine, precise and potentially tedious work of wiring projects easier. -> Display generated text of instructions to complete project How we built it We allowed the user to upload a photo, have it get sent to the backend (handled by Flask), used Python and Google Vision AI to do image classification and identify with 80% accuracy the component. To provide our users with a high quality and creative response, we used a central LLM to find projects that could be created based on inputted components, and from there generate instructions, schematics, and codes for the user to use to create their project. For this central LLM, we offer two options: Cohere and Groq. Our default model is the Cohere LLM, which using its integrated RAG and preamble capability offers superior accuracy and a custom personality for our responses, providing more fun and engagement for the user. Our second option Groq though providing a lesser quality of a response, provides fast process times, a short coming of Cohere. Both of these LLM's are based on large meticulously defined prompts (characterizing from the output structure to the method of listing wires), which produce the results that are necessary in generating the final results seen by the user. In order to provide the user with different forms of information, we decide to present electrical schematics on the webpage. However during the development due to many circumstances, our group had to use simple JavaScript libraries to create its functionality. Challenges we ran into Accomplishments that we're proud of Create electrical schematics using basic js library. Create consistent outputting LLM's for multiple fields. What we learned Ability to overcome troubles - consistently innovating for solutions, even if there may not have been an easy route (ex. existing library) to use - our schematic diagrams were custom made! What's next for TinkerFlow Aiming for faster LLM processing speed. Update the user interface of the website, especially for the electrical schematic graph generation. Implement the export of code files, to allow for even more information being provided to the user for their project.",
    "matched_prize": "Cohere: Best Use of Cohere"
  },
  {
    "title": "AutOST",
    "link": "https://devpost.com/software/autost",
    "text": "Inspiration Our idea was inspired by our group's shared interest in musical composition, as well as our interests in AI models and their capabilites. The concept that inspired our project was: \"What if life had a soundtrack?\" What it does AutOST generates and produces a constant stream of original live music designed to automatically adjust to and accompany any real-life scenario. How we built it We built our project in python, using the Mido library to send note signals directly to FL studio, allowing us to play constant audio without a need to export to a file. The whole program is linked up to a live video feed that uses Groq AI's computer vision api to determine the mood of an image and adjust the audio accordingly. Challenges we ran into The main challenge we faced in this project is the struggle that came with making the generated music not only sound coherent and good, but also have the capability to adjust according to parameters. Turns out that generating music mathematically is more difficult than it seems. Accomplishments that we're proud of We're proud of the fact that our program's music sounds somewhat decent, and also that we were able to brainstorm a concept that (to our knowlege) has not really seen much experimentation. What we learned We learned that music generation is much harder than we initially thought, and that AIs aren't all that great at understanding human emotions. What's next for AutOST If we continue work on this project post-hackathon, the next steps would be to expand its capabilities for recieving input, allowing it to do all sorts of amazing things such as creating a dynamic soundtrack for video games, or integrating with smart headphones to create tailored background music that would allow users to feel as though they are living inside a movie.",
    "matched_prize": "None"
  },
  {
    "title": "Craven",
    "link": "https://devpost.com/software/craven",
    "text": "Inspiration 🍪 We’re fed up with our roommates stealing food from our designated kitchen cupboards. Few things are as soul-crushing as coming home after a long day and finding that someone has eaten the last Oreo cookie you had been saving. Suffice it to say, the university student population is in desperate need of an inexpensive, lightweight security solution to keep intruders out of our snacks... Introducing Craven, an innovative end-to-end pipeline to put your roommates in check and keep your snacks in stock. What it does 📸 Craven is centered around a small Nest security camera placed at the back of your snack cupboard. Whenever the cupboard is opened by someone, the camera snaps a photo of them and sends it to our server, where a facial recognition algorithm determines if the cupboard has been opened by its rightful owner or by an intruder. In the latter case, the owner will instantly receive an SMS informing them of the situation, and then our 'security guard' LLM will decide on the appropriate punishment for the perpetrator, based on their snack-theft history. First-time burglars may receive a simple SMS warning, but repeat offenders will have a photo of their heist, embellished with an AI-generated caption, posted on our X account for all to see. How we built it 🛠️ Challenges we ran into 🚩 In order to have unfettered access to the Nest camera's feed, we had to find a way to bypass Google's security protocol. We achieved this by running an HTTP proxy to imitate the credentials of an iOS device, allowing us to fetch snapshots from the camera at any time. Fine-tuning our facial recognition model also turned out to be a bit of a challenge. In order to ensure accuracy, it was important that we had a comprehensive set of training images for each roommate, and that the model was tested thoroughly. After many iterations, we settled on a K-nearest neighbours algorithm for classifying faces, which performed well both during the day and with night vision. Additionally, integrating the X API to automate the public shaming process required specific prompt engineering to create captions that were both humorous and effective in discouraging repeat offenders. Accomplishments that we're proud of 💪 What we learned 🧠 Over the course of this hackathon, we gained valuable insights into how to circumvent API protocols to access hardware data streams (for a good cause, of course). We also deepened our understanding of facial recognition technology and learned how to tune computer vision models for improved accuracy. For our X integration, we learned how to engineer prompts for Cohere's API to ensure that the AI-generated captions were both humorous and contextual. Finally, we gained experience integrating multiple APIs (Nest, Twilio, X) into a cohesive, real-time application. What's next for Craven 🔮",
    "matched_prize": "None"
  },
  {
    "title": "Ludwig",
    "link": "https://devpost.com/software/ludwig",
    "text": "💡 Inspiration 💡 Have you ever wished you could play the piano perfectly? Well, instead of playing yourself, why not get Ludwig to play it for you? Regardless of your ability to read sheet music, just upload it to Ludwig and he'll scan, analyze, and play the entire sheet music within the span of a few seconds! Sometimes, you just want someone to play the piano for you, so we aimed to make a robot that could be your little personal piano player! This project allows us to bring music to places like elderly homes, where live performances can uplift residents who may not have frequent access to musicians. We were excited to combine computer vision, MIDI parsing, and robotics to create something tangible that shows how technology can open new doors. Ultimately, our project makes music more inclusive and brings people together through shared experiences. ❓What it does ❓ Ludwig is your music prodigy. Ludwig can read any sheet music that you upload to him, then convert it to a MIDI file, convert that to playable notes on the piano scale, then play each of those notes on the piano with its fingers! You can upload any kind of sheet music and see the music come to life! ⚙️ How we built it ⚙️ For this project, we leveraged OpenCV for computer vision to read the sheet music. The sheet reading goes through a process of image filtering, converting it to binary, classifying the characters, identifying the notes, then exporting them as a MIDI file. We then have a server running for transferring the file over to Ludwig's brain via SSH. Using the Raspberry Pi, we leveraged multiple servo motors with a servo module to simultaneously move multiple fingers for Ludwig. In the Raspberry Pi, we developed functions, key mappers, and note mapping systems that allow Ludwig to play the piano effectively. Challenges we ran into ⚔️ We had a few roadbumps along the way. Some major ones included file transferring over SSH as well as just making fingers strong enough on the piano that could withstand the torque. It was also fairly difficult trying to figure out the OpenCV for reading the sheet music. We had a model that was fairly slow in reading and converting the music notes. However, we were able to learn from the mentors in Hack The North and learn how to speed it up to make it more efficient. We also wanted to Accomplishments that we're proud of 🏆 What we learned 📚 What's next for Ludwig 🤔",
    "matched_prize": "None"
  },
  {
    "title": "Ted",
    "link": "https://devpost.com/software/ted-twz9b1",
    "text": "Inspiration Our biggest inspiration came from our grandparents, who often felt lonely and struggled to find help. Specifically, one of us have a grandpa with dementia. He lives alone and finds it hard to receive help since most of his relatives live far away and he has reduced motor skills. Knowing this, we were determined to create a product -- and a friend -- that would be able to help the elderly with their health while also being fun to be around! Ted makes this dream a reality, transforming lives and promoting better welfare. What it does Ted is able to... How we built it Challenges we ran into Some challenges we ran into during development was making sure every part was secure. With limited materials, we found that materials would often shift or move out of place after a few test runs, which was frustrating to keep fixing. However, instead of trying the same techniques again, we persevered by trying new methods of appliances, which eventually led a successful solution! Having 2 speech-to-text models open at the same time showed some issue (and I still didn't fix it yet...). Creating reactive movements was difficult too but achieved it through the use of keywords and a long list of preset moves. Accomplishments that we're proud of What we learned What's next for Ted",
    "matched_prize": "None"
  },
  {
    "title": "Dime Defender",
    "link": "https://devpost.com/software/budget-defender",
    "text": "Live Demo Link: https://www.youtube.com/live/I5dP9mbnx4M?si=ESRjp7SjMIVj9ACF&t=5959 Inspiration We all fall victim to impulse buying and online shopping sprees... especially in the first few weeks of university. A simple budgeting tool or promising ourselves to spend less just doesn't work anymore. Sometimes we need someone, or someone's, to physically stop us from clicking the BUY NOW button and talk us through our purchase based on our budget and previous spending. By drawing on the courtroom drama of legal battles, we infuse an element of fun and accountability into doing just this. What it does Dime Defender is a Chrome extension built to help you control your online spending to your needs. Whenever the extension detects that you are on a Shopify or Amazon checkout page, it will lock the BUY NOW button and take you to court! You'll be interrupted by two lawyers, the defence attorney explaining why you should steer away from the purchase 😒 and a prosecutor explains why there still are some benefits 😏. By giving you a detailed analysis of whether you should actually buy based on your budget and previous spendings in the month, Dime Defender allows you to make informed decisions by making you consider both sides before a purchase. The lawyers are powered by VoiceFlow using their dialog manager API as well as Chat-GPT. They have live information regarding the descriptions and prices of the items in your cart, as well as your monthly budget, which can be easily set in the extension. Instead of just saying no, we believe the detailed discussion will allow users to reflect and make genuine changes to their spending patterns while reducing impulse buys. How we built it We created the Dime Defender Chrome extension and frontend using Svelte, Plasma, and Node.js for an interactive and attractive user interface. The Chrome extension then makes calls using AWS API gateways, connecting the extension to AWS lambda serverless functions that process queries out, create outputs, and make secure and protected API calls to both VoiceFlow to source the conversational data and ElevenLabs to get our custom text-to-speech voice recordings. By using a low latency pipeline, with also AWS RDS/EC2 for storage, all our data is quickly captured back to our frontend and displayed to the user through a wonderful interface whenever they attempt to check out on any Shopify or Amazon page. Challenges we ran into Using chrome extensions poses the challenge of making calls to serverless functions effectively and making secure API calls using secret api_keys. We had to plan a system of lambda functions, API gateways, and code built into VoiceFlow to create a smooth and low latency system to allow the Chrome extension to make the correct API calls without compromising our api_keys. Additionally, making our VoiceFlow AIs arguing with each other with proper tone was very difficult. Through extensive prompt engineering and thinking, we finally reached a point with an effective and enjoyable user experience. We also faced lots of issues with debugging animation sprites and text-to-speech voiceovers, with audio overlapping and high latency API calls. However, we were able to fix all these problems and present a well-polished final product. Accomplishments that we're proud of Something that we are very proud of is our natural conversation flow within the extension as well as the different lawyers having unique personalities which are quite evident after using our extension. Having your cart cross-examined by 2 AI lawyers is something we believe to be extremely unique, and we hope that users will appreciate it. What we learned We had to create an architecture for our distributed system and learned about connection of various technologies to reap the benefits of each one while using them to cover weaknesses caused by other technologies. Also..... Don't eat the 6.8 million Scoville hot sauce if you want to code. What's next for Dime Defender The next thing we want to add to Dime Defender is the ability to work on even more e-commerce and retail sites and go beyond just Shopify and Amazon. We believe that Dime Defender can make a genuine impact helping people curb excessive online shopping tendencies and help people budget better overall.",
    "matched_prize": "Voiceflow: Best Non-Chatbot AI Agent Built on Voiceflow"
  },
  {
    "title": "PulseGrip - 2024 Finalist",
    "link": "https://devpost.com/software/pulsegrip",
    "text": "Inspiration Members of our team know multiple people who suffer from permanent or partial paralysis. We wanted to build something that could be fun to develop and use, but at the same time make a real impact in people's everyday lives. We also wanted to make an affordable solution, as most solutions to paralysis cost thousands and are inaccessible. We wanted something that was modular, that we could 3rd print and also make open source for others to use. What it does and how we built it The main component is a bionic hand assistant called The PulseGrip. We used an ECG sensor in order to detect electrical signals. When it detects your muscles are trying to close your hand it uses a servo motor in order to close your hand around an object (a foam baseball for example). If it stops detecting a signal (you're no longer trying to close) it will loosen your hand back to a natural resting position. Along with this at all times it sends a signal through websockets to our Amazon EC2 server and game. This is stored on a MongoDB database, using API requests we can communicate between our games, server and PostGrip. We can track live motor speed, angles, and if it's open or closed. Our website is a full-stack application (react styled with tailwind on the front end, node js on the backend). Our website also has games that communicate with the device to test the project and provide entertainment. We have one to test for continuous holding and another for rapid inputs, this could be used in recovery as well. Challenges we ran into This project forced us to consider different avenues and work through difficulties. Our main problem was when we fried our EMG sensor, twice! This was a major setback since an EMG sensor was going to be the main detector for the project. We tried calling around the whole city but could not find a new one. We decided to switch paths and use an ECG sensor instead, this is designed for heartbeats but we managed to make it work. This involved wiring our project completely differently and using a very different algorithm. When we thought we were free, our websocket didn't work. We troubleshooted for an hour looking at the Wifi, the device itself and more. Without this, we couldn't send data from the PulseGrip to our server and games. We decided to ask for some mentor's help and reset the device completely, after using different libraries we managed to make it work. These experiences taught us to keep pushing even when we thought we were done, and taught us different ways to think about the same problem. Accomplishments that we're proud of Firstly, just getting the device working was a huge achievement, as we had so many setbacks and times we thought the event was over for us. But we managed to keep going and got to the end, even if it wasn't exactly what we planned or expected. We are also proud of the breadth and depth of our project, we have a physical side with 3rd printed materials, sensors and complicated algorithms. But we also have a game side, with 2 (questionable original) games that can be used. But they are not just random games, but ones that test the user in 2 different ways that are critical to using the device. Short burst and hold term holding of objects. Lastly, we have a full-stack application that users can use to access the games and see live stats on the device. What's next for PulseGrip We think this project had a ton of potential and we can't wait to see what we can do with the ideas learned here. Check it out https://hacks.pulsegrip.design https://github.com/PulseGrip",
    "matched_prize": "None"
  },
  {
    "title": "3D Print a Picture",
    "link": "https://devpost.com/software/3d-print-a-picture",
    "text": "What it does Take a picture, get a 3D print of it! Challenges we ran into The 3D printers going poof on the prints. How we built it",
    "matched_prize": "None"
  },
  {
    "title": "NoteHacks",
    "link": "https://devpost.com/software/notehacks-bsjdoi",
    "text": "Inspiration Ever found yourself struggling to keep up during a lecture, caught between listening to the professor while scrambling to scribble down notes? It’s all too common to miss key points while juggling the demands of note-taking – that’s why we made a tool designed to do the hard work for you! What it does With a simple click, you can start recording of your lecture, and NoteHacks will start generating clear, summarized notes in real time. The summary conciseness parameter can be fine tuned depending on how you want your notes written, and will take note of when it looks like you've been distracted so that you can have all those details you would have missed. These notes are stored for future review, where you can directly ask AI about the content without having to provide background details. How we built it Challenges we ran into Accomplishments that we're proud of What we learned What's next for NoteHacks",
    "matched_prize": "None"
  },
  {
    "title": "PERCEIV/IO",
    "link": "https://devpost.com/software/perceiv-io",
    "text": "💡 INSPIRATION We first thought of building something for the deaf & blind because one of our team member's close family relations has slowly been acquiring age-related dual vision and hearing loss. Deafblindness has been traditionally overlooked. The combination of sensory losses means existing solutions, such as sign language for the deaf or speech transcription for the blind, cannot be used. As a result, over 160 million Deafblind people around the world are severely limited in their ability to communicate. Current solutions require uncomfortable physical contact or the presence of a 24/7 guide-communicator, preventing independence. These barriers to being able to freely explore and interact with your world can lead to you feeling isolated and alone. 🎯 OUR GOAL Our goal was to build a communication device that can help Deafblind persons understand and communicate with the world without invasive physical contact, a requirement for a volunteer communicator, or an inaccessible/impractical design. 🔭 WHAT IS PERCEIV/IO? PERCEIV/IO is a novel hardware device powered by the latest AI advancements to help the Deafblind. We designed PERCEIV/IO to have two main use-cases. First, used a camera to take periodic captures of the world in front of the user and use AI image-to-text technologies to translate it into braille, which is then displayed on a tactile hand-held device that we have fabricated. This can help a Deafblind person understand their surroundings, which is currently very limited in scope. Second, we designed a speech-to-braille system to help allow for communication with the Deafblind. We also incorporated sentiment analysis to effectively convey the tones of standard English phrases to make communication more robust. The specific details can be found in the next section. We hope some of the ideas generated during this hackathon turn are able to be used in organizations such as CNIB Deafblind Community Services in order to help them with their goal of assisting people who need it most. 📣 FEATURES 🛠️ HOW WE BUILT IT Hardware We designed our hardware using four stepper motors and their drivers, 3D printed braille discs, 3D printed stepper motor housings, a Raspberry Pi 3B, and custom 3D printed housings. Each stepper motor controls an octagonal braille disc. Placing two of these side-by-side creates a full braille cell, with different stepper motor rotations producing different braille characters. We designed the Raspberry Pi code on Python and developed our own algorithim for converting text directly into stepper motor coordinates (position to display the correct braille character). Software We used image recognition AI based on Groq using their Llava v1.5 image-to-text endpoint, and that text is then put through Groq's Llama 3.1 70b endpoint to condense it down into just two or three words. We used Google Speech-to-Text and the Facebook RoBERTa base model for simple sentiment analysis with a Genesys-based sentiment analysis feature currently being deployed. We developed our own code to convert into Grade II Unified English Braille Code. On the backend, our database is based on Convex along with our accompanying app. We also built a prototype, web-hosted app on the Streamlit platform integrated with hugging face ML models loaded through the transformers pipeline. We facilitated wireless communication between the apps and the hardware through Github by running functions to update a file and check for updates on a file. We are also working on a Genesys based AI chat bot, the responses of which would be translated into braille and communicated to the user. 🌎 ENVIRONMENTALLY CONSCIOUS As users of AI during this boom of innovation over the last couple years, we strive to be as environmentally conscious as possible and be aware of the power we are consuming whenever we prompt an LLM. Over the course of a large model's first one million messages, it has an estimated power cost of 55.1Mwh. That's enough power to power roughly 120 American homes for a year! Thankfully, with Groq, not only do we have access to lightning-fast response speeds, but due to their LPUs, SRAM, and sequential processing we are using less power than if we were to use the traditional NVIDIA counterpart. Conserving energy is very important to us, and we are thankful for Groq to providing a solution for us. 🧗‍♀️ CHALLENGES WE'VE OVERCOME Hackathons are about nothing if not overcoming challenges. Whenever a piece of hardware broke, software wouldn't cooperate, or motors wouldn't connect, we would work as a team to solve the issue and push on through. It ended up being a very rewarding experience for all of us. One major challenge that's still fresh in my mind is our issues with the Raspberry Pi. We ended up going through three Pi's before we realized that none of their SD cards had any OS on them! Eventually, with the help of the amazing mentors here at Hack The North (thank you again Moody!), we we're able to get the SD card flashed and were able to start uploading our code. Thankfully, due to our adoption of SOLID principles and our policy of encapsulation, we were able to implement the code we has worked on in the meantime with ease. Our hardware issues didn't end there! After careful testing of almost every different software & hardware component to determine why our code was not working, we found that 2 stepper motor drivers were faulty. With the help of mentors, however, we got this resolved and we learned the painstakingly careful process of hardware verification! Another problem was more to do with logic- turning an image into text is easy. Turning that text into braille, and then turning that braille into signals from a Raspberry Pi into 4 proprietary stepper motors that control half of a braille character each is whole other. Luckily with some wisdom from our team members with past experience with these kinds of electronics combined with some logic learned in hardware classes, we as a team were able to come up with the implementation to make it work. 🏆 WHAT WE'RE PROUD OF We are most proud of coming up with a novel idea that uses novel technology to help people in need. I think it is everyone's desire to develop something that is actually useful to other humans, especially while utilizing the latest technologies. We think that technology's greatest advantages are the advantages it is able to give to others in order to promote diversity, equity and inclusion. Some Specifics The device our team members' relative would have to use costs upwards of $3000 USD (link). This is prohibitively expensive, especially given that many of the world's Deafblind come from underprivileged nations. The prototyping costs for our hardware were free, thanks to Hack the North. However, we estimate manufacturing costs to be under $100 given the low-cost hardware, making this system far more affordable. Second, our system is novel in its comprehensive approach. Current solutions (such as the $3000 product) require a connection to an external computer to function. This means they cannot be used portably, thereby preventing a Deafblind person from being able to explore. Moreover, they do not integrate any of the image-to-braille or speech & sentiment-to-braille technologies, severely limiting their scope. We believe our project will substantially improve the status quo by allowing for these features. Our device is portable, can be used without additional peripherals, and integrates all necessary conversion technology. Lastly, our prototype has two fully-functioning braille cells that follow the distance and format requirements of Grade II Braille. This means that our hardware is not just a proof of concept for a braille device, but could actually be implemented directly in Deafblind communities. Moreover, it is likely to be extremely efficient as the stepper motor design does not require individual motors for each braille pin, but rather just one motor for a whole column. Perhaps most importantly, though... We are so proud of the fact that we came up with and created a working prototype for our idea within only 32 hours(!!!). For many of us, this was our first time working with hardware within such a short time frame, so learning the ins and outs of it enough to make a product is a huge accomplishment. 💡 WHAT DID WE LEARN? Over the course of this hackathon, we learned so much about the Deafblind community by reading online testimonials of those who have been diagnosed. It allowed us insight into a corner of the world that we had otherwise not known much about prior. Through developing our product while keeping these testimonials in mind, we also realized the difference between developing a product in order to beat others in a competition and developing a product because we believe this could actually be useful to real people one day. The many sponsors here at Hack The North were also very valuable in teaching us about their product and how we can implement them into our product to improve it's functionality and efficiency. For example, Groq were very helpful in describing exactly why utilizing their API was more energy efficient than the big guys. We were super eager to learn about new technologies such as Symphonic labs, as we realized their use-case as an AI that can read lips was very applicable to our device. 🔜 THE FUTURE OF PERCEIV/IO We have thought long and hard about the future of PERCEIV/IO. We've already come up with a laundry list of ideas that we want to implement sometime in the future in order for this product to achieve it's full potential. The number one thing is to shrink the size of the actual hardware. Raspberry Pi's are extremely useful for prototyping- but are a bit too bulky for use in a commercial product. We want to achieve a size similar to a smart watch with similar processing power. We also want to relocate the camera and have it wirelessly communicate with the computer. One idea was to embed the camera into a pair of sunglasses in a similar style to Spectacles by Snap Inc. And finally, we are waiting for the day where AI technology and hardware reaches a point where we can run all models locally, without the need for a massive power bank or network connection. That would truly untether the user from any external requirements for a real feeling of freedom. 🪿 Thank you to all who showed interested in PERCEIV/IO, and a huge congratulations to all hackers who submitted in time. We did it!",
    "matched_prize": "Genesys: Empowering Empathetic Experiences - Unleashing the Power of Genesys Cloud APIs"
  },
  {
    "title": "excus.us",
    "link": "https://devpost.com/software/excus-us",
    "text": "Excus.us: Fostering Diverse Connections Across Tech Events About Excus.us Excus.us is an innovative networking app designed to transform how professionals connect at tech events such as conferences and hackathons. Born from the observation that truly diverse interactions often lead to the most creative and impactful outcomes, Excus.us aims to break down social barriers and encourage connections between individuals with complementary yet different skills, interests, and backgrounds. Key Features really...really fast user growth: Within a span of 4-5 hours there were 30 users actively using and chatting on the platform with 61 total registered hackers; just showing it's potential and value add in this space! How Excus.us Works Profile Creation: Users create a detailed profile highlighting their skills, interests, and background. AI Analysis: Excus.us uses Cohere's embedding model with R+ to create embeddings based on user bios and interests. Diverse Matching: The app uses Groq for quick reranking, utilizes Cohere's embeddings on Chroma to assist in matching users with those who have diverse profiles (based on their bios and interests) compared to their own. In-Event Connections: Users receive suggestions about potential matches and can use the app to locate and chat with them during the event. Cross-Event Networking: Excus.us maintains connections post-event, allowing users to build a diverse network that grows with each new event attended. Technologies Powering Excus.us Challenges and Insights Future Roadmap Booth Interactions and Footfall Optimization During our research, we discovered that exhibitors at tech events often struggle with inconsistent booth traffic and difficulty in attracting the right audience. To address this, Excus.us is expanding its functionality to include: Booth-Attendee Matching: Using our AI-powered matching system to connect attendees with booths that align with their interests and potential business needs. Booth Traffic Analytics: Providing real-time and post-event analytics on booth traffic, helping exhibitors optimize their strategies. Scheduled Booth Visits: Allowing attendees to book short meetings with booth representatives, ensuring a steady flow of interested visitors. Interest-Based Notifications: Sending targeted notifications to attendees about relevant booth presentations or demos. Virtual Queue Management: Implementing a virtual queuing system for popular booths to reduce crowding and improve the experience for both attendees and exhibitors. Feedback Loop: Collecting and analyzing attendee feedback on booth interactions to help exhibitors refine their approach for future events. By addressing both attendee networking and booth interaction challenges, Excus.us aims to create a more holistic and efficient event experience for all participants. Excus.us is committed to creating a more interconnected and diverse professional community, one event at a time. By leveraging advanced AI technologies to assist in fostering unexpected connections and optimizing event interactions, we aim to spark innovation, broaden perspectives, and create a more inclusive and efficient tech ecosystem.",
    "matched_prize": "Best DEI Hack sponsored by Fidelity"
  },
  {
    "title": "DAFP",
    "link": "https://devpost.com/software/dafp",
    "text": "Inspiration Traditional startup fundraising is often restricted by stringent regulations, which make it difficult for small investors and emerging founders to participate. These barriers favor established VC firms and high-networth individuals, limiting innovation and excluding a broad range of potential investors. Our goal is to break down these barriers by creating a decentralized, community-driven fundraising platform that democratizes startup investments through a Decentralized Autonomous Organization, also known as DAO. What It Does To achieve this, our platform leverages blockchain technology and the DAO structure. Here’s how it works: How We Built It Backend: Frontend: Challenges We Faced, Solutions, and Learning Challenge 1 - Creating a Unique Concept: Our biggest challenge was coming up with an original, impactful idea. We explored various concepts, but many were already being implemented. Solution:\nAfter brainstorming, the idea of a DAO-driven decentralized fundraising platform emerged as the best way to democratize access to startup capital, offering a novel and innovative solution that stood out. Challenge 2 - DAO Governance: Building a secure, fair, and transparent voting system within the DAO was complex, requiring deep integration with smart contracts, and we needed to ensure that all members, regardless of technical expertise, could participate easily. Solution:\nWe developed a simple and intuitive voting interface, while implementing robust smart contracts to automate and secure the entire process. This ensured that users could engage in the decision-making process without needing to understand the underlying blockchain mechanics. Accomplishments that we're proud of Developing a Fully Functional DAO-Driven Platform: We successfully built a decentralized platform that allows startups to tokenize their assets and engage with a global community of investors. Integration of Robust Smart Contracts for Secure Transactions: We implemented robust smart contracts that govern token issuance, investments, and governance-based voting bhy writing extensice unit and e2e tests. User-Friendly Interface: Despite the complexities of blockchain and DAOs, we are proud of creating an intuitive and accessible user experience. This lowers the barrier for non-technical users to participate in the platform, making decentralized fundraising more inclusive. What we learned The Importance of User Education: As blockchain and DAOs can be intimidating for everyday users, we learned the value of simplifying the user experience and providing educational resources to help users understand the platform's functions and benefits. Balancing Security with Usability: Developing a secure voting and investment system with smart contracts was challenging, but we learned how to balance high-level security with a smooth user experience. Security doesn't have to come at the cost of usability, and this balance was key to making our platform accessible. Iterative Problem Solving: Throughout the project, we faced numerous technical challenges, particularly around integrating blockchain technology. We learned the importance of iterating on solutions and adapting quickly to overcome obstacles. What’s Next for DAFP Looking ahead, we plan to:",
    "matched_prize": "None"
  },
  {
    "title": "traint",
    "link": "https://devpost.com/software/traint",
    "text": "Inspiration One of our teammates had an internship at a company where they were exposed to the everyday operations of the sales and customer service (SCS) industry. During their time there, they discovered how costly and time consuming it was for companies to properly train, manage and analyze the performance of their SCS department. Even current technologies within the industry such as CRM (Customer Relation Management) softwares were not sufficient to support the demands of training and managing. Our solution encompasses a gamified platform that incorporates an AI tool that’s able to train, analyze and manage the performance of SCS employees. The outcome of our solution provides customers with the best support tailored towards them at a low-cost for the business. What it does traint is used in 4 ways: Utilize AI to facilitate customer support agents in honing their sales and conflict resolution skills Provides feedback and customer sentiment analysis after every customer interaction simulation Ranks customer support agents on a leaderboard based on “sentiment score” Matches top performers within the leaderboard in their respective functions (sales and conflict) to difficult clients. traint provides businesses with the capability to jumpstart and enhance their customer service and sales at a low cost. traint also interfaces AI agents and analysis in a digestible manner for individuals of all technological fluency. How we built it traint was built using the following technologies: Next.js React, Tailwind and shadcn/ui Voiceflow API Groq API Genesys API The Voiceflow API was used mainly to develop the two customer archetypes. One of which being a customer looking to purchase something (sales) and the other being a customer that is unsatisfied with something (conflict resolution). Genesys API was utilized to perform the customer sentiment analysis - a key metric in our application. The API was also used for its live chat queue system, allowing us to requeue and accept “calls” from customers. \\ The Groq API allowed us to analyze the conversation transcript to provide detailed feedback for the operator. Most of our features were interfaced through our web application which hosts action buttons and chat/performance analytics like the average customer sentiment score. Challenges we ran into There was a steep learning curve to Genesys’s extensive API. At first we were overwhelmed with the amount of API endpoints available, and where to begin. We went to the many workshops, including the Genesys workshop, which helped us get started, and we consulted with the teams when we ran into issues with the platform. Initially, we would run into issues with setting up and getting access to certain endpoints since they required us to be granted permissions explicitly from the Genesys team, but they were very prompt and friendly with getting us the access we need to build our app. We also ran into many issues with the prompt engineering for the AI agent on Voiceflow. When building the customer archetypes, the model wasn’t performing as expected so it took a lot of time to get it to work like we wanted. Although we had many challenges, we were all proud of the end product and the work we did despite the many roadblocks we faced :) Accomplishments that we're proud of In a short span of time, we were able to familiarize ourselves with several tools, namely the Genesys’ extensive API and the Voiceflow platform. We integrated these two tools together to create a product that serves both the customer as well as the customer service representatives. What we learned We learnt a lot about API integration, namely the Genesys API and Voiceflow software. Both the API’s were completely new to us and it took alot of research and trial and error to get our product to work. We also learned about the link between frontend and backend in Next.js, and how to transfer information between the both of them via API routes, client and server side components, etc. Our team came into this hackathon not really knowing much about each other. Coming from 4 different universities, we learnt a lot working with each other. What's next for traint We wish that we had more time to fully develop our leaderboard process through the gamification/employee performance API. We ran into some issues with accessing the API and given more time, we would have implemented this feature. Another feature we wanted to add was the ability to assign agents based off of client history. Although there may be some data concerns - providing clients with the ability to talk to the same agents that they preferred in the past would be very beneficial. Finally, we would love to take traint to the next level and see it incorporated in some real life businesses. We truly believe in the use case and hope it solves some key pain points for people.",
    "matched_prize": "'Genesys: Empowering Empathetic Experiences - Unleashing the Power of Genesys Cloud APIs'"
  },
  {
    "title": "Rex",
    "link": "https://devpost.com/software/rex-inywpb",
    "text": "Inspiration It takes over 9 months to train a guide dog. The issue is: dogs can't talk (sorry). What if your favorite furry friend could not only guide you, but also talk to you — and understand exactly where you need to go? This was the founding idea behind Rex. We were inspired to create a fantastically helpful pup, who could lead you anywhere you wanted to go. By combining \"classic\" robotics with cutting edge AI, we were able to bring this dream to reality. We hope, with this, to facilitate the next generation of accessibility support at malls, hospitals, and even hackathons! What it does Rex is a wayfinding bot with agentic functionality. They listen to your request — whatever it may be — and interprets that to be a location in the building you're in. For example, if you told it \"I'm feeling hungry\", it would lead you to the RBC Oasis tent for some snacks! Along the way, it will look out for obstacles in your path and avoid them if necessary. It focuses highly on accessibility. How we built it On the software side, we made extensive use of VoiceFlow and Mappedin. This provided a significant challenge since these services could not efficiently be run on a Raspberry Pi, so we created a mobile-friendly web application that takes care of human-computer interaction (text to speech, speech to text) as well as making API requests. We used a custom knowledge base on VoiceFlow to customize the agent to be very specific to the different rooms in E7 and point of interests at Hack the North. We used VoiceFlow's Supabase integration to log a request in the database to begin a trip in the database (that the Raspberry Pi picks up on). Through the app, we made calls to VoiceFlow's API to progress the user's conversation with the agent. For Mappedin, we rendered the map on a React Typescript application and made use of the Wayfinding endpoint to retrieve thorough directions. Not only are these directions displayed on the app, but they are sent to the Supabase database which the Raspberry Pi listens to for entries to trigger certain movements. Supabase was used to connect the Raspberry Pi, VoiceFlow Agent, and Mappedin wayfinding. We chose Supabase due to its realtime subscription abilities and also its compatibility with all three ends of Rex. On the firmware side, our hardware system involved multiple sensors and actuators such as a Gyroscope, accelerometer, ultra-sonic sensor, and 4 DC motors. Challenged with not having access to a GPS or motor encoders, we used the accelerometer to determine our current location with respect to the origin when navigating through some area, and used the Gyroscope to feed data into our PID controller to ensure we can travel straight and make accurate turns. We also used an ultra-sonic sensor for obstacle avoidance. Once we have received the directions to the requested destination from the database, we use these sensors and actuators to ensure you are able to get there safely. On the mechanical side, the dog-shaped cover, axles, and ears for this project were 3D printed. The cover was designed to resemble a Dachshund, serving as a protective casing for the wheels, MCU, and circuit components. Two axles were fabricated to hold the assembly together, with the lower axle specifically supporting the main microcontroller. One of the main challenges we encountered was the placement of the battery pack inside the dog’s head, which affected the balance. To counteract this, we added another axle vertically, connecting the base to the top of the robot dog to maintain stability. Challenges we ran into We ran into many significant challenges primarily in the realm of interfacing between all the different services and especially the fact that we had to package everything into a Raspberry Pi with real-world movement. We had to leverage a cloud database to communicate with all the different end devices. Moreover, not having a GPS receiver made us look at other solutions to maintain accuracy during movement. Lastly, some APIs didn't have support in certain languages which forced us to be creative with our implementations. Accomplishments that we're proud of We are proud of the way we were able to interconnect all these systems and put it into a compelling form factor. We truly learned a lot about various areas of engineering. What's next for Rex",
    "matched_prize": "'MappedIn: Lead the Way in Indoor Mapping: Transform Spaces with Mappedin SDK'"
  },
  {
    "title": "SecureStep",
    "link": "https://devpost.com/software/securestep",
    "text": "Inspiration With evolutions in bipedal robotics, balance has become something that we take for granted. Yet despite the many steps taken forward in advancing technological walking, we've neglected to innovate and support our elderly populations with their mobility issues. The older we get, the more dangerous a fall can become. We set out to create a solution that provides instant aid to seniors that have been involved in loss of coordination, applying advanced technology and APIs to locate and support them. What it does SecureStep is a smart walking cane designed for seniors in old age homes, responsible for notifying care workers when a fall has occurred. SecureStep's built-in sensors detect falls and transmit an exact geolocation of the incident. Paired with the power of the MappedIn SDK, the geolocation is published to an in-depth floor plan of the old age home, alerting caregivers of the incident, and the best indoor route to reach the victim. Communication over Wi-Fi between the cane's microcontroller and a local webpage ensures urgent care for a fallen senior. Any location covered by the Wi-Fi network will allow the cane to connect to the webpage, enabling long-range data communication. The webpage displays a list of the cane's acceleration and gyration data, along with the longitude and latitude. A map of the floor plans is also included and displays a route to a senior after a fall has occurred. How we built it SecureStep was developed with many technologies. The mechanical enclosure was designed in SolidWorks, with space allocated for an ESP-8266 microcontroller, a 6V battery pack, an MPU6050 accelerometer/gyro sensor, and an RGB LED used for displaying cane status (blinking blue - connecting to Wi-Fi, solid blue - searching for react webpage server, green - connected and upright, red - fall detected). The ESP microcontroller transmits the MPU data to a react webpage via a built-in Wi-Fi module, processing the acceleration of the cane to verify whether a fall has occurred. The ESP also calls the Google Cloud Geolocation API to triangulate its latitude and longitude based on other Wi-Fi devices on the network. The react webpage receives and displays all raw data, along with a MappedIn map of E7. The map is updated with the latitude and longitude of the cane only when a fall is detected, and a path is drawn from a dedicated medical zone to the incident location. Once the cane is returned to its upright position, the fall detection warning is removed along with the path. This ensures that a cane falling on its own and picked back up does not generate a false positive. Challenges we ran into The first (and definitely most annoying) challenge we ran into was uploading code to the ESP module. After an hour of verifying drivers, chugging coffee, and praying to C itself, we realized that our micro-USB cable was unable to transfer data... :/ Besides small annoyances and hurdles to overcome, a significant amount of time was spent trying to perfect the communication between the ESP and the webpage. With over 1000 people using the HTN Wi-Fi network, our solution had to find the right balance between polling for information and not blowing up our ESP! Accomplishments that we're proud of Our team is incredibly proud with the speed in which we developed our idea and brought a fully functional prototype to fruition over the course of the weekend. With no pre-planned idea or resources, we let the technology speak to us and guide us over the past two days! It's truly a reward to have finished a project that not only helped us to learn more skills but also created a solution to a very real problem that affects elderly people across the world. We were also excited to work with amazing sponsored technology, and implement solutions to expand on their API functionality, taking our project to the next level :) What we learned It wouldn't be a hackathon if we had learned nothing. SecureStep has been a fantastic opportunity for our team to try out new APIs, take a crack at web development with React, and expand on our understanding of network communication and synchronization. What's next for SecureStep Moving forward, there are a few changes we'd love to add:",
    "matched_prize": "'MappedIn: Lead the Way in Indoor Mapping: Transform Spaces with Mappedin SDK'"
  },
  {
    "title": "LooGuessr",
    "link": "https://devpost.com/software/looguessr",
    "text": "Inspiration As first-year students, we have experienced the difficulties of navigating our way around our new home. We wanted to facilitate the transition to university by helping students learn more about their university campus. What it does A social media app for students to share daily images of their campus and earn points by accurately guessing the locations of their friend's image. After guessing, students can explore the location in full with detailed maps, including within university buildings. How we built it Mapped-in SDK was used to display user locations in relation to surrounding buildings and help identify different campus areas. Reactjs was used as a mobile website as the SDK was unavailable for mobile. Express and Node for backend, and MongoDB Atlas serves as the backend for flexible datatypes. Challenges we ran into Accomplishments that we're proud of What we learned What's next for LooGuessr",
    "matched_prize": "'MappedIn: Lead the Way in Indoor Mapping: Transform Spaces with Mappedin SDK'"
  },
  {
    "title": "Geese Love Merch",
    "link": "https://devpost.com/software/gesse-love-merch",
    "text": "The Spark How Geese Love Merch was created Beware this story may make you cry when reading .When our team first applied to Hack The North, we were excited about the iconic goose merch typically given out during the opening ceremony. As we approached Laz Hall, we aimed to secure the best spot for a chance to catch one of these flying geese from the cannon. We chose to sit in the first row on the second floor, hoping to be in the perfect position. However, our hopes slowly disappeared when we realized that the geese that were being launched to the second floor were always falling down or being aimed at the worst spots for us to grab them. Disappointed, we decided to create Geese Love Merch to fill the void left by our missed opportunity. We hoped this would encourage the organizers to aim higher and throw some gooses our way. (please do). Powered On and Secure What is Geese Love Merch ? Geese Love Merch is based in the famous Lazarus Hall, featuring our *main character the iconic goose on wheels (yes, we know its amazing). The game's primary objective is to fire merch at sponsors randomly appearing across the seats every 6 seconds. The more sponsors you hit with merch from your gun, the closer you get to securing your dream internship as the ultimate prize. The game gives a fun atmosphere letting the user have a fun and different objective to play on. Gameplay Highlights YouTube tutorial, Google Search, Code, Repeat How we built Geese Love Merch The technology we used for Geese Love Merch we went into cold, with barely any experience. The whole game was built Unity, with primary language being #C. We used tools to enhance our projects such as the Unity asset store, AI modeling and blender to create our own designs. We then further levelled our game up by including animations, text and music to give more of a lively feel towards our game. The Urge to Break our Laptops The challenges we ran into One of the major challenges our team faced was the struggle to commit to a single idea and see it through to completion. Throughout the 36-hour hackathon, we had around 4-5 different project ideas and even began work on a few. However, our biggest struggle was learning Unity and C# in such a short timeframe (essentially a day), and pushing throughout our all nighters. Another significant obstacle we encountered was the limitation of working on Unity one person at a time. If multiple team members ran the project simultaneously and pushed changes, it would cause the game to crash, potentially ruining our entire progress. Key Challenges: The Top of the Hill What we're proud of! Finishing. This 36-hour hackathon has been a rollercoaster of emotions; however, we chose to do something that we loved and would play ourselves. We took this period not only to learn Unity and C# but also to create something that we enjoyed making. At first, we were scared and picked ideas that we thought would be the \"winning idea\"; however, we soon realized that we weren't having any fun or really learning anything. So we decided to risk it all and attempted to do something we never imagined we would do when we first got here. Sunday Morning What we learned Have fun and don't take everything too seriously.. This is the biggest lesson our team learned, and we wish we had learned it sooner. Initially, we wanted our idea to be the next big thing in Silicon Valley; however, the reality was that we weren't having much fun. We were working on something none of our group members felt proud of. Then we decided to create \"Geese Love Merch,\" which completely changed our group's energy. We started laughing and became excited about what the final product would be. What's next for Geese Love Merch We want to keep working on this even after the hackathon and add to it to make it a even better game. We hope you enjoy the game as much as we enjoyed making it :)",
    "matched_prize": "'Ubisoft: Ubisoft Game Dev Challenge'"
  },
  {
    "title": "One in the Chamber",
    "link": "https://devpost.com/software/one-in-the-chamber",
    "text": "Inspiration Ubisoft's challenge (a matter of time) + VR gaming + Epic anime protagonists What it does It entertains (and it's good at it too!) How we built it Unity, C#, Oculus SDK Challenges we ran into Time crunch, limited assets, sleep debt Accomplishments that we're proud of Playable game made with only 2 naps and 1 meal What we learned 36 hrs is a lot less than 48 hrs What's next for One in the Chamber Oculus Start?? :pray:",
    "matched_prize": "'Ubisoft: Ubisoft Game Dev Challenge'"
  },
  {
    "title": "Shop The North",
    "link": "https://devpost.com/software/shop-the-north",
    "text": "Inspiration Online shopping is hard because you can't try on clothing for yourself. We want to make a fun and immersive online shopping experience through VR that will one day hopefully become more realistic and suitable for everyday online shopping. We hope to make a fun twist that helps improve decisiveness while browsing online products. What it does This VR experience includes movement, grabbable clothes, dynamic real-time texturing of models using images from Shopify’s Web API, and a realistic mirror of yourself to truly see how these fits shall look on you before buying it! The user that enters the virtual clothing store may choose between a black/yellow shirt, and black/tropical pants which are products from an online Shopify store (see screenshots). The user can also press a button that would simulate a purchase :) How we built it Using the Shopify ProductListing API, we accessed the different clothing items from our sample online store with a C# script. We parsed the JSON that was fetched containing the product information to give us the image, price, and name of the products. We used this script to send this information to our VR game. Using the images of the products, we generated a texture for each item in virtual reality in the Unity game engine, which was then put onto interactable items in the game. Also, we designed some models, such as signs with text, to customize and accessorize the virtual store. We simulated the shopping experience in a store as well as interactable objects that can be tried on. Challenges we ran into Linking a general C# script which makes REST API calls to Unity was a blocker for us because of the structure of code Unity works with. We needed to do some digging and search for what adjustments needed to be made to a generic program to make it suitable for the VR application. For example, including libraries such as Newtonsoft. We also ran into conflicts when merging different branches of our project on GitHub. We needed to spend additional time rolling back changes and fixing bugs to take the next step in the project. One significant difficulty was modelling the motion of the virtual arms and elbows when visible in the mirror. This must be done with inverse kinematics which we were never quite able to smoothly implement, although we achieved something somewhat close. Getting the collision boundaries was difficult as well. The player and their surroundings constantly change in VR, and it was a challenge to set the boundaries of item interaction for the player when putting on items. Accomplishments that we're proud of Our project has set a strong foundation for future enhancements. We’re proud of the groundwork we’ve laid for a concept that can grow and evolve, potentially becoming a game-changing solution in VR shopping. What we learned We learned the importance of anticipating potential technical blockers, such as handling complex features like inverse kinematics and collision limits. Properly allocating time for troubleshooting unexpected challenges would have helped us manage our time more efficiently. Also, many technical challenges required a trial-and-error approach, especially when setting up collision boundaries and working with avatar motion. This taught us that sometimes it's better to start with a rough solution and iteratively refine it, rather than trying to perfect everything on the first go. Finally, working as a team, we realized the value of maintaining clear communication, especially when multiple people are contributing to the same project. Whether it was assigning tasks or resolving GitHub conflicts, being aligned on priorities and maintaining good communication channels kept us moving forward. What's next for Shop The North We want to add even more variety for future users. We hope to develop more types of clothing, such as shoes and hats, as well as character models that could suit any body type for an inclusive experience. Additionally, we would like to implement a full transaction system, where the user can add all the products they are interested into a cart and complete a full order (payment information, shipping details, and actually placing a real order). In general, the goal would be to have everything a mobile/web online shopping app has, and more fun features on top of it.",
    "matched_prize": "Shopify API Challenge"
  },
  {
    "title": "Hack the Bomb",
    "link": "https://devpost.com/software/hack-the-bomb",
    "text": "Built With",
    "matched_prize": "None"
  },
  {
    "title": "GenomIQ",
    "link": "https://devpost.com/software/genomiq",
    "text": "A dictionary is at the end for reference to biology terms. Inspiration We're at a hackathon right? So I thought, why stop at coding with 1s and 0s when we can code with A, T, G, and C? 🧬 Genetic engineering is a cornerstone of the modern world, whether its for agriculture, medicine, and more; but let's face it, it's got a bit of a learning curve. That's where GenomIQ comes in. I wanted to create a tool that lets anyone – yes, even you – play around with editing plasmids and dive into genetic engineering. But here's the kicker: we're not just making it easier, we're turbocharging it with the expressive ability of LLMs to potentially generate functional protein-coding DNA strings. What it does GenomIQ streamlines plasmid engineering by combining AI-powered gene generation with a curated gene database. It uses a custom-finetuned Cohere model to create novel DNA sequences, validated for biological plausibility via AlphaFold 2 and iterated on. Alternatively, you can rapidly search for existing genes stored in our Chroma vectordb. The platform automatically optimizes restriction sites and integrates essential genetic elements. Users can easily design, modify, and export plasmids ready for real-world synthesis, bridging the gap between computational design and practical genetic engineering. How I built it This is a Flask web app built with python, and vanilla html/css/js on the frontend. The vectordb is powered by Chroma. LLM is Cohere fine tuned on a short custom dataset included in the github repo. Restriction sites are automatically scored and sorted based on usefulness for clean insertion. Verification is performed by a local instance of Alphafold 2, which based on the provided DNA sequence will give you a structure file. I found a website that implements Prosa, a scoring metric for proteins, and built a web scrapper/bot that uploads your structure file and gathers the z-score from there. The plasmid viewer is a canvas that is updated whenever a route returns new features. The repo also includes a file for a short fine tuning dataset builder tool with a GUI, that I put together to make it easier to fine tune my model. I developed a benchmark set and performed an evaluation of the standard cohere model vs the fine tuned model, and compared their z-score across. As displayed in the image, the fine tune is much more capable of producing biologically plausible strings of DNA. Challenges I ran into Cohere api timeouts: lots of requests would not work randomly, had to use threading to check how long it was running, and be able to cut it off if it takes too long. Frontend as a whole was a big challenge, I have hardly built web apps before so this was a lot of back and forth, wondering why X element wont go to the center of the page no matter how hard I try. Accomplishments that I'm proud of Building a cool project in a day and a half :) What I learned Vector db, alphafold, genetic engineering, What's next for GenomIQ I want to evaluate what a tool like GenomIQ's place in the world could be. I want to reach out to people who would be interested in such a tool, and see what direction to take it in. There are a lot of improvements that can be made, as well as opportunity for some incredible new features. Dictionary Plasmid: Small circular ring of DNA. These are typically cut up and have new genes inserted into them. Afterwards, these plasmids are inserted into organisms like yeast, bacteria, etc who will now express the new gene. Restriction site: The zones on the plasmid where we do the cutting. Some sites are more desirable than others, typically given by uniqueness (only want to cut in one spot) and distance from other genes/features (don't want to cut up something important). Sorry if any of this seems jumbled... im really tired.",
    "matched_prize": "'Chroma: Retrieval for AI'"
  },
  {
    "title": "BrainBeatReal",
    "link": "https://devpost.com/software/heartbeatreal",
    "text": "Inspiration Have you ever thought that BeReal's are too fake? An app built to challenge the unrealistic beauty standards set by typical social media turned into one of the giants it aimed to destroy. With BrainbeatReal we capture the real \"you\". Our state of the art social media platform prompts you to take a picture only when your heart rate and brain waves are elevated - because you're only real when your body tells you that you are. What it does Our application monitors your heart rate and brain waves measured by EEG to capture moments in your life to share a more real you. When your heart rate is high and brainwave activity signals more vivid emotions, the app will prompt you to take a picture through your front and back camera to share this interesting moment in your life. There is no other way to post media, only when your body feels like it, you can! We have our AI Assistant Brainreader perform sentiment analysis on your pictures to generate a nonsensical caption. These moments are aggregated and analyzed to generate daily insights for the users. We auto-generate your captions to represent what your mind and heart really think without an opportunity for filtering. How we built it See systems diagram above Our application interacts with three physical devices, an Apple watch for heart rate monitoring, the Neurosity crown for brainwave monitoring, and the user facing application on the user’s mobile device. At a certain threshold the Apple watch will send a request to the backend to start the brainbeat recording process, this notification is relayed to the user’s phone which checks and collects the user’s current brainwaves to confirm an interesting event has occurred which triggers the start of taking a brainbeat. To record a “brainbeat”, the heart rate, brain wave and front/back camera image data is passed into our backend server. The backend server will (1) generate an interesting caption for the images and (2) store and process the data into the database. (1) The caption generation is powered by the LLaVA (Large Language and Vision Assistant) V1.5 7B and LLaMA 3.1 8B models through the Groq. The LLaVA model analyzes the images to provide a description for the front and back camera photos. The descriptions are then fed into the LLaMA model which is prompted to generate a funny nonsensical caption for the brainbeat. (2) The brainbeat data (caption, photo urls, heart rate etc.) is stored into a Databricks managed table to take advantage of Databricks’ extensive infrastructure. This table is queried in a scheduled job to aggregate the day’s brainbeat data which is stored in a separate table for analytics and insights. The job is run everyday at 12am for the previous day’s data. This process allows for more optimized retrievals of the aggregated insights on user request. There is also an opportunity for future extension in collecting streaming data of the user’s brain waves / heart rate information for more intensive processing using Databricks’ pipelines. Technologies used: Challenges we ran into We worked with a lot of new technologies and three different devices to create our vision of this app. Some challenges included: Hardware Integration: We did a lot of planning based on the possibilities of hardware availability and how we could allow the devices to communicate with each other. In the end a lot of the hardware we originally wanted was unavailable, so we had to choose a backup plan with more difficult implementation. Working with new technology: We had the opportunity to try out a lot of new technology that we haven’t had experience using before. For example, no one on the team has used Swift, Groq or Databricks before, but they ended up being essential parts to our project's success. Accomplishments that we're proud of We’re proud to have completed an end to end project with a couple of different applications and integrations. There were many hiccups along the way where we could have thrown in the towel but we chose to stay up and work hard to complete our project. What we learned Throughout the course of 32 hours, tireless effort was put into making a product that met our expectations. We learned that perseverance can get you a long way and how much of an itch a good project can scratch. Technically, we were able to learn more about integrations, databases, data analytics, mobile applications, watch applications and EEG analysis. What’s next? BrainBeatReal has a lot of potential on the analytics side; we believe that users can gain a lot of insightful information through the analysis of their heartbeat and brain wave data. As a result, we want to integrate a streaming table and a data pipeline to better capture and aggregate data in real time.",
    "matched_prize": "Databricks: Best Use of Databricks"
  },
  {
    "title": "VerbaTeX",
    "link": "https://devpost.com/software/verbatex",
    "text": "Inspiration As computer science and math students, many of the classes we've taken have required us to use LaTeX - and we all know just how much of a struggle it can be to memorize each and every formula exactly. We knew there had to be a better way, and decided that this weekend would be as good a time as any to tackle this challenge. What it does VerbaTeX converts speech to LaTeX code, performs server-side compilation, and renders an image preview in real-time. How we built it The web app is built with React, TypeScript, and Flask. Firebase and AWS were used for database management and authentication. A custom fine-tuned ChatGPT API was used for NLP and LaTeX translation. Challenges we ran into Although the 32 hours we were given sounded like more than enough at the start, the amount of times we changed plans and ran into bugs quickly made us change our minds. Accomplishments that we're proud of We are proud of building a fully functional full-stack application in such a short time span. We spent a lot of time and effort trying to meet all of our ambitious goals. We truly felt that we dreamed big at Hack the North 2024. What we learned We learned that choosing new and unfamiliar technologies can sometimes be better than sticking to what we're used to. Although none of us had much experience with Defang before Hack the North, we learned about them during workshops and sponsor boothings and were intrigued - and choosing to build our project on it was definitely a good idea! What's next for VerbaTeX LaTeX has so many more functionalities that could be simplified through VerbaTeX, from creating anything from diagrams to tables to charts. We want to make even more of these functions accessible and easier to use for LaTeX users all over the world.",
    "matched_prize": "'Defang: Best Use of Defang'"
  },
  {
    "title": "www.LooLooLoo.co",
    "link": "https://devpost.com/software/waterwaterwater-loolooloo",
    "text": "Inspiration www.loolooloo.co is inspired by the famous Waterloo chant—Water Water Water! Loo Loo Loo! And, of course, the natural consequence of drinking lots of water, water, water, being a visit to the loo, loo, loo! What it does www.loolooloo.co detects when you go to a water fountain, and sends you a text message with a custom link to an interactive map that gives you directions to the nearest bathroom. It also has a daily water tracker that incentivizes you to stay hydrated. How we built it We utilized the MappedIn API to implement an indoor navigation system for building maps. The front-end was developed using React, while the back-end was powered by Node.js. Twilio's API was integrated to enable SMS notifications with custom map links. Challenges we ran into Deploying a Bluetooth beacon to detect user proximity and send real-time requests to an HTTPS port proved to be a complex task, particularly in managing secure communications and ensuring reliable detection within a defined radius. Accomplishments that we're proud of We successfully built a fully functional system where the front and back ends work seamlessly together, delivering a real-time user experience by sending SMS directions to the nearest restroom whenever a user approaches a water fountain. What we learned We gained valuable experience in integrating proximity detection systems with cloud services, handling real-time data, and optimizing our full stack application for responsive and secure user interactions. What's next for www.loolooloo.co Future plans include enhancing the system's accuracy, and making the UI more user friendly.",
    "matched_prize": "'MappedIn: Lead the Way in Indoor Mapping: Transform Spaces with Mappedin SDK'"
  },
  {
    "title": "Studylingo",
    "link": "https://devpost.com/software/studiolingo",
    "text": "Inspiration We were inspired to build a tool that addresses the everyday struggle of staying focused while studying or browsing the web. We wanted to turn passive reading into an active learning experience while helping students manage their time efficiently. Integrating a Pomodoro timer, web summarization, and quiz generation into a single Chrome extension emerged as a creative way to keep learning fun and engaging. What it does Studylingo is a Chrome extension with three main features: Web Page Summarization: This feature automatically summarizes the content of any web page the user is browsing, making it easier to absorb key points quickly. Pomodoro Timer: Helps users stay focused using timed study sessions with customizable work and break intervals. Quiz: Turns the summarized content into interactive quizzes, making the learning experience fun and reinforcing the material meaningfully. How we built it We built Studylingo using React for the Chrome extension and Python FastAPI for the backend. Cohere’s AI helps us summarize web pages and turn them into quizzes. It works through a Chrome extension, so you can learn and quiz yourself while browsing seamlessly. We deployed the backend using Defang. Challenges we ran into It was tricky to get all the features to work smoothly in one Chrome extension. The Chrome extension library we used, CRXJS, was no longer actively maintained, so documentation was minimal. Accomplishments that we're proud of We’re proud of creating a Chrome extension for the first time, something we’ve never tried before, and getting all the features to work together. We are also proud of leveraging LLMs like Cohere to help others further themselves, in this case, academically. Plus, being part of Hack the North!!! What we learned We learned how to combine browser APIs with AI-driven content generation, building an efficient Chrome extension. In addition, we learned how to build AI-driven APIs and the different ways data gets transferred when using LLMs on APIs. What's next for Studylingo We plan to enhance the quiz generation feature by introducing more question formats and difficulty levels. Additionally, we want to expand the summarization capabilities to handle more complex web pages while offering users personalized study analytics and progress tracking within the extension.",
    "matched_prize": "'Cohere: Best Use of Cohere'"
  },
  {
    "title": "Savvy Saver",
    "link": "https://devpost.com/software/savvy-saver",
    "text": "Inspiration As students, we have found that there are very few high-quality resources on investing for those who are interested but don't have enough resources. Furthermore, we have found that investing and saving money can be a stressful experience. We hope to change this for those who want to save better with the help of our app, hopefully making it fun in the process! What it does Our app first asks a new client a brief questionnaire about themselves. Then, using their banking history, it generates 3 \"demons\", aka bad spending habits, to kill. Then, after the client chooses a habit to work on, it brings them to a dashboard where they can monitor their weekly progress on a task. Once the week is over, the app declares whether the client successfully beat the mission - if they did, they get rewarded with points which they can exchange for RBC Loyalty points! How we built it We built the frontend using React + Tailwind, using Routes to display our different pages. We used Cohere for our AI services, both for generating personalized weekly goals and creating a more in-depth report. We used Firebase for authentication + cloud database to keep track of users. For our data of users and transactions, as well as making/managing loyalty points, we used the RBC API. Challenges we ran into Piecing the APIs together was probably our most difficult challenge. Besides learning the different APIs in general, integrating the different technologies got quite tricky when we are trying to do multiple things at the same time! Besides API integration, definitely working without any sleep though was the hardest part! Accomplishments that we're proud of Definitely our biggest accomplishment was working so well together as a team. Despite only meeting each other the day before, we got along extremely well and were able to come up with some great ideas and execute under a lot of pressure (and sleep deprivation!) The biggest reward from this hackathon are the new friends we've found in each other :) What we learned I think each of us learned very different things: this was Homey and Alex's first hackathon, where they learned how to work under a small time constraint (and did extremely well!). Paige learned tons about React, frontend development, and working in a team. Vassily learned lots about his own strengths and weakness (surprisingly reliable at git, apparently, although he might have too much of a sweet tooth). What's next for Savvy Saver Demos! After that, we'll just have to see :)",
    "matched_prize": "'RBC: RBC Avion Rewards – Making Your Point with Our Points'"
  },
  {
    "title": "EcoRewards",
    "link": "https://devpost.com/software/ecorewards-t0qw26",
    "text": "🌱 Inspiration With the ongoing climate crisis, we recognized a major gap in the incentives for individuals to make greener choices in their day-to-day lives. People want to contribute to the solution, but without tangible rewards, it can be hard to motivate long-term change. That's where we come in! We wanted to create a fun, engaging, and rewarding way for users to reduce their carbon footprint and make eco-friendly decisions. 🌍 What it does Our web app is a point-based system that encourages users to make greener choices. Users can: 🛠️ How we built it We used a mix of technologies to bring this project to life: 🔧 Challenges we ran into One of the biggest challenges was figuring out how to fork the RBC points API, adapt it, and then code our own additions to match our needs. This was particularly tricky when working with the database schemas and migration files. Sending image files across the web also gave us some headaches, especially when incorporating real-time processing with AI. 🏆 Accomplishments we're proud of One of the biggest achievements of this project was stepping out of our comfort zones. Many of us worked with a tech stack we weren't very familiar with, especially Remix. Despite the steep learning curve, we managed to build a fully functional web app that exceeded our expectations. 🎓 What we learned 🚀 What's next We have exciting future plans for the app:",
    "matched_prize": "RBC: RBC Avion Rewards – Making Your Point with Our Points"
  },
  {
    "title": "Unmute: Speak, Even When You Can’t.",
    "link": "https://devpost.com/software/unmute-speak-even-when-you-can-t",
    "text": "Inspiration We've all been there - racing to find an empty conference room as your meeting is about to start, struggling to hear a teammate who's decided to work from a bustling coffee shop, or continuously muting your mic because of background noise. As the four of us all conclude our internships this summer, we’ve all experienced these over and over. But what if there is a way for you to simply take meetings in the middle of the office… What it does We like to introduce you to Unmute, your solution to clear and efficient virtual communication. Unmute transforms garbled audio into audible speech by analyzing your lip movements, all while providing real-time captions as a video overlay. This means your colleagues and friends can hear you loud and clear, even when you’re not. Say goodbye to the all too familiar \"wait, I think you're muted\". How we built it Our team built this application by first designing it on Figma. We built a React-based frontend using TypeScript and Vite for optimal performance. The frontend captures video input from the user's webcam using the MediaRecorder API and sends it to our Flask backend as a WebM file. On the server side, we utilized FFmpeg for video processing, converting the WebM to MP4 for wider compatibility. We then employed Symphonic's API to transcribe visual cues. Challenges we ran into Narrowing an idea was one of the biggest challenges. We had many ideas, including a lip-reading language course, but none of them had a solid use case. It was only after we started thinking about problems we encountered in our daily lives did we find our favorite project idea. Additionally, there were many challenges on the technical side with using Flask and uploading and processing videos. Accomplishments that we're proud of We are proud that we were able to make this project come to life. Next steps Symphonic currently does not offer websocket functionality, so our vision of making this a real-time virtual meeting extension is not yet realizable. However, when this is possible, we are excited for the improvements this project will bring to meetings of all kinds.",
    "matched_prize": "'Symphonic Labs: Best use of Symphonic Labs API'"
  },
  {
    "title": "SpyWatch",
    "link": "https://devpost.com/software/spywatch",
    "text": "Inspiration We were interested in the Symphonic Labs API and it's potential in audioless environments. As we were discussing potential uses, we got our hands on a Tello drone with full video streaming capabilities and realized that this drone has no audio. What kind of spy drone can't even peep in on your target's conversations? So we applied our idea of using the Symphonic Labs API in tandem with the Tello drone to create SpyWatch, a drone surveillance software that easily and reliably lets you listen in on conversations, without needing to actually listen! What it does Spy's on conversations using a live drone video feed and the Symphonic Labs lip reading technology. How we built it Challenges we ran into Accomplishments that we're proud of What we learned What's next for SpyWatch",
    "matched_prize": "Symphonic Labs: Best use of Symphonic Labs API"
  },
  {
    "title": "LipsLips Revolution",
    "link": "https://devpost.com/software/lipslips-revolution",
    "text": "Inspiration Why not? We wanted to try something new. Lip reading and the Symphonic Labs API was something we hadn't seen before. We wanted to see how far we could push it! What it does Like singing? LLR is for you! Don’t like singing? LLR IS STILL FOR YOU! Fight for the top position in a HackTheNorth lip-syncing challenge. How does it work? Very simple, very demure: How we built it LLR is a web app built with Next.js, enabling the rapid development of reactive apps with backend capability. The Symphonic API is at the core of LLR, powering the translation from lip movement to text. We’re using OpenAI’s embedding models to determine the lip sync’s accuracy/similarity and MongoDB as the database for score data. Challenges we ran into While the Symphonic API is super cool, we found it slow sometimes. We found that a 10-second video took around 5 seconds to upload and 30 seconds to translate. This just wasn’t fast enough for users to get immediate feedback. We looked at Symphonic Lab’s demo of Mamo, and it was much faster. We delved deeper into Mamo’s network traffic, we found that it used a much faster web socket API. By figuring out the specifications of this newfound API, we lowered our latency from 30 seconds to 7 seconds with the same 10-second clip. Accomplishments that we're proud of The friends we made along the way. What we learned Over the course of this hackathon, we learned from workshops and our fellow hackers. We learned how to quickly create an adaptive front end from the RWD workshop and were taught how to use network inspection to reverse engineer API processes. What's next for LipsLips Revolution We hope to integrate with the Spotify API or other music services to offer a larger variety of songs. We also wish to add a penalty system based on the amount of noise made. It is, after all, lip-syncing and not just singing. We do hope to turn this into a mobile app! It’ll be the next TikTok, trust…",
    "matched_prize": "Symphonic Labs: Best use of Symphonic Labs API"
  },
  {
    "title": "Vanguard",
    "link": "https://devpost.com/software/vanguard-lm40dt",
    "text": "🎯 The Project Story 🔍 About Vanguard In today's fast-paced digital landscape, cybersecurity is not just important—it's essential! As threats multiply and evolve, security teams need tools that are agile, compact, and powerful. Enter Vanguard, our groundbreaking Raspberry Pi-powered vulnerability scanner and WiFi hacker. Whether you’re defending air-gapped networks or working on autonomous systems, Vanguard adapts seamlessly, delivering real-time insights into network vulnerabilities. It's more than a tool; it's a cybersecurity swiss army knife for both blue and purple teams! 🛡️🔐 Air Gapped Network Deployability (CSE Challenge) Having a dedicated database of vulnerabilities in the cloud for vulnerability scanning could pose a problem for deployments within air-gapped networks. Luckily, Vanguard can be deployed without the need for an external vulnerability database. A local database is stored on disk and contains precisely the information needed to identify vulnerable services. If necessary, Vanguard can be connected to a station with controlled access and data flow to reach the internet; this station could be used to periodically update Vanguard’s databases. Data flow is crucial in an embedded cybersecurity project. The simplest approach would be to send all data to a dedicated cloud server for remote storage and processing. However, Vanguard is designed to operate in air-gapped networks, meaning it must manage its own data flow for processing collected information. Different data sources are scraped by a Prometheus server, which then feeds into a Grafana server. This setup allows data to be organized and visualized, enabling users to be notified if a vulnerable service is detected on their network. Additionally, more modular services can be integrated with Vanguard, and the data flow will be compatible and supported. It is important for Vanguard to be able to receive tasks. Our solution provides various methods for controlling Vanguard's operations. Vanguard can be pre-packaged with scripts that run periodically to collect and process data. Similar to the Assemblyline product, Vanguard can use cron jobs to create a sequence of scripts that parse or gather data. If Vanguard goes down, it will reboot and all its services will restart automatically. Services can also be ran as containers. Within an air-gapped network, Vanguard can still be controlled and managed effectively. Vanguard will scan the internal air-gapped network and keep track of active IP addresses. This information is then fed into Grafana, where it serves as a valuable indicator for networks that should have only a limited number of devices online. Air Gapped Network Scanning (Example) Context: Raspberri Pi is connected to a hotspot network to mimic an air gapped network. Docker containers are run to simulate devices being on the air gapped network. This example will show how Vanguard identifies a vulnerable device on the air gapped network. Here are the cron scripts: In the /var/log Vanguard Logged a new IP: Vanguard's port scanner found open ports on our vulnerable device: IP Activity history show how many time an IP was seen: Vulnerability logs are displayed on our Grafana dashboard and we can see that our ports were scanned as running a vulnerable serivce. (2 red blocks on the right) (Only Port 21 and 22 All this data flow was able to detect a new device and vulnerable services without the need of cloud or internet services. Vanguard's automated script's ran and detected the anomaly! 💡 Inspiration Our team was fascinated by the idea of blending IoT with cybersecurity to create something truly disruptive. Inspired by the open-source community and projects like dxa4481’s WPA2 handshake crack, we saw an opportunity to build something that could change the way we handle network vulnerabilities. We didn’t just want a simple network scanner—we wanted Vanguard to be versatile, portable, and powerful enough to handle even the most secure environments, like air-gapped industrial networks or autonomous vehicles 🚗💻. 🏆 Accomplishments And everything comes together seamlessly in the vangaurd dashboard. Additionally, we integrated Convex as our backend data store to keep things fast, reliable, and easy to adapt for air-gapped networks (swap Convex for MongoDB with a breeze 🌬️ we really wanted to do take part in the convex challenge). 🔧 Challenges We Faced Building Vanguard wasn’t without its obstacles. Here's what we had to overcome: 🏗️ How We Built It 🚀 What’s Next for Vanguard? We're just getting started! Here’s what’s in store for Vanguard: Vanguard isn’t just a project; it’s the future of portable, proactive cybersecurity. 🌐🔐 Stay secure, stay ahead!",
    "matched_prize": "'CSE: Working in an Air-Gapped Environment'"
  },
  {
    "title": "LLM Pro Max",
    "link": "https://devpost.com/software/llm-pro-max",
    "text": "Inspiration Large Language Models (LLMs) are limited by a token cap, making it difficult for them to process large contexts, such as entire codebases. We wanted to overcome this limitation and provide a solution that enables LLMs to handle extensive projects more efficiently. What it does LLM Pro Max intelligently breaks a codebase into manageable chunks and feeds only the relevant information to the LLM, ensuring token efficiency and improved response accuracy. It also provides an interactive dependency graph that visualizes the relationships between different parts of the codebase, making it easier to understand complex dependencies. How we built it Our landing page and chatbot interface were developed using React. We used Python and Pyvis to create an interactive visualization graph, while FastAPI powered the backend for dependency graph content. We've added third-party authentication using the GitHub Social Identity Provider on Auth0. We set up our project's backend using Convex and also added a Convex database to store the chats. We implemented Chroma for vector embeddings of GitHub codebases, leveraging advanced Retrieval-Augmented Generation (RAG) techniques, including query expansion and re-ranking. This enhanced the Cohere-powered chatbot’s ability to respond with high accuracy by focusing on relevant sections of the codebase. Challenges we ran into We faced a learning curve with vector embedding codebases and applying new RAG techniques. Integrating all the components—especially since different team members worked on separate parts—posed a challenge when connecting everything at the end. Accomplishments that we're proud of We successfully created a fully functional repo agent capable of retrieving and presenting highly relevant and accurate information from GitHub repositories. This feat was made possible through RAG techniques, surpassing the limits of current chatbots restricted by character context. What we learned We deepened our understanding of vector embedding, enhanced our skills with RAG techniques, and gained valuable experience in team collaboration and merging diverse components into a cohesive product. What's next for LLM Pro Max We aim to improve the user interface and refine the chatbot’s interactions, making the experience even smoother and more visually appealing. (Please Fund Us)",
    "matched_prize": "'Chroma: Retrieval for AI'"
  },
  {
    "title": "ReCall: Memories done for you",
    "link": "https://devpost.com/software/recall-memories-done-for-you",
    "text": "Inspiration 2 days before flying to Hack the North, Darryl forgot his keys and spent the better part of an afternoon retracing his steps to find it- But what if there was a personal assistant that remembered everything for you? Memories should be made easier with the technologies we have today. What it does A camera records you as you go about your day to day life, storing \"comic book strip\" panels containing images and context of what you're doing as you go about your life. When you want to remember something you can ask out loud, and it'll use Open AI's API to search through its \"memories\" to bring up the location, time, and your action when you lost it. This can help with knowing where you placed your keys, if you locked your door/garage, and other day to day life. How we built it The React-based UI interface records using your webcam, screenshotting every second, and stopping at the 9 second mark before creating a 3x3 comic image. This was done because having static images would not give enough context for certain scenarios, and we wanted to reduce the rate of API requests per image. After generating this image, it sends this to OpenAI's turbo vision model, which then gives contextualized info about the image. This info is then posted sent to our Express.JS service hosted on Vercel, which in turn parses this data and sends it to Cloud Firestore (stored in a Firebase database). To re-access this data, the browser's built in speech recognition is utilized by us along with the SpeechSynthesis API in order to communicate back and forth with the user. The user speaks, the dialogue is converted into text and processed by Open AI, which then classifies it as either a search for an action, or an object find. It then searches through the database and speaks out loud, giving information with a naturalized response. Challenges we ran into We originally planned on using a VR headset, webcam, NEST camera, or anything external with a camera, which we could attach to our bodies somehow. Unfortunately the hardware lottery didn't go our way; to combat this, we decided to make use of MacOS's continuity feature, using our iPhone camera connected to our macbook as our primary input. Accomplishments that we're proud of As a two person team, we're proud of how well we were able to work together and silo our tasks so they didn't interfere with each other. Also, this was Michelle's first time working with Express.JS and Firebase, so we're proud of how fast we were able to learn! What we learned We learned about OpenAI's turbo vision API capabilities, how to work together as a team, how to sleep effectively on a couch and with very little sleep. What's next for ReCall: Memories done for you! We originally had a vision for people with amnesia and memory loss problems, where there would be a catalogue for the people that they've met in the past to help them as they recover. We didn't have too much context on these health problems however, and limited scope, so in the future we would like to implement a face recognition feature to help people remember their friends and family.",
    "matched_prize": "None"
  },
  {
    "title": "Tangle: Connect the North",
    "link": "https://devpost.com/software/tangle-connect-the-north",
    "text": "Inspiration Behind Tangle ⚛️ The idea for Tangle emerged from a recognition that traditional networking methods, such as LinkedIn profiles, have remained static, even as large in-person events like Hack the North make a strong comeback. We saw an opportunity to reimagine how people connect by creating a more dynamic and visual way of representing relationships formed at events. Tangle aims to bridge the gap by providing an interactive web app that helps participants remember, revisit, and reinforce connections made during events, ensuring those relationships don’t just fade into the background.🤝 How We Built Tangle 🛠️ Tangle is a web app powered by React for the front end and Convex for the back end, with the entire platform hosted on Vercel. We used a GoDaddy domain to make Tangle easily accessible to all users.🌐 Our backend, built with Convex, integrates a vector embedding search powered by Cohere. This allows us to offer advanced search functionality that helps users locate people they’ve met based on attributes and interactions. On the front end, we designed a simple yet intuitive user interface in React, focusing on ease of navigation. The landing page introduces users to Tangle, while the home page allows them to search for individuals or features related to people they've met at the event. The query is sent to the backend, where it’s processed, and the results are returned in real-time.⚡️ Challenges We Overcame Building Tangle 🚧 Design Challenges 🎨 One of our main challenges was creating a platform that caters to different event attendees—hackers, recruiters, and speakers—all of whom have different goals. Designing Tangle to meet the needs of all these groups while maintaining a cohesive user experience required careful planning and iteration. Vector Embeddings 🧠 A technical challenge was mastering the use of vector embeddings. These embeddings were critical for enabling intelligent search functions within Tangle. We invested considerable time in optimizing the embedding process to accurately capture the nuanced relationships between event attendees. Personal Challenges 💪 Balancing external commitments and the time pressure of the hackathon was no easy feat. At one point, we experienced challenges leading to a demotivating lull 10 hours prior to submissions, but we pushed through as a team in the final hours to finish strong. Accomplishments We Celebrate at Tangle 🎉 Lessons Learned from Tangle's Journey 📚 The Importance of Planning 📝 One of the key takeaways from building Tangle was the necessity of meticulous planning, especially regarding user flow and system architecture. Diving straight into coding without first understanding how users would interact with our app resulted in some avoidable setbacks and rework. We learned that investing time upfront in planning leads to a more streamlined and efficient development process. Simplifying Systems 🧩 We initially overcomplicated parts of our application, which led to inefficiencies. Simplifying and focusing on the core functionalities of Tangle allowed us to optimize the system and deliver a better user experience.",
    "matched_prize": "'Best Use of Cohere'"
  },
  {
    "title": "aligned.ai",
    "link": "https://devpost.com/software/align-sqzt8c",
    "text": "Inspiration Have you ever attended a networking event and felt overwhelmed by the sheer number of people, unsure of how to find the right connections? We've all been there, wishing for a more streamlined way to meet individuals who align with our goals and values. The inspiration for Aligned.ai comes from this common challenge—finding people who will empower you to do your life's best work. Our goal was to create a system that helps individuals build sustainable and long-lasting relationships in the startup space, whether it’s founder-to-founder or founder-to-VC. What it does Aligned.ai is a matchmaking platform designed to connect individuals in the startup ecosystem based on deep personality and goal alignment. Users engage in live, in-depth conversations with Aligned Voice, an AI-powered by Groq, which simulates a real human interaction. The system then generates a unique personality embedding using Cohere, which is stored in Chroma DB. Using powerful vector similarity algorithms, Aligned.ai ranks potential connections, presenting users with the best matches first. This way, users can find those who are not only aligned with their professional aspirations but also resonate with their personal values. How we built it Aligned.ai was developed with a focus on seamless integration across multiple technologies: Challenges we ran into Accomplishments that we're proud of What we learned Through this project, we learned the immense potential of AI-driven systems to facilitate meaningful connections between individuals. The ability of autonomous agents to understand and simulate human interaction signals a new paradigm in networking and relationship-building. We also gained valuable insights into prompt engineering, real-time AI processing, and the importance of seamless integration across the tech stack. What's next for Aligned.ai The potential for Aligned.ai is vast. Moving forward, we plan to expand the action space of personality embedding and vector based search, integrating more powerful features to enhance the matchmaking process. We aim to refine the AI's ability to simulate even more nuanced human interactions and to improve the accuracy of our personality embeddings. As we continue to develop Aligned.ai, our goal is to make it the go-to platform for building meaningful, long-term relationships in the startup ecosystem.",
    "matched_prize": "Cohere: Best Use of Cohere"
  },
  {
    "title": "APIcasso",
    "link": "https://devpost.com/software/apicasso",
    "text": "Inspiration No API? No problem! LLMs and AI have solidified the importance of text-based interaction -- APIcasso aims to harden this concept by simplifying the process of turning websites into structured APIs. What it does APIcasso has two primary functionalities: ✌️ Schema generation: users provide a website URL and an empty JSON schema, which is automatically filled and returned by the Cohere AI backend as a well-structured API. It also generates a permanent URL for accessing the completed schema, allowing for easy reference and integration. Automation: users provide a website URL and automation prompt, APIcasso returns an endpoint for the automation. For each JSON schema or automation requested, the user is prompted to pay for their token via ETH and Meta Mask. How we built it Challenges we ran into We initially struggled with clearly defining our goal -- this idea has a lot of exciting potential projects/functionalities associated with it. It was difficult to pick just two -- (1) schema generation and (2) automation. Accomplishments that we're proud of Being able to properly integrate the frontend and backend despite working separately throughout the majority of the weekend. Integrating ETH verification. Working with Cohere (a platform that all of us were new to). Functioning on limited sleep. What we learned We learned a lot about the intricacies of working with real-time schema generation, creating dynamic and interactive UIs, and managing async operations for seamless frontend-backend communication. What's next for APIcasso If given extra time, we would plan to extend APIcasso’s capabilities by adding support for more complex API structures, expanding language support, and offering deeper integrations with developer tools and cloud platforms to enhance usability.",
    "matched_prize": "'Cohere: Best Use of Cohere'"
  },
  {
    "title": "GooseOnTheLoose",
    "link": "https://devpost.com/software/goosehunt",
    "text": "Inspiration Amid the fast-paced rhythm of university life at Waterloo, one universal experience ties us all together: the geese. Whether you've encountered them on your way to class, been woken up by honking at 7 am, or spent your days trying to bypass flocks of geese during nesting season, the geese have established themselves as a central fixture of the Waterloo campus. How can we turn the staple bird of the university into a asset? Inspired by the quintessential role the geese play in campus life, we built an app to integrate our feather friends into our academic lives. Our app, Goose on the Loose allows you to take pictures of geese around the campus and turn them into your study buddies! Instead of being intimidated by the fowl fowl, we can now all be friends! What it does Goose on the Loose allows the user to \"capture\" geese across the Waterloo campus and beyond by snapping a photo using their phone camera. If there is a goose in the image, it is uniquely converted into a sprite added to the player's collection. Each goose has its own student profile and midterm grade. The more geese in a player's collection, the higher each goose's final grade becomes, as they are all study buddies who help one another. The home page also contains a map where the player can see their own location, as well as locations of nearby goose sightings. How we built it This project is made using Next.js with Typescript and TailwindCSS. The frontend was designed using Typescript React components and styled with TailwindCSS. MongoDB Atlas was used to store various data across our app, such as goose data and map data. We used the @React Google Maps library to integrate the Google maps display into our app. The player's location data is retrieved from the browser. Cohere was used to help generate names and quotations assigned to each goose. OpenAI was used for goose identification as well as converting the physical geese into sprites. All in all, we used a variety of different technologies to power our app, many of which we were beginners to. Challenges we ran into We were very unfamiliar with Cohere and found ourselves struggling to use some of its generative AI technologies at first. After playing around with it for a bit, we were able to get it to do what we wanted, and this saved us a lot of head pain. Another major challenge we underwent was getting the camera window to display properly on a smartphone. While it worked completely fine on computer, only a fraction of the window would be able to display on the phone and this really harmed the user experience in our app. After hours of struggle, debugging, and thinking, we were able to fix this problem and now our camera window is very functional and polished. One severely unexpected challenge we went through was one of our computers' files corrupting. This caused us HOURS of headache and we spent a lot of effort in trying to identify and rectify this problem. What made this problem worse was that we were at first using Microsoft VS Code Live Share with that computer happening to be the host. This was a major setback in our initial development timeline and we were absolutely relieved to figure out and finally solve this problem. A last minute issue that we discovered had to do with our Cohere API. Since the prompt did not always generate a response within the required bounds, looped it until it landed in the requirements. We fixed this by setting a max limit on the amount of tokens that could be used per response. One final issue that we ran into was the Google Maps API. For some reason, we kept running into a problem where the map would force its centre to be where the user was located, effectively prohibiting the user from being able to view other areas of the map. Accomplishments that we're proud of During this hacking period, we built long lasting relationships and an even more amazing project. There were many things throughout this event that were completely new to us: various APIs, frameworks, libraries, experiences; and most importantly: the sleep deprivation. We are extremely proud to have been able to construct, for the very first time, a mobile friendly website developed using Next.js, Typescript, and Tailwind. These were all entirely new to many of our team and we have learned a lot about full stack development throughout this weekend. We are also proud of our beautiful user interface. We were able to design extremely funny, punny, and visually appealing UIs, despite this being most of our's first time working with such things. Most importantly of all, we are proud of our perseverance; we never gave up throughout the entire hacking period, despite all of the challenges we faced, especially the stomach aches from staying up for two nights straight. This whole weekend has been an eye-opening experience, and has been one that will always live in our hearts and will remind us of why we should be proud of ourselves whenever we are working hard. What we learned What's next for GooseOnTheLoose In the future, we hope to implement more visually captivating transitional animations which will really enhance the UX of our app. Furthermore, we would like to add more features surrounding the geese, such as having a \"playground\" where the geese can interact with one another in a funny and entertaining way.",
    "matched_prize": "Best Use of MongoDB Atlas"
  },
  {
    "title": "StudySync",
    "link": "https://devpost.com/software/studying-with-hack-the-north",
    "text": "Inspiration Our project was inspired by people who don't learn the \"old school\" way. Some people learn best by reciting things out loud, but the problem with that is sometimes it's too quiet to talk, and sometimes it's too loud to talk. What it does Our project is a website that allows you to create a group with your classmates/coworkers, in which you can record video of yourself silently mouthing out whatever lecture or book you're reading, allowing you to pay full attention without worrying about writing notes, a loud environment, or an environment in which you're not allowed to talk. Our project reads your lips and puts it into text, creating notes for the video you take. Then, with your notes, we summarize your key points and build a quiz so you can focus on the important parts of your work. How we built it For the front end of our website, we used React JS and Chakra. We used Auth0 to create an account system for our project. We used Symphonic Labs' API to read your lips and convert your silent words to text. We used MongoDB to save all your notes to a database in your group. We then used a Cohere API to summarize the key points of the text and make a quiz out of it. We also used an Open AI API to convert mp3 files to text, in case you wanted to record a lecturer or friend speaking, and then convert it to notes and quizzes. Challenges we ran into We tried to get live video transcribing with Symphonic Lab's API using Open CV, but since Open CV was giving us frames of the video, Symphonic Lab's API wasn't able to transcribe it. Instead, we switched to allowing users to record their videos on the site, immediately download them and upload it for transcribing. Accomplishments that we're proud of We are very proud of combining all the complicated API's to create a complete and flowing product, something that all of us will definitely be using for school. What we learned This was our group's first time using more than one API for a project, but it ended up with a website that performs a useful and complicated function, and we will be using multiple APIs in the future! What's next for Studying with Hack the North",
    "matched_prize": "'Best Use of Symphonic Labs API'"
  },
  {
    "title": "MoneyMoves",
    "link": "https://devpost.com/software/moneymoves",
    "text": "Inspiration In today's fast-paced world, the average person often finds it challenging to keep up with the constant flow of news and financial updates. With demanding schedules and numerous responsibilities, many individuals simply don't have the time to sift through countless news articles and financial reports to stay informed about stock market trends. Despite this, they still desire a way to quickly grasp which stocks are performing well and make informed investment decisions. Moreover, the sheer volume of news articles, financial analyses and market updates is overwhelming. For most people finding the time to read through and interpret this information is not feasible. Recognizing this challenge, there is a growing need for solutions that distill complex financial information into actionable insights. Our solution addresses this need by leveraging advanced technology to provide streamlined financial insights. Through web scraping, sentiment analysis, and intelligent data processing we can condense vast amounts of news data into key metrics and trends to deliver a clear picture of which stocks are performing well. Traditional financial systems often exclude marginalized communities due to barriers such as lack of information. We envision a solution that bridges this gap by integrating advanced technologies with a deep commitment to inclusivity. What it does This website automatically scrapes news articles from the domain of the user's choosing to gather the latests updates and reports on various companies. It scans the collected articles to identify mentions of the top 100 companies. This allows users to focus on high-profile stocks that are relevant to major market indices. Each article or sentence mentioning a company is analyzed for sentiment using advanced sentiment analysis tools. This determines whether the sentiment is positive, negative, or neutral. Based on the sentiment scores, the platform generates recommendations for potential stock actions such as buying, selling, or holding. How we built it Our platform was developed using a combination of robust technologies and tools. Express served as the backbone of our backend server. Next.js was used to enable server-side rendering and routing. We used React to build the dynamic frontend. Our scraping was done with beautiful-soup. For our sentiment analysis we used TensorFlow, Pandas and NumPy. Challenges we ran into The original dataset we intended to use for training our model was too small to provide meaningful results so we had to pivot and search for a more substantial alternative. However, the different formats of available datasets made this adjustment more complex. Also, designing a user interface that was aesthetically pleasing proved to be challenging and we worked diligently to refine the design, balancing usability with visual appeal. Accomplishments that we're proud of We are proud to have successfully developed and deployed a project that leverages web scrapping and sentiment analysis to provide real-time, actionable insights into stock performances. Our solution simplifies complex financial data, making it accessible to users with varying levels of expertise. We are proud to offer a solution that delivers real-time insights and empowers users to stay informed and make confident investment decisions. We are also proud to have designed an intuitive and user-friendly interface that caters to busy individuals. It was our team's first time training a model and performing sentiment analysis and we are satisfied with the result. As a team of 3, we are pleased to have developed our project in just 32 hours. What we learned We learned how to effectively integrate various technologies and acquired skills in applying machine learning techniques, specifically sentiment analysis. We also honed our ability to develop and deploy a functional platform quickly. What's next for MoneyMoves As we continue to enhance our financial tech platform, we're focusing on several key improvements. First, we plan to introduce an account system that will allow users to create personal accounts, view their past searches, and cache frequently visited websites. Second, we aim to integrate our platform with a stock trading API to enable users to buy stocks directly through the interface. This integration will facilitate real-time stock transactions and allow users to act on insights and make transactions in one unified platform. Finally, we plan to incorporate educational components into our platform which could include interactive tutorials, and accessible resources.",
    "matched_prize": "None"
  },
  {
    "title": "Skip the Walk",
    "link": "https://devpost.com/software/skip-the-walk",
    "text": "Inspiration Old school bosses don't want want to see you slacking off and always expect you to be all movie hacker in the terminal 24/7. As professional slackers, we also need our fair share of coffee and snacks. We initially wanted to create a terminal app to order Starbucks and deliver it to the E7 front desk. Then bribe a volunteer to bring it up using directions from Mappedin. It turned out that it's quite hard to reverse engineer Starbucks. Thus, we tried UberEats, which was even worse. After exploring bubble tea, cafes, and even Lazeez, we decided to order pizza instead. Because if we're suffering, might as well suffer in a food coma. What it does Skip the Walk brings food right to your table with the help of volunteers. In exchange for not taking a single step, volunteers are paid in what we like to call bribes. These can be the swag hackers received, food, money, How we built it We used commander.js to create the command-line interface, Next.js to run MappedIn, and Vercel to host our API endpoints and frontend. We integrated a few Slack APIs to create the Slack bot. To actually order the pizzas, we employed Terraform. Challenges we ran into Our initial idea was to order coffee through a command line, but we soon realized there weren’t suitable APIs for that. When we tried manually sending POST requests to Starbucks’ website, we ran into reCaptcha issues. After examining many companies’ websites and nearly ordering three pizzas from Domino’s by accident, we found ourselves back at square one—three times. By the time we settled on our final project, we had only nine hours left. Accomplishments that we're proud of Despite these challenges, we’re proud that we managed to get a proof of concept up and running with a CLI, backend API, frontend map, and a Slack bot in less than nine hours. This achievement highlights our ability to adapt quickly and work efficiently under pressure. What we learned Through this experience, we learned that planning is crucial, especially when working within the tight timeframe of a hackathon. Flexibility and quick decision-making are essential when initial plans don’t work out, and being able to pivot effectively can make all the difference. Terraform We used Terraform this weekend for ordering Domino's. We had many close calls and actually did accidentally order once, but luckily we got that cancelled. We created a Node.JS app that we created Terraform files for to run. We also used Terraform to order Domino's using template .tf files. Finally, we used TF to deploy our map on Render. We always thought it funny to use infrastructure as code to do something other than pure infrastructure. Gotta eat too! Mappedin Mappedin was an impressive tool to work with. Its documentation was clear and easy to follow, and the product itself was highly polished. We leveraged its room labeling and pathfinding capabilities to help volunteers efficiently deliver pizzas to hungry hackers with accuracy and ease. What's next for Skip the Walk We plan to enhance the CLI features by adding options such as reordering, randomizing orders, and providing tips for volunteers. These improvements aim to enrich the user experience and make the platform more engaging for both hackers and volunteers.",
    "matched_prize": "Best Use of Terraform"
  },
  {
    "title": "getBounced",
    "link": "https://devpost.com/software/getbounced",
    "text": "Inspiration We have all had our fair share of hosting gatherings with unwanted guests. We are tired of birthday parties and wedding crashers, causing unnecessary clean-up and drama! We are also broke college students who cannot afford to hire a bouncer and not everybody has a 6’7 friend. Say hello to getBounced - the automated bouncer bot to suit your security needs! Our product mounts easily to any entranceway and uses facial recognition to decide if a guest is allowed through or if they’ll receive a disciplinary smack! What it does Before the event, the host will input event information such as the name, location, and start and end time. This information will generate a link to a form the guests will use to register. In addition to providing their contact information, the guests will be prompted to take a 10-second clip of their faces. These clips will be fed to train our computer vision model. On the day of the event, our smart camera will be set up at the entrance of the event and it will determine if the person is a homie or a nobody. If the latter, then they’ll getBounced! A swift and demure sweep of the blade (trust us, it’s safe) and the alerts that the smart camera will fire will, hopefully, deter the uninvited away. How we built it The webapp was built with typescript and sandboxed with expo. We also made a quick backend in Flask, which helps us deal with the controller logic to preprocess image data, while enabling us to gather analytics on the behaviours of our party attendants, like when they arrive, leave, how long they've stayed at the party, and who they've interacted with. We leveraged several machine learning algorithms to segment and predict the identities of those that show up to our front door. Using mtcnn, we were able to isolate the faces from other noise in images, while using FaceNet (by Google) to generate embedding vectors of said faces. Finally, we trained an SVM and Logistic Regression model (choosing the latter at the end due to increased performance) to classify the vectors into users. We were able to use data preprocessing and image handling algorithms to reduce latency in facial analysis and recognition! For the hardware component, we used a Jetson Nano to run our machine learning and computer vision algorithms, getting live guest video data from a Tapo Wifi Camera. We then connected the Jetson to an Arduino, which is used to control the servo motor upon which our deterrent weapon is mounted. The Jetson sends a signal through serial communication to the Arduino whenever an intruder is detected. Challenges we ran into One challenge we ran into was our lack of initial hardware materials such as power supplies and mounting equipment. This led to many wild goose chases around campus and visits to the hackathon hardware desk begging them to “check if they have any extras in the back”. Eventually though, after a lot of phone calls and a lot of kind souls willing to lend us parts from their collection, we were able to source everything we needed for our amazing bot. Another challenge we ran into was not being able to control the stepper and servo motors properly using the Jetson Nano. This is because we were trying to use GPIO pins to do PWM control, which they are not meant to do. This led to a lot of wacky behaviour from our motors. To solve this problem, we decided to use an Arduino to control our servo motor, as there are many servo libraries built for Arduino. Accomplishments that we're proud of The biggest thing we are proud of is how we were able to integrate many different fields and skills in engineering together to create a cool and useful product! We were able to integrate computer vision with machine learning and IoT embedded systems motors and app development and even hot glue and cardboard! What we learned On the hardware side, we learned that it is best practice to use specially made microcontrollers to control electronics, especially power ones like motors. This way, our main system on a chip can be used primarily for algorithm running and communication. What's next for getBounced One next step would be to create “hot-swappable” punishments for unwanted guests. Currently, we only have a sword-smacking mechanism but in the future, we would elevate our product to include other options such as water gun spraying, trap door, ice bucket dump, and many more devious things. Another next step would be to connect our product to a mobile app which can notify the host in real-time whenever any unwanted guests arrive and capture video evidence of their embarrassing downfall!",
    "matched_prize": "None"
  },
  {
    "title": "Heal Us",
    "link": "https://devpost.com/software/heal-us-al4zhu",
    "text": "Inspiration Have you ever felt numb excruciating pain gasping for air but deciding...dying by suicide is better? The essence of this app comes from my lived experiences of depression and healing. We often use coping mechanisms to deal with pain and seeking peace and love outwardly is too common. This results in unhealed traumas seeping into relationships of all kind affecting life and future generations through generational trauma What it does Heal Us provides the ultimate love of your life beyond lifetimes, YOU! All versions of you, child you, teen you, young adult you, and adult you. You will have opportunities to learn to love and heal as we aren't taught these skills in some environments we grow up in. There are spaces like the Journals, therapy, date calendar, inner child rewards, self love languages profiles, that help along your healing journey! You will get to converse with your various versions of yourself through prompted messages. Your love match will be a check-in every week to see how you are doing and where you need to fill the love cup. You can earn self points by interacting with writing journals, reflecting, creating dates with your matches. Those points can be used to redeem free therapy sessions. You will be given 5 free complimentary therapy sessions and as you progress you will heal and learn to love in healthy ways! How I built it Adalo Challenges I ran into Did this solo with no tech background thankfully got help from mentors Accomplishments that I'm proud of Creating this in hours and adapting to changes along the way. What I learned Learned alotta cool things from mentors as well as connected to my inner child through making this What's next for Heal Us Creating an app with a developer and helping others heal trauma from the root!",
    "matched_prize": null
  },
  {
    "title": "Ramsey",
    "link": "https://devpost.com/software/ramsey",
    "text": "Inspiration For many college students, university life often means living alone and cooking for themselves for the first time. We've all experienced those chaotic moments in the kitchen—burnt meals, missed steps, or fumbling with our phones to follow a recipe video. The learning curve is real, and the frustration is common. That's why we created Ramsay, an AI cooking assistant designed to make cooking easier and more enjoyable. With Ramsay, users can play, pause, or navigate YouTube cooking videos using voice commands, eliminating the need to rush back and forth between the kitchen and their devices. This hands-free experience allows users to cook along seamlessly with the video, making it feel as if they're working with a real assistant who’s there to guide them. Additionally, Ramsay intelligently analyzes each video’s cooking time, ingredients, and calorie count, helping students make informed decisions on which recipe fits their needs, whether it’s for a quick meal or a more balanced option. By streamlining the cooking process and offering personalized insights, Ramsay takes the stress out of cooking, allowing students to enjoy the experience with confidence. What it does Voice-Controlled Video Navigation: Ramsay allows users to control YouTube cooking videos with simple voice commands. You can play, pause, rewind, or skip forward in a recipe video without having to touch your phone or computer—perfect for when your hands are busy or messy in the kitchen. Smart Recipe Analysis: Ramsay analyzes each recipe video for important details like cooking time, ingredient lists, and calorie counts. This helps users quickly find meals that suit their time constraints, dietary preferences, and nutritional needs. Personalized Cooking Assistance: By combining voice control with real-time video navigation, Ramsay creates an interactive and personalized cooking experience. It feels like having a real assistant by your side, guiding you through every step of the recipe, so you never miss an important instruction. How we built it Frontend: Our journey with the frontend began with designing in Figma, where we explored various design resources from Dribbble and Figma to better understand industry standards and our target audience. We carefully selected our color palettes and decided on our tech stack, opting for Node.js and TypeScript. This choice enabled us to customize, animate, and structure our application effectively, ensuring a cohesive and readable design. Key features of our frontend include: Opinionated Recipe Retriever: Delivers the most delicious recipes tailored to user preferences. Ramsay: Our voice-activated assistant that controls media features and interacts directly with users during their cooking sessions. Situation-Dependent Timers: Allows users to control the pacing of their cooking session, providing precise timing for optimal culinary results. We prioritized accessibility, aiming to create a website that is inclusive and easy to use for everyone, regardless of their situation or location. Backend: We developed three primary endpoints to support our frontend: GET api/search/?query=query: Handles searching, analyzing, and presenting YouTube videos based on user queries. This endpoint uses external APIs to retrieve related search results, fetch transcript objects with timestamps, and leverage Groq’s API to engineer and analyze transcripts, extracting key details like ingredients, cooking time, and calorie count. GET api/video?url=url: Retrieves detailed information about a YouTube video through its URL. This endpoint analyzes the video’s content to suggest when to set a timer, and communicates this information to the frontend to prompt users to set timers at specific points in the video. POST api/audio/: Accepts audio byte streams and converts them into .wav files. Groq is then used to transcribe the audio into text, followed by another Groq request to intelligently select the best recipe option for the user. These backend services work seamlessly with the frontend to provide a smooth, interactive cooking experience. Challenges we ran into One of the biggest challenges we faced during development was managing all the moving parts, especially when it came to integrating the sponsors and APIs we wanted to work with. A core hurdle was handling the YouTube iFrame API and ensuring smooth synchronization between the voice commands issued to Ramsay and the actual video interactions. Making sure that commands like play, pause, and rewind were executed seamlessly was a complex task. Another significant obstacle was the APIs themselves. Many of the ones we wanted to use either lacked a free tier or had performance issues, which slowed down our development process. We had to explore multiple alternatives, and even when we found workable solutions, we frequently hit rate limits due to the restrictions of the free versions. And finally, the broken charging ports on floor 4. Many devices have fallen victim to empty charging stations, but we ultimately persevered! Accomplishments that we're proud of Implementing an idea within the last 16 hours of the hackathon, a fully-loaded voice to text to direct action application in improving ease of accessibility. This is a stepping stone towards creating more interactive assistants to improve the quality of life for students, developers and anyone who consumes media! What we learned Throughout this project, we learned the importance of thoroughly reading and understanding documentation—not just skimming, but diving deep to grasp the finer details. This allowed us to better navigate the technical challenges and make smarter decisions. We also learned to stay flexible and open-minded when exploring new APIs and technologies. Adaptability became key as we encountered obstacles, pushing us to experiment with different tools and solutions to get the job done. From a technical perspective, we gained valuable experience in efficiently sending audio bytes over HTTP, which improved the overall performance and responsiveness of Ramsay’s voice-command system. What's equally important was learning to collaborate effectively as a team. We worked closely together, discussing solutions, sharing ideas, and syncing regularly to ensure we stayed aligned and could tackle challenges head-on as a unit. This project not only helped us grow technically but also taught us the importance of teamwork and communication. What's next for Ramsey Ramsey has come a long way solely from Hack the North but has even further to go. We want to build upon this idea in the cooking industry and reach audience bases further than beginners in the kitchen and college students. We aim to add more features that allow users to ask more intricate questions in the kitchen and to identify the ingredients being used and automatically advise them. We plan to add more filters, support for different languages Outside of our primary audience base, we have plans to implement our product into people’s lifestyles. Watching any video can become so much easier with the help of Ramsey to help you out pausing, rewinding, taking notes, sending messages, switching tabs, and more.",
    "matched_prize": null
  },
  {
    "title": "Hack The Resume",
    "link": "https://devpost.com/software/hack-the-resume",
    "text": "Inspiration It can feel impossible to get a job in today's job market. We wanted to develop a solution that would help people get past the first and usually most difficult step in their job hunt: getting the interview. With this in mind, we decided that we should find a way to provide suggestions to a resume in order to improve it and increase the chances of it being seen by an employer. What it does Our app takes in the user's resume either in the form of a PDF or TXT file and will produce a resulting file with generated suggestions for improvements. How we built it We made use of React Vite, TypeScript and Tailwind-CSS for the frontend elements and styling. The functionality of the app was done using TypeScript and we used Cohere in order to generate suggestions for the user's resume. Challenges we ran into Being a group of 4 new hackers, almost every part of the development was a challenge. Learning new technologies on the fly meant that we did not have the experience to solve common or simple bugs right away. This applied to the frontend and functional components of the project. Accomplishments that we're proud of We are incredibly proud of building a functional app especially as we are all pretty much beginners to hackathons and web app development. We are proud of our nice looking frontend and that we were able to integrate Cohere's API to generate our suggestions. What we learned We learned a lot about web development as a whole. For some of us, this was our first time using React and TypeScript so there was a lot to learn. We also learned about deployment with Vercel and the difficulties of integrating APIs and trying to build a backend. What's next for Hack The Resume In the future, the main addition we would like to make is the inclusion an ATS api in order to score resumes. Further changes we would like to implement include displaying the user's current resume and suggestions on the website.",
    "matched_prize": null
  },
  {
    "title": "Duck Chrome Game",
    "link": "https://devpost.com/software/duck-chrome-game",
    "text": "Built With",
    "matched_prize": null
  },
  {
    "title": "Linguist",
    "link": "https://devpost.com/software/linguist-c35a61",
    "text": "Inspiration We we wanted to make learning and understanding each other more equitable and fair. Thus we decided to solve a problem that exists everywhere. What it does How we built it React/Vite + Tailwind + Convex Frontend Convex Backend Convex Database Convex API Symphonic Labs Video to Text Chroma RAG Cohere Multilingual LLM Challenges we ran into The main Challenge that we all ran into was fitting our experiences and intuition from unopinionated technologies onto convex, which was very opinionated. We struggled in learning and fitting our existing knowledge (JS, NodeJS), onto the opinionated format, which slowed us down significantly. We alos struggled to host the service on AWS and Azure.## Accomplishments that we're proud of Accomplishments We were able to get a end-end user case working, from uploading the video, to getting the transcript with Symphonic and using the Cohere LLM chatbot. What we learned We learned a ton about effective system design, full stack info What's next for Linguist We hope to build for more users in the future, and add more features",
    "matched_prize": null
  },
  {
    "title": "ErosLink",
    "link": "https://devpost.com/software/eroslink",
    "text": "An AI wingman to reimagine your love, one text at a time. Inspiration 🏹 ErosLink - the Greek god of love combined with tech - now coming to you as an AI wingman! Modern dating platforms focus only on appearances (Tinder) or require you to read long, over-curated profile descriptions (Hinge). While differences tend to attract, long-term compatibility comes down to the people’s similarities. At ErosLink, we wanted to be the platform that combines the passion of Eros with the connectivity of modern technology! What it does 💘 tl;dr: ErosLink connects you with a match with respect to your inputted info; then, you enter a chat with an AI-moderator (aka wingman) to help you and your potential partner's conversation flow well : ) ErosLink first asks you for your info i.e. interests, favourite movies, etc. Then, it uses graph theory to identify similarities between you and a potential partner and links you to a chat. From there the surprise begins… I.e. David wants to find some love and he starts using ErosLink David has entered the chat. David: Hiiii! Shrad: Hey! David: How you doing? Shrad: I’m ok. Yeah. David: Yeah. Bot: Psst. It’s kinda awkward… fun fact: both of y’all love fencing! Shrad: Oh really?? David: Yup! I’ve been doing it since 5 as a sabre! I think it’s rly fun cuz … You get it. Our AI moderated, powered by VoiceFlow and Groq LLM, creates conversation starters / fun facts based on your and your potential partners’ inputs at the start. That way users can more easily open up with each other and perhaps find a match! How we built it 💌 Challenges we ran into 🥺 We pivoted our idea at 2 pm yesterday, after a fuel-filled 14 hours of working on a different project. Despite our limited experience, we managed to entirely change our project. Accomplishments that we're proud of 🥰 We came in and finished our project! (Fuelled by adrenaline and coffee) What we learned 🫰 What's next for ErosLink 💕",
    "matched_prize": null
  },
  {
    "title": "TraFFIX",
    "link": "https://devpost.com/software/traffix-sva85n",
    "text": "GoDaddy Domain User Website: https://thetraffixperiment.info/ Inspiration Our team was inspired by the noticeable yet unsolved modern challenge faced by millions of regular people everyday: Inefficient traffic. The more time we spend stalled on the road, exhaust fumes piling into the sky, the greater our digital blueprint and the worse our mood. Traffic signals are usually moderated by hard-coded interval times, which is not always intuitive; how many times have you driven to an empty intersection, only to be stopped by a red light despite the complete and absolute absence of cars on the road perpendicular? How much time could you save if you could just fly through those empty intersections (without breaking the law, of course!) That is how TraFFIX was born. TraFFIX employs image recognition AI to count the number of cars present at an intersection. An AI model detects and labels different objects on a screen, as well as quantifies the number visible on the screen. This data can then be analyzed and the traffic light intervals are thus adjusted. To test on a real intersection was not feasible; unfortunately, as regular University and High-school students, we did not have access to the traffic light control systems. As such, we created a miniature intersection model in order to demonstrate and execute our idea. Traffic light templates were 3D printed and LEDs were placed in the holes. An arduino was used and programmed to control each of the lights and the duration for which they were turned on. Like aforementioned, we did not have access to a real life intersection to test and develop our idea. We solved this by creating a miniature model. Integrating the AI model and extracting and analyzing the data was a challenging task that we overcame and succeeded in. Traffix leverages a convolutional neural network (CNN) trained to detect vehicles, pedestrians, and cyclists using image detection from cameras installed at intersections. What we learned We gained experience and skills in working with AI databases as well as data analysis and management. Additionally, we explored the complexity of real world problems; the variables to consider in a real-time real life road situation was astounding. We learned to navigate these, managing what we could, and distributing tasks. Moving forward, TraFFIX will work on making even more clever systems, and traffic lights that communicate with each other: we will begin engineering better ROADS, not just intersections.",
    "matched_prize": null
  },
  {
    "title": "MinGO",
    "link": "https://devpost.com/software/mingo-ua6mey",
    "text": "Inspiration MinGO aims to enhance the attendee experience at large events, particularly in the tech and cybersecurity industries, by leveraging AI-powered tools and gamification elements. The goal is to provide a more engaging, informative, and personalized experience, while also making it easier for attendees to navigate chaotic spaces and retain connections they make. What it does The web application provides a user-friendly interface to help event attendees explore indoor or outdoor venues, manage and maintain personal interactions, and track points earned through engagement and networking activities. How we built it MinGO is built using a React and TailwindCSS frontend. We used Cohere's API: Generate to help summarize conversations and provide contextually relevant insights as well as Voiceflow agents for our chatbot! Challenges we ran into It was definitely difficult to manage our time and decide which features we should prioritize. We all worked with unfamiliar technologies through this project. Accomplishments that we're proud of We are happy to present a finished product that we would make use of. We find projects more meaningful and enjoyable when we know they can encourage better experiences, which is what we hope MinGO can achieve. What we learned We learned the process of developing a product from brainstorming to deployment. What's next for MinGO We had a lot of fun enhancements and big ideas for MinGO, that we definitely want to explore someday. We love the idea of allowing participants to connect and get to know each other with a simple tap of a badge. We also wanted to enhance the gamification aspects of the application with the following:",
    "matched_prize": null
  },
  {
    "title": "SymphonicSlates",
    "link": "https://devpost.com/software/symphonicslates",
    "text": "Inspiration Music has the power to evoke deep emotions, and we wanted to translate that emotional depth into visual art. SymphonicSlates bridges the gap between auditory and visual experiences, offering users a unique never-before-seen way to see the music they love. What it does SymphonicSlates takes any Spotify track and generates an image that captures its underlying emotional tone. It does this by analyzing the track’s hidden semantic meaning. Unlike anything currently available on the market, our product moves beyond basic audio analysis, offering a unique \"Audio2Image\" experience. How we built it SymphonicSlates was trained on the Moodify dataset, which contains a 278 thousand dataset mapping data about tracks such as \"Tempo\", \"Energy\", \"Livelyness\" to 1 of four moods: Happy, Sad, Angry, and Relaxed. Finally, the extracted mood is then combined with other metrics to be fed into an image diffusion model. To streamline the data flow, Convex was used to create and deploy a robust back-end database, allowing us to store, query, and manage both the music features and image generation outputs efficiently. Challenges we ran into One of our primary challenges was cleaning and normalizing the Moodify dataset, which had inconsistent ranges for musical features. After thorough research, we found a GitHub project that offered insights into solving these issues. Another major hurdle came during deployment. The Spotify API’s browser-based authentication caused complications when we tried to integrate our app into Hugging Face, which interacts with Convex. We had to implement creative workarounds to ensure seamless Spotify integration. Accomplishments that we're proud of We’re proud of creating a truly unique solution, as no similar \"Audio2Image\" products currently exist on the market. A thorough search on Google revealed that SymphonicSlates stands out by translating audio tracks into visual art, filling a gap in both the music and visual tech space. In addition, we successfully integrated machine learning models, music feature extraction, and image generation into a cohesive system. Overcoming the challenges with the Spotify API and deploying the app on Hugging Face was also a major achievement for the team. Convex played a vital role in ensuring the project ran smoothly by managing data flow seamlessly. What we learned Throughout this project, we learned the importance of flexibility when working with external APIs like Spotify. We also gained hands-on experience with Convex for back-end management, deepened our understanding of music feature extraction, and sharpened our skills in deploying machine learning models. Most importantly, we learned how to rapidly pivot and adjust to unexpected roadblocks, making us better prepared for future challenges. What's next for SymphonicSlates We plan to expand SymphonicSlates by incorporating additional music genres and moods, further enhancing the model's ability to capture subtle nuances in audio. We're also exploring real-time audio-to-image generation and expanding our use of Convex for even more advanced data handling needs.",
    "matched_prize": null
  },
  {
    "title": "MyAvion",
    "link": "https://devpost.com/software/myavion",
    "text": "Inspiration As Avion Members, we all agreed on a topic that fulfilled our wishes regarding what we thought the platform could improve at. What it does This app leverages location services to display nearby deals, better integrating the customer in the world of Avion Rewards. Furthermore, it uses a nomination system to allow users and nearby businesses mutual benefit in using the program. How we built it We built this with Swift, Swift UI, Firebase, the RBC API, and lots of love Challenges we ran into We realized that in order to create the functionalities we were aiming for, the architecture became way more complex than expected, and table relations became hard to manage in the short amount of time Accomplishments that we're proud of This was out first time doing a hackathon together and we made a project that was way more intricate than we were expecting What we learned We learned a lot about database structuring, using REST APIs, UI/UX design and general coding concepts What's next for MyAvion We want to add purchase intelligence, to understand customers financial habits as they go through life phases. By analyzing customer transaction data, we want to provide exclusive point offers to customers.",
    "matched_prize": null
  },
  {
    "title": "StopIt!",
    "link": "https://devpost.com/software/stopit-7o15k6",
    "text": "Inspiration I have seen people around me, including myself, fall prey to minor negative behaviors because we think it's not a big problem. In reality, the effects compound, and your reinforcement continues to overpower your efforts to overcome it. One personal bad habit that I picked up was pulling my mustache, which is essentially very similar to biting your nails. Back When I used to live with my family I would always find my sister biting her nails and it somewhat disgusted me. Funny enough, a few years later, I started playing with and pulling my mustache and it continued to get worse. Now it is wasting so much of my time and I couldn't find any way to overcome it other than building this app What it does Empower users to be more mindful of their actions by having their laptop camera monitor them and alert them when they commit a bad habit/addiction they have configured in the web app. How I built it With the motivation of helping myself. Saving me around 30 minutes every day. I also believe it's a pain point in other people's lives as well. Used convex as a backend to store user's data and to enable the ability to have friends to expose your bad habits to. Implemented several 'Actions' that I used for several external APIs that I used like cohere, Elevenlabs, and resend. I also found the process very smooth with adding an auth system with Convex Auth. I'm a big fan of tRPC so I'm glad y'all took inspiration from them and made Convex also end-to-end type-safe. Challenges I ran into Convex's way of doing things. Nothing can beat a stack you've already worked with before. Especially if you want speed. Accomplishments that I'm proud of Unfortunately, I worked solo but managed to get into the flow, got a lot done, and enjoyed myself in that process. What I learned Image Recognition in the Web Convex backend The value of having teammates in hackathons. Makes it a whole another experience compared to hacking solo. What's next for StopIt! You can follow other people who you trust them in knowing your bad habits A team of habit breakers who put some money in. Whoever loses will have to give out the money among the other breakers Gamifying",
    "matched_prize": null
  },
  {
    "title": "Froque",
    "link": "https://devpost.com/software/froque",
    "text": "Inspiration The inspiration behind it was my own experiences. Coming from a first-generation student family, I was always obligated to find any discounts or free opportunities to capitalize on. However, these were not always available easily off the internet. A tool like Froque could help many individuals questioning to spend on something slightly over budget, for it to now become more managable. What it does Essentially, any activities or discounts are written on the app and linked to to that website in order to register. How we built it Using SwiftUI on XCode for the GUI, along with Firebase for the Authentication and database storage in order to update the app in realtime, effectively. Challenges we ran into The biggest challenge would have been connecting Firebase to the app. This required quite a bit of scrolling from the documentation, with some outdated versions providing incorrect code. Accomplishments that we're proud of Being a solo group, this was my very first app on SwiftUI and I am extremely excited to share this with you. What we learned Linking Firebase, force unwrapping vs. using guard statements, safe coding practices, designing, What's next for Froque I have so many ideas planned for Forque. Hopefully, using an AI model, I could easily update the discounts/extra-curricular activities within the area a lot easier. Not only that, but tailoring specific activities based on location would be an amazing idea.",
    "matched_prize": null
  },
  {
    "title": "SherlockMediBot",
    "link": "https://devpost.com/software/sherlockmedibot",
    "text": "Inspiration Oscar is from Mexico, where there is rampant misinformation online about certain diseases, medicines, and a lot of social media influencers who claim ridiculous health facts and give advice to people. We wanted to reduce the harm of misinformation through a fun threads bot. What it does A threads user can reply to a thread and @ our bot account to have it look through the post, and respond with sources that either support or disagree with the content. How we built it We used cohere's api and prompt engineered the chat bot to reply to posts and provide reliable sources from WHO, HHS, nutrition.gov, nih.gov, and more. Challenges we ran into Threads has a 500 character limit, so we had to find a workaround to provide more information. As a two person team without much front-end experience, we found it difficult to setup a nice looking frontend for the bot's replies to be hosted. What we learnt Learned more about LLM's and cohere's api, learned more about prompt engineering. More experience working with UI and web scraping.",
    "matched_prize": null
  },
  {
    "title": "VerbalFlow",
    "link": "https://devpost.com/software/verbalflow",
    "text": "Inspiration As undergrad and high school students, we have to go through various interview processes for internships and CO-OPs. We would also all benefit greatly from polishing our public speaking and communication skills. So we built a tool to help you become a better speaker. What it does Verbalflow gives its users tailored feedback on their communication skills using various APIs. It collects data about the user's face using vgg-face and facenet APIs, and it also records the user's word choice and tone. It then feeds all of the data it collected into groq's API to create it's feedback. How we built it We built Verbalflow over 36 hours using various languages and APIs such as Python and Flask for the back-end, and HTML and CSS for the front-end. We used Flask to host a web server and our files on the frontend, and HTTPS requests to send the data that was gathered from the frontend to the backend. After receiving the data in the backend, Groq analyzes the data and curates feedback based on the information. Challenges we ran into While building the speech recognition part of our project we ran into audio corruption problems; every time we tried to receive audio data from the frontend it would always be unusable. We eventually, did some digging and found out that Google didn't support the file type that we were trying to send. But when it did work, we started getting memory and pointer-related issues on top of more audio corruption. Accomplishments that we're proud of In a short demo, we were able to have it successfully gather data on my speech, tone and facial language, and provide feedback. What we learned We learned many new languages since most of us had little experience with the tech stack we worked with such as Flask and Javascript. On top of that, we learned that sometimes bugs can be things, you would never have suspected to root of the problem, but with a lot of time and research, it's possible to find any bugs. What's next for VerbalFlow In the future, we are thinking of implementing other methods of data collection to make the feedback it gives even better, such as retina tracking, gesture, and posture tracking. We are also going to improve the feedback it gives and make it feel more personalized and human.",
    "matched_prize": null
  },
  {
    "title": "Hazard",
    "link": "https://devpost.com/software/hazard-zygkq9",
    "text": "Built With",
    "matched_prize": null
  },
  {
    "title": "NOTFA (Not Financial Advice)",
    "link": "https://devpost.com/software/notfa-not-financial-advice",
    "text": "Background Before we don't give you financial advice, let's go through some brief history on the financial advisors and the changes they've seen since their introduction. Financial advisors have been an essential part of the financial world for decades, offering individuals tailored advice on everything from investments to retirement plans. Traditionally, advisors would assess a client's financial situation and suggest investment strategies or products, charging a fee for their services. In Canada, these fees often range from 1% to 2% of a client's assets under management (AUM) annually. For example, if a client had $500,000 invested, they could be paying $5,000 to $10,000 a year in advisor fees. However, over the past two decades, consumers have been migrating away from traditional financial advisors toward lower-cost alternatives like Exchange-Traded Funds (ETFs) and robo-advisors. ETFs, which are passively managed and track indexes like the S&P 500, became popular because they offer diversification at a fraction of the cost—typically charging less than 0.5% in fees. This shift is part of a broader trend toward fee transparency, where investors demand to know exactly what they're paying for and opt for lower-cost options when they can. But while ETFs offer cost savings, they come with their own set of risks. For one, passive investing removes the active decision-making of traditional advisors, which can lead to market-wide issues. In times of high volatility, ETFs can exacerbate market instability because of their algorithmic trading patterns and herd-like behaviours. Furthermore, ETFs don't account for an investor's specific financial goals or risk tolerance, which is where human advisors can still play a critical role. Understanding this transition helps illustrate why a tool like NFA (Not Financial Advice) can fill the gap—offering insights into personal finances without the high fees or potential drawbacks of fully passive investing or even the requirements to invest! Whether it be an individual who is looking to optimize their existing investments, or one who simply wants to learn about what their options are to begin with, NFA is a platform for all! Inspiration The inspiration for NFA came from recognizing a gap in the market for accessible, personalized financial insights. Traditional financial advice is often expensive and not readily available to everyone. We wanted to create a platform that could analyze a user's financial situation and provide valuable insights without crossing the line into regulated financial advice. The rise of fintech and the increasing financial literacy needs of younger generations also played a role in inspiring this project. We saw an opportunity to leverage technology to empower individuals to make more informed financial decisions. Team Background and Individual Inspiration Our diverse team brings a unique blend of experiences and motivations to the NFA project: Cole Dermott: A 21-year-old fourth-year student at the University of Waterloo (Computer Science) and Wilfrid Laurier University (Business Administration). With experience in various software fields, Cole's business background and frequent interactions with financial news and peer inquiries inspired him to develop a tool that could provide quick financial insights. Daniel Martinez: A grade 12 student from Vaughan, Ontario, with experience in fullstack development including mobile, web, and web3. As a young startup founder (GradeAssist), Daniel has faced challenges navigating the financial world. These systemic and information barriers motivated him to join the NFA team and create a solution for others facing similar challenges. Musa Aqeel: A second-year university student working full-time as a fullstack developer at Dayforce. Musa's personal goal of setting himself up for an early retirement drove him to develop a tool that would help him truly understand his finances in-depth and make informed decisions. Alex Starosta: A second-year Software Engineering student at the University of Waterloo. Alex's meticulous approach to personal finance, including constant budgeting and calculations, inspired him to create a financial tool that would provide insights at a glance, eliminating the need for continuous manual checks. What it does NFA is a comprehensive platform that: Collects detailed user financial information, including: Analyzes this data to provide personalized insights. Identifies potential \"red flags\" in the user's financial situation. Offers notifications and alerts about these potential issues. Provides educational resources tailored to the user's financial situation and goals. All of this is done without crossing the line into providing direct financial advice, hence the name \"Not Financial Advice\" - since this is MOST DEFINITELY NOT FINANCIAL ADVICE!!!!! How we built it We leveraged a modern tech stack to build NFA, focusing on scalability, performance, and developer experience. Our technology choices include: Frontend: Backend and Database: API and AI Integration: Development Tools: This tech stack allowed us to create a robust, scalable application that can handle complex financial data processing while providing a smooth user experience. The combination of Firebase for backend services and Next.js for the frontend enabled us to rapidly develop and iterate on our platform. The integration of Cohere API for AI capabilities was crucial in developing our intelligent insights engine, allowing us to analyze user financial data and provide personalized recommendations without crossing into direct financial advice territory. Challenges we ran into Building NFA presented us with a unique set of challenges that pushed our skills and creativity to the limit: Navigating Regulatory Boundaries: One of our biggest challenges was designing a system that provides valuable financial insights without crossing into regulated financial advice territory. We had to carefully craft our algorithms and user interface to ensure we were providing information and analysis without making specific recommendations that could be construed as professional financial advice. Ensuring Data Privacy and Security: Given the sensitive nature of financial data, implementing robust security measures was paramount. We faced challenges in configuring Firebase Auth and Firestore to ensure end-to-end encryption of user data while maintaining high performance. This required a deep dive into Firebase's security rules and careful consideration of data structure to optimize for both security and query efficiency. Integrating AI Responsibly: Incorporating AI through the Cohere API and Groq presented unique challenges. We needed to ensure that the AI-generated insights were accurate, unbiased, and explainable. This involved extensive testing and fine-tuning of our prompts and models to avoid potential biases and ensure the AI's outputs were consistently reliable and understandable to users of varying financial literacy levels. Optimizing Performance with Complex Data Processing: Balancing the need for real-time insights with the computational intensity of processing complex financial data was a significant challenge. We had to optimize our Next.js and React components to handle large datasets efficiently, implementing techniques like virtualization for long lists and strategic data fetching to maintain a smooth user experience even when dealing with extensive financial histories. Creating an Intuitive User Interface for Complex Financial Data: Designing an interface that could present complex financial information in an accessible way to users with varying levels of financial literacy was a major hurdle. We leveraged Tailwind CSS to rapidly prototype and iterate on our UI designs, constantly balancing the need for comprehensive information with clarity and simplicity. Cross-Browser and Device Compatibility: Ensuring consistent functionality and appearance across different browsers and devices proved challenging, especially when dealing with complex visualizations of financial data. We had to implement various polyfills and CSS tweaks to guarantee a uniform experience for all users. Managing Team Dynamics and Skill Diversity: With team members ranging from high school to university students with varying levels of experience, we faced challenges in task allocation and knowledge sharing. We implemented a peer programming system and regular knowledge transfer sessions to leverage our diverse skillsets effectively. Handling Real-Time Updates and Notifications: Implementing a system to provide timely notifications about potential financial \"red flags\" without overwhelming the user was complex. We had to carefully design our notification system in Firebase to balance immediacy with user experience, ensuring critical alerts were not lost in a sea of notifications. Scalability Considerations: Although we're starting with a prototype, we had to design our database schema and server architecture with future scalability in mind. This meant making tough decisions about data normalization, caching strategies, and potential sharding approaches that would allow NFA to grow without requiring a complete overhaul. Ethical Considerations in Financial Technology: Throughout the development process, we grappled with the ethical implications of providing financial insights, especially to potentially vulnerable users. We had to carefully consider how to present information in a way that empowers users without encouraging risky financial behavior. These challenges not only tested our technical skills but also pushed us to think critically about the broader implications of financial technology. Overcoming them required creativity, teamwork, and a deep commitment to our goal of empowering users with financial insights. Accomplishments that we're proud of Innovative Financial Insight Engine: We successfully developed a sophisticated algorithm that analyzes user financial data and provides valuable insights without crossing into regulated financial advice. This delicate balance showcases our understanding of both technology and financial regulations. Seamless Integration of AI Technologies: We effectively integrated Cohere API and Groq to power our AI-driven insights, creating a system that can understand and analyze complex financial situations. This accomplishment demonstrates our ability to work with cutting-edge AI technologies in a practical application. Robust and Scalable Architecture: Our implementation using Firebase, Firestore, and Next.js resulted in a highly scalable and performant application. We're particularly proud of our data model design, which allows for efficient querying and real-time updates while maintaining data integrity and security. User-Centric Design: We created an intuitive and accessible interface for complex financial data using React and Tailwind CSS. Our design makes financial insights understandable to users with varying levels of financial literacy, a crucial aspect for broadening financial education and accessibility. Advanced Data Visualization: We implemented sophisticated data visualization techniques that transform raw financial data into easily digestible graphs and charts. This feature significantly enhances user understanding of their financial situation at a glance. Responsive and Cross-Platform Compatibility: Our application works seamlessly across various devices and browsers, ensuring a consistent user experience whether accessed from a desktop, tablet, or smartphone. Real-Time Financial Alerts System: We developed a nuanced notification system that alerts users to potential financial issues or opportunities without being overwhelming. This feature demonstrates our attention to user experience and the practical application of our insights. Comprehensive Security Implementation: We implemented robust security measures to protect sensitive financial data, including end-to-end encryption and careful access control. This accomplishment showcases our commitment to user privacy and data protection. Efficient Team Collaboration: Despite our diverse backgrounds and experience levels, we established an effective collaboration system that leveraged each team member's strengths. This resulted in rapid development and a well-rounded final product. Ethical AI Implementation: We developed guidelines and implemented checks to ensure our AI-driven insights are unbiased and ethically sound. This proactive approach to ethical AI use in fintech sets our project apart and demonstrates our awareness of broader implications in the field. Rapid Prototyping and Iteration: Using our tech stack, particularly Next.js and Tailwind CSS, we were able to rapidly prototype and iterate on our designs. This allowed us to refine our product continuously based on feedback and testing throughout the hackathon. Innovative Use of TypeScript: We leveraged TypeScript to create a strongly-typed codebase, significantly reducing runtime errors and improving overall code quality. This showcases our commitment to writing maintainable, scalable code. Successful Integration of Multiple APIs: We seamlessly integrated various APIs and services (Firebase, Cohere, Groq) into a cohesive platform. This accomplishment highlights our ability to work with diverse technologies and create a unified, powerful solution. Creation of Educational Resources: Alongside the main application, we developed educational resources that help users understand their financial situations better. This additional feature demonstrates our holistic approach to financial empowerment. Performance Optimization: We implemented advanced performance optimization techniques, resulting in fast load times and smooth interactions even when dealing with large datasets. This showcases our technical proficiency and attention to user experience. These accomplishments reflect not only our technical skills but also our ability to innovate in the fintech space, our commitment to user empowerment, and our forward-thinking approach to financial technology. What we learned Navigating Financial Regulations: We gained a deep understanding of the fine line between providing financial insights and giving regulated financial advice. This knowledge is crucial for anyone looking to innovate in the fintech space. The Power of AI in Finance: Through our work with Cohere API and Groq, we learned how AI can be leveraged to analyze complex financial data and provide valuable insights. We also understood the importance of responsible AI use in financial applications. Importance of Data Privacy and Security: Working with sensitive financial data reinforced the critical nature of robust security measures. We learned advanced techniques in data encryption and secure database management using Firebase and Firestore. User-Centric Design in Fintech: We discovered the challenges and importance of presenting complex financial information in an accessible manner. This taught us valuable lessons in UX/UI design for fintech applications. Full-Stack Development with Modern Technologies: Our team enhanced their skills in full-stack development, gaining hands-on experience with Next.js, React, TypeScript, and Tailwind CSS. We learned how these technologies can be integrated to create a seamless, efficient application. Real-Time Data Handling: We learned techniques for efficiently managing and updating real-time financial data, balancing the need for immediacy with performance considerations. Cross-Platform Development Challenges: Ensuring our application worked consistently across different devices and browsers taught us valuable lessons in responsive design and cross-platform compatibility. The Value of Rapid Prototyping: We learned how to quickly iterate on ideas and designs, allowing us to refine our product continuously throughout the hackathon. Effective Team Collaboration: Working in a diverse team with varying levels of experience taught us the importance of clear communication, task delegation, and knowledge sharing. Balancing Features and MVP: We learned to prioritize features effectively, focusing on creating a viable product within the hackathon's time constraints while planning for future enhancements. The Intersection of Finance and Technology: This project deepened our understanding of how technology can be used to democratize financial insights and empower individuals in their financial decision-making. Ethical Considerations in AI and Finance: We gained insights into the ethical implications of using AI in financial applications, learning to consider potential biases and the broader impact of our technology. Performance Optimization Techniques: We learned advanced techniques for optimizing application performance, especially when dealing with large datasets and complex calculations. The Importance of Financial Literacy: Through creating educational resources, we deepened our own understanding of financial concepts and the importance of financial education. API Integration and Management: We enhanced our skills in working with multiple APIs, learning how to integrate and manage various services within a single application. Scalability Considerations: We learned to think beyond the immediate project, considering how our application architecture could scale to accommodate future growth and features. The Power of Typed Programming: Using TypeScript taught us the benefits of strongly-typed languages in creating more robust, maintainable code, especially in complex applications. Data Visualization Techniques: We gained skills in transforming raw financial data into meaningful visual representations, learning about effective data visualization techniques. Agile Development in a Hackathon Setting: We applied agile methodologies in a compressed timeframe, learning how to adapt these principles to the fast-paced environment of a hackathon. The Potential of Open Banking: Although not directly implemented, our project made us aware of the possibilities and challenges in the emerging field of open banking and its potential impact on personal finance management. These learnings not only enhanced our technical skills but also broadened our understanding of the fintech landscape, ethical technology use, and the importance of financial empowerment. The experience has equipped us with valuable insights that will inform our future projects and career paths in technology and finance. What's next for NFA (Not Financial Advice) Enhanced AI Capabilities: Open Banking Integration: Expanded Financial Education Platform: Community Features: Gamification of Financial Goals: Advanced Data Visualization: Personalized Financial Product Recommendations: Multi-Language Support: Blockchain Integration: Mobile App Development: API for Developers: Sustainability Focus: Customizable Dashboard: Integration with Financial Advisors: Expanded AI Ethics Board: Research Partnerships: Accessibility Enhancements: Predictive Life Event Planning: Voice Interface: Continuous Learning AI: By implementing these features, NFA aims to become a comprehensive, intelligent, and indispensable tool for personal financial management. Our goal is to democratize access to high-quality financial insights, empower individuals to make informed financial decisions, and ultimately contribute to improved financial well-being on a global scale.",
    "matched_prize": null
  },
  {
    "title": "Cooked or Cooking",
    "link": "https://devpost.com/software/cooked-or-cooking",
    "text": "Inspiration Do you ever feel like everyone around you is working harder than you at a hackathon? It's already tough enough and that added stress of wondering if you are being productive enough wears down on you. We began to find out that many people at Hackathons often experience imposter syndrome, questioning their abilities of being able to contribute or even build a project on time. That's where the idea of Cooked or Cooking emerged. We wanted to create a platform where we track real time productivity of other hackers and see who is cooking (working hard) or cooking (taking a break). Not only does this help you take everything at your own pace and understand that everyones doing their own thing on their own schedule but it can act as a little motivator that can push others to be more productive in such a competitive environment. After all, all the best things are made when there's competition. What it does Our application provides insights on real time productivity by visualizing the current background applications of users. By analysing users open tabs, it generates a heatmap that displays whether users are “cooking” (currently working) or “cooked” (taking a break, a long one) How to get started! How we built it Cooked or Cooking was built using a variety of technologies, including Python, React, MongoDB, Mapedin, Deck.GL, and GoDaddy domains. Our database is stored using MongoDB, and the application that analyzes open tabs is written in Python. The backend was powered by MongoDB and Python, while the frontend was built using React. Key features, such as the heatmap and map visualization, were implemented using Mapedin and Deck.GL. Challenges we ran into We faced several challenges throughout this project, the biggest being the need for a complete pivot. Initially, we were working with the Spotify API and Mappedin. However, late on Saturday, we discovered that the Spotify API capped us at 25 users, which effectively rendered our project unusable. This left us with two choices: either create predictive data to continue with our original concept or build something from scratch that other attendees could actually use and find value in. That’s when we pivoted to Cooked or Cooking. We also struggled with creating a system to effectively pinpoint the location of the user inside the building, this is because we weren't able to access bluedot data from the users iphone and therefore we eventually requested the users to submit their own location. In the future we would like to utlize indoor positioning systems to improve the project. Accomplishments that we're proud of We’re very proud of how much we've learned. This was our first time using React as well as our first time building fully packaged Python applications. We've gained a lot of valuable experience, especially from working with the Mapedin API and Deck.GL. What we learned On a technical level, we learned a lot about React, packaged Python applications, the Mapedin API, and Deck.GL. But beyond the tech, we gained some valuable lessons. Here are a few: Adaptability is key: One of the most important lessons we learned this weekend is the need for flexibility. Projects don’t always go as planned, but it’s crucial to stay calm and think on your feet to overcome setbacks. You don’t need to be an expert to start: A major challenge our team faced early on was deciding what to build. We brainstormed a variety of ideas and realized that you don’t need years of coding experience to create something meaningful—you just need the courage to start and the willingness to learn along the way. It’s okay to ask for help: As high school students with little to no hackathon experience, we often sought help. We're incredibly grateful to the mentors and the MappedIn team members who guided us. Special thanks to Zach and Kimberly for all their support! What's next for Cooked or Cooking We believe there is significant value in tracking productivity data and pairing it with location information. Not only at hackathons, where such data can help organizers optimize future events—like introducing new activities or workshops in areas where productivity drops—but also on a larger scale. This data can be highly beneficial for companies and businesses. An enterprise product built around the core idea of Cooked or Cooking would allow large tech companies and enterprises to analyze employee productivity across different locations. For example, companies like Nvidia or Shopify, with open-concept offices, could use this data to better optimize their workspaces, ultimately improving employee productivity and performance for a higher return on investment.",
    "matched_prize": null
  },
  {
    "title": "WellRise",
    "link": "https://devpost.com/software/wellrise",
    "text": "About the Project: WellRise Inspiration WellRise was inspired by the need for better sleep and overall wellness in our busy, and modern lives. We wanted to create something that combined a sense of calm with playful charm, which led to the choice of a cute Goose as the theme for the project. The Goose represents peacefulness and lighthearted joy, making WellRise feel approachable and fun. What We Learned Building WellRise taught us a lot about the importance of simplicity and user experience in wellness enhancement. We learned how to design an intuitive interface that could offer a balance between relaxation and utility, ensuring users feel at ease when the project. We also deepened our understanding of how sleep affects overall health, and how small changes can make a big difference in sleep quality. How We Built It WellRise was built using Arduino and flask framework for the website component, with a focus on clean, minimal design to avoid overwhelming users. The app integrates features like sleep tracking and reminders to help users create healthy habits. We used user feedback to iteratively improve the app, making sure it remains user-friendly and impactful. Challenges One of the biggest challenges was finding the right balance between functionality and simplicity. While we wanted to offer helpful features, it was important to not overwhelm users with too many options. Designing a sleep-focused app that could cater to different lifestyles while maintaining a lighthearted tone also required careful thought. Accomplishments that We're Proud Of We're proud to have created an app that truly resonates with users and helps improve their wellness, particularly their sleep quality. One of the biggest accomplishments was developing an interface that feels lighthearted and engaging, thanks to the inclusion of the Goose mascot, while still providing real value in terms of wellness tools. What's Next for WellRise Looking ahead, there are exciting plans for WellRise. The next steps include expanding wellness features beyond sleep, such as mindfulness exercises, daily mood check-ins, and personalized wellness tips with AI chatbots. We also plan to introduce community features, allowing users to share their progress and encourage one another on their wellness journeys. With future updates, we aim to incorporate AI-driven insights that offer personalized sleep and wellness recommendations tailored to individual needs. Ultimately, WellRise will evolve into a comprehensive wellness companion that supports users in every aspect of their health and wellbeing. WellRise is a project that reflects our passion for seamlessly integrating hardware and software as well as creating tools that help others improve their lives, starting with something as essential as sleep.",
    "matched_prize": null
  },
  {
    "title": "RizzVision",
    "link": "https://devpost.com/software/rizzvision",
    "text": "Inspiration Have you ever walked into a room, stepped into a meeting, or even gone on a date, only to feel completely out of place? It's like everything is going wrong—you say the wrong things, thinking you’ve got it under control, but in reality, you don’t. You’re left wondering what went wrong and wishing you had known what to do differently. Well, we have gotten the fix for you. What it does We first open our mobile app, which contains settings and a profile of your conversations and uses. Using a camera and microphone, we capture real-time data from your conversation. This includes live audio transcription, capturing spoken words, and performing semantic analysis on facial expressions. All this data is stored in a conversational database. During the conversation, our state-of-the-art AI inference system analyzes the sentences spoken, providing a score and a brief explanation for it. Additionally, it offers live suggestions for responses, leveraging past context to craft the next optimal question or statement. At the end of the conversation, we format the conversation data similar to a chess game analysis, providing an overall summary overview. This includes key insights on what went well and areas for improvement, with specific pointers highlighting the strengths and missteps throughout the conversation. All of this is comfortably displayed and centralized all in the portability of your phone inside the mobile app. It also notes of the blunder you made when you asked them if they liked Jazz. How we built it Input Processing: We utilized a Raspberry Pi as the central hub for input processing. This compact yet powerful device efficiently connects to various capturing devices (likely including a camera and microphone) as well as output devices. AI Inference System: The core of our solution is an external service running a constant AI inference system. We leveraged Langchain as our framework and Groq for high-performance AI computations. At the heart of this system, we implemented the Mistral 7B language model, known for its efficiency and strong natural language understanding capabilities. Mobile Application: To make our solution accessible and user-friendly, we developed a mobile app using React Native. This cross-platform approach allows us to reach both iOS and Android users with a single codebase. Data Management: All conversational data is stored and managed using MongoDB, a flexible and scalable NoSQL database. This allows us to efficiently handle the real-time nature of our application while maintaining a robust data structure for analysis and retrieval. This architecture allows us to capture real-time audio and visual data, process it through our AI system for instant analysis and suggestions, and present the results to users through an intuitive mobile interface. The combination of edge computing on the Raspberry Pi and cloud-based AI processing enables us to balance performance, latency, and scalability effectively. Challenges we ran into Due to the use of large language models (LLMs), predictions can occasionally be noisy, leading to inaccuracies or \"hallucinations\" in determining the correct score or analysis. This can affect the precision of the real-time feedback and suggestions. Additionally, the process of integrating and migrating software to hardware is a significant challenge. Hardware, in general, presents numerous issues, from linking devices to ensuring seamless communication between inputs/outputs and the service. Establishing stable connections, handling device compatibility, and ensuring real-time processing all contributed to the primary hurdles we faced in the development of this product. Overcoming these technical obstacles was crucial in shaping the final solution. Accomplishments that we're proud of We take pride in the challenge we set for ourselves, pushing through an intensive process to bring this unique idea to life. Special thanks to Eddy for staying awake for over 24 hours and helping us push through when it mattered most. What we learned Hardware is not easy. Software isn't either, but integrating and migrating these 2 worlds is really really not easy. It's also not about the outcome but the friends we made along the wa- What's next for RizzVision Although we initially developed this tool to help with the thousands of struggling, desperate students at Waterloo who may never find a date, the potential for expansion on this project is limitless. This analysis-based tool can be applied to a wide range of scenarios, from analyzing conversations in business meetings to evaluating how you interact with an employer during a job interview. Sentiment analysis and live suggestions can be valuable across various aspects of life, particularly in the career of an engineer.",
    "matched_prize": null
  },
  {
    "title": "Take5",
    "link": "https://devpost.com/software/placeholder-mw067j",
    "text": "Inspiration Our generation, ourselves included, spend copious amounts of time in front of our computers. It’s not a good habit to have, but simply put, building the self-control to wrench oneself away from the screen is quite boring! What it does Take5 is an innovative app that records the user’s movements when browsing, and identifies instances of subpar browsing behaviour - leaning in too far, straining your eyes, staying at your computer too long - and corrects the user through a variety of possible punishments, ranging from playing embarrassing music to triggering fork bombs. All the while, our AI bot will chastise users whenever they misbehave, further teaching users to build good browsing habits. No WiFi? No problem! Take5 can run without the AI bot functionality, and all of the other functions of Take5 can run without being connected to the internet. How we built it Challenges we ran into Accomplishments that we're proud of What we learned What's next for Take5 Further implementation of a chatbot which gives advice about healthy habits while studying. More variety of punishments and punishable behaviours.",
    "matched_prize": null
  },
  {
    "title": "Wildfyre",
    "link": "https://devpost.com/software/wildfyre",
    "text": "Inspiration One of our teams mates (Nat) was a wildland firefighter in Northern Ontario before she made the switch to comp sci. One thing Nat noticed is that other provinces such as BC and Alberta have mobile apps to track where fires are in the province and give general information about the forest fire situation. Ontario actually doesn't have anything like that. They do have a website and an interactive map but it's not very user friendly. What it does Our idea- built for residents in Ontario especially NorthEast and NorthWest where the fires prevail, it would be beneficial if the app provides real-time wildfire updates, reporting tools, and a Google Maps visualization for Ontario. It also features an AI-powered forest fire behavior prediction tool for localized queries. This way anyone can stay up-to-date and have all resources in one space. How we built it We used Flutter to make the app, google maps API with flutter to display a map with current fires and their statuses, python for csv/excel file conversions to json and finally OpenAI API with flutter which can be used to generate a predictive image of what the fire would look like as well as prediction based answers such as \"how much is the fire intensity in x?\" Challenges we ran into Getting started was a challenge considering for all members, it was our first time creating an application, first time using flutter, first time using API's, first time using AI. Later on, combining each persons page into one application linked to a navigation menu also took time and research. Accomplishments that we're proud of Considering it was our first time doing everything in this project so far, we are extremely proud of 1) finishing the envisioned idea 2)having it fully functioning the way we expected 3) having fun learning the new technologies in this short span of hours What we learned Overall we learnt to just get started from somewhere, anywhere as even any failures are signs of learning and progress. What's next for Wildfyre We hope to train Wildfyre with large amounts of data found online to create a more accurate prediction and reponse model as well as look at ways to be more efficient with flutter code.",
    "matched_prize": null
  },
  {
    "title": "Navify",
    "link": "https://devpost.com/software/navify-6mhk41",
    "text": "✨ Inspiration My team and I noticed that visually impaired individuals often face challenges when navigating indoor spaces filled with corridors and sharp turns, largely due to the absence of clear directions and instructions. We also observed that many rely on a walking cane, which we saw as an opportunity—an untapped potential to enhance and transform into a more intelligent tool for better assistance. 🚀 What it does Navify is an innovative navigation system built for the visually impaired, harnessing the power of NFC technology, Mappedin, and Voiceflow. NFC tags are strategically placed on indoor floors where people commonly walk. Visually impaired users carry their cane, but with a game-changing twist—a special attachment that can scan NFC tags. With this, users can: Navify turns a simple walking cane into an intelligent, guiding tool that helps users navigate complex indoor environments with ease. 🛠️ How we built it Hardware: We built the hardware using a combination of smart components: The Arduino program reads the unique UID from each NFC tag and sends it to the app, which processes the data by matching the UID to corresponding coordinates in the database. We also utilized 3D-printed parts designed in SOLIDWORKS and printed with the reliable Bambu Lab A1 Mini printer. The cane itself is built with a sturdy 20mm extrusion, resulting in a high-tech walking cane ready to make a difference. Software: The app was developed using a powerful tech stack: We integrated the MappedIn SDK using documentation from their Hack the North Guide. We built a functional web app with React and Typescript, then hosted it to access maps through our Flutter App. We leveraged many MappedIn features, including labels and pathfinding. For our project, we used the E7 Floor 2 floorplan, where each NFC scan of a chip identifies the use precise location in the building. From there, Voiceflow AI assists by answering questions and guiding the user, transforming this technology into a comprehensive navigation tool. Our TTS assistant, Navi, is built using Voiceflow and a custom knowledge base for the A.I. component to ensure our assistant can provide high-quality and accurate help. We connected the voice agent to our application through the Voiceflow API so we can make calls to the agent directly from our mobile application. 🧗 Challenges we ran into Software: One of the main challenges was that the Mappedin SDK didn’t support Flutter, which is what our team was more comfortable with. To work around this, we switched to React to build the app and used Vercel for hosting. For dynamic routing, we implemented React Router DOM, ensuring smooth navigation between pages. The biggest challenge while using Voiceflow was understanding utterances and how each workflow worked with one another. We had to experiment extensively with what words the A.I. picked up on to trigger certain events and pathways. Hardware: On the hardware side, we faced issues with the sensor not consistently picking up NFC signals. After troubleshooting, we discovered the problem was due to faulty wire connections caused by poor-quality materials and weak connections. To solve this, we soldered all the wires to create strong, reliable connections, eliminating weak points and minimizing errors. 🏆 Accomplishments that we're proud of We’re proud of several key achievements: These accomplishments highlight our team's efficiency and commitment to delivering a functional and well-designed solution. 📚 What we learned Throughout the project, we gained valuable insights and skills, including: These lessons will be instrumental in enhancing our skills for future endeavors and projects. 🔮 What's next for Navify Stay tuned for exciting updates and future developments! We’re continuously working on enhancing Navify and exploring new possibilities to make it even more impactful.",
    "matched_prize": null
  },
  {
    "title": "DiaBites",
    "link": "https://devpost.com/software/diabites",
    "text": "Inspiration DiaBites was inspired by the need for a free, simplified, personalized meal planning solution for individuals with diabetes. Managing diabetes often means navigating complex nutritional requirements, so we aimed to create a tool that makes meal planning easy, enjoyable, and accessible. What it does DiaBites offers personalized meal plans tailored to the dietary needs of people with diabetes. By analyzing the user's body data and dietary restrictions, the platform generates balanced, diabetes-friendly recipes, ensuring users can enjoy delicious meals that align with their conditions. How we built it We built DiaBites using React.js for the frontend, Flask and Express.js for the backend, and MySQL for the database. We integrated data from trusted sources like Mayo Clinic and Medical News Today to ensure that all meal plans are diabetes-friendly and nutritionally sound. The team worked on algorithms for personalizing meal recommendations based on user profiles and dietary needs. Challenges we ran into We were able to put in a lot of work towards our back end and front end technologies equally. However, we struggled with integrating the two different sides of our technologies together, which led to us working together and learning at a much more intricate level. Accomplishments that we're proud of We’re proud to have built a functional, accessible, user-friendly platform that can provide tailored meal plans for a condition as complex as diabetes. Additionally, achieving a balance between taste and nutrition in the recommended meals is something we’re particularly proud of. What we learned We learned a great deal about the complexities of diabetes management and how personalized nutrition can play a vital role. On the technical side, we gained experience in integrating multiple backend technologies and handling sensitive health-related data. What's next for DiaBites The next step for DiaBites is to expand its scope beyond diabetes by incorporating meal plans for other health conditions such as hypertension, cardiovascular disease, and obesity. We also aim to support more dietary preferences and expand our recipe database for broader customization options.",
    "matched_prize": null
  },
  {
    "title": "Magic Mirror",
    "link": "https://devpost.com/software/magic-mirror-4g1vko",
    "text": "Inspiration In the digital landscape of the 21st century, we realized a core issue in online shopping: the gap between expectation and reality. The inability to physically try on clothes creates uncertainty in sizing, fit, and overall appearance, creating frustration and anxiety. After doing a market analysis of potential competing products, we realized that there is no publicly accessible solution that can feasibly be used. This challenge inspired us to re-imagine the online shopping experience, aiming not just to empower shoppers with better decisions but to enhance their confidence in the choices they make. Magic Mirror bridges the gap between the digital and physical worlds, allowing consumers to visualize how they’d look in an outfit before making the purchase. It's not just about finding the perfect fit; it’s about making sure every shopper can see themselves in the style they aspire to—making shopping a more informed and delightful experience. What it does Magic Mirror offers two transformative features that redefine the online shopping journey: In-Depth Product Insights: Shoppers can access a comprehensive overview of product reviews, ratings, and summaries. By utilizing natural language processing, Magic Mirror curates detailed insights from a variety of sources. This empowers shoppers to not only make informed decisions but also feel confident in their choices. The feature fosters transparency, enabling businesses to build long-term trust and customer loyalty by offering genuine, data-backed insights. Virtual Try-On: This core functionality allows users to upload an image of themselves and see how the clothes they are considering purchasing would look on them. Using advanced AI techniques, including image segmentation and inpainting with stable diffusion, Magic Mirror overlays garments with high fidelity, giving users a realistic preview of their future look. This not only reduces shopper regret but also contributes to fewer product returns, benefiting both consumers and businesses. A satisfied customer is more likely to return, making this feature essential for driving long-term business growth. Behind the scenes, Magic Mirror runs a powerful computational pipeline, sorting and analyzing vast amounts of product information. The AI-driven system also outlines the pros and cons of each item. By seamlessly blending product insights and cutting-edge virtual try-ons, Magic Mirror turns a once-daunting decision into a delightful, confidence-building process. How we built it Our journey began by designing a sleek, intuitive user-interface using React, ensuring an engaging user experience that is both accessible and visually appealing. On the back-end, we deployed FastAPI, which served as the backbone for handling dynamic content, user uploads, and external API requests. For the product insights, we leveraged Cohere’s LLMs (Large Language Models) to extract and summarize relevant product information, such as reviews and ratings, from across the web. This feature arms shoppers with all the information they need, compiled neatly in one place. The virtual try-on functionality was built using open-source AI models. By integrating Stable Diffusion for image generation, we created a way for users to visualize their outfits with high levels of realism. This process involves AI-powered segmentation and in-painting to ensure the virtual try-on experience feels authentic and trustworthy. Lastly, we developed a demo Shopify storefront and used Shopify’s API to dynamically pull products into Magic Mirror for demonstration purposes. The goal was to showcase how this technology could seamlessly integrate with any e-commerce platform. Throughout this process, we iterated over both the front- and back-end components, continuously refining them to ensure a smooth and polished user experience. Challenges we ran into Dev environments: Managing the dependency-heavy Python projects for the machine learning models was a significant challenge. Setting up and maintaining virtual environments while managing package dependencies became a top priority to ensure seamless development. Compute times: Running complex image generation models initially led to long compute times, often in the range of hours. To optimize this, we optimization techniques to compress runtimes down to minutes. Complex integration: Integrating the multi-component front-end and back-end posed significant hurdles. Communication between these two layers required rigorous testing and debugging, particularly as we introduced third-party APIs. Ensuring the various services worked in tandem while maintaining a consistent user experience required continuous iteration. Device agnosticity: Although modern tools like React streamline the front-end development process, ensuring the interface maintained a consistent look across different devices was complex. Accomplishments that we're proud of Realistic Image Generation: Successfully implementing a machine learning pipeline that allows users to generate realistic, custom images of themselves wearing potential outfits. The level of realism is key in making Magic Mirror a valuable tool for consumers. Aesthetically Pleasing Front-End: We’re particularly proud of our thematic, branded front-end that blends fashion sensibility with user-friendliness. The polished interface ensures that users engage confidently with the product. Seamless API Integration: Bringing together diverse systems—AI image generation, product reviews, and Shopify’s API—into one cohesive, fully integrated platform is a significant technical feat. What we learned The Importance of Virtual Environments: Managing Python environments efficiently is crucial when working on machine learning projects with multiple dependencies. Proper environment isolation simplified the development process and ensured fewer compatibility issues. Value of Design-Driven Planning: Focusing on both aesthetic and functional design from the start helped to create a product that is not only visually appealing but also functionally robust. A well-designed interface amplifies the impact of the technology behind it. Power of a Singular Focus: Having a clear, singular vision allowed us to remain on track. Focusing on creating a virtual try-on tool while integrating key features like product insights helped us create a more cohesive and user-driven experience. What's next for Magic Mirror Enhanced Image Generation: We aim to further refine our image generation model, improving the overall realism and supporting more complex clothing items, such as layered outfits, jewelry, and accessories. Chrome Extension for Seamless Integration: One of our milestones is the development of a Chrome extension for Magic Mirror. This would allow users to interact with the platform directly while browsing their favorite e-commerce stores. This streamlined integration will make the try-on process more convenient and accessible, expanding Magic Mirror’s usability. Personalized Accounts with Trained LoRA Models: We are working toward creating an account system that allows users to save preferences, store images, and refine the accuracy of try-on results. In future iterations, users will be able to generate their own LoRA (Low-Rank Adaptation) models to create more personalized and realistic virtual try-ons. Shopify Integration and Deployment: The next phase involves optimizing the app for Shopify, aiming for seamless integration that can be published on the Shopify marketplace. This will open the door for thousands of online stores to offer virtual try-on services, enhancing the experience.",
    "matched_prize": null
  },
  {
    "title": "The Singing Goose",
    "link": "https://devpost.com/software/the-singing-goose",
    "text": "Inspiration Imagine, that you, a bright eyed first time hackathoner, are on your way to your very first Hack The North. But, there is one issue. You can't sing! And you wouldn't want to embarrass yourself in front of all your cool friends would you? So, you seek out The Singing Goose: The cutting edge new innovation in singing technology. The Singing Goose provides real time feedback on your singing abilities, helping you match your pitch to the song with clever, creative, and very funny, goose puns. With The Singing Goose you are guaranteed to go from lose to goose at the speed of sound! What it does The Singing Goose uses an inbuilt neural network to create a frequency time curve of any song you choose to sing. By recording the user's live audio and comparing it to the graph, the singing goose can make estimations of the user's singing ability or lack thereof. The goose then contacts an AI model to provide feedback which is displayed to the user in speech bubble form! How we built it The goose is currently only partially built due to various constraints, but assuming it was completed, it would have been built on a Laravel application with a Flask API managing frequency conversion. Voiceflow would have been used for animating the goose, and most importantly, the goose is a hand-drawn masterpiece that is outside of this world. Challenges we ran into Our team ran into many challenges during the building of the project, ranging from difficulties contacting our API, to issues implementing our initial React frontend. We eventually resorted to a last minute stack swap and sadly could not complete the full project in time. Accomplishments that we're proud of Many great accomplishments were achieved in the process of making our golden goose. First, and most notably, we designed a social icon in our Singing Goose. With over 1000000 Spotify listeners, The Singing Goose is a social sensation and contemporary artist. We also successfully ran a neural network on our local devices to determine frequency ranges for songs, implemented a file transfer system using Flask, and made pretty graphs. What we learned We learned that determining a tech stack that works for everyone is essential for a good project workflow. Our project was limited in its ability to be split equally among members, and the task of developing our lord goose was simply too large for our small shoulders. What's next for The Singing Goose Despite The Singing Goose's currently unfinished form, we believe that there is a future where our goose rules the world makes the lives of many better with the power of singing (and our AI overlords.) Three Honks for The Singing Goose!",
    "matched_prize": null
  },
  {
    "title": "JustDuckIt",
    "link": "https://devpost.com/software/justduckit",
    "text": "Inspiration As coders, we often don’t have the time (or the will) to cook, which leads to one inevitable outcome—fast food, and not always the healthiest choices! We wanted to help fellow coders in workspaces and universities make better eating decisions. That’s where Goose the Duck comes in, our solution to encourage healthier choices without too much thinking involved. Because who doesn't trust a duck? And let’s be honest, when we hit a coding problem, we already talk to rubber ducks for advice (don’t deny it). P.S. Yes, we know—it’s ironic that the duck is named Goose, but that just makes it even more fun! 😉 What it does Goose, the not-so-goosey duck, is your personal food advisor. It is trained to help you make better food choices by identifying what's healthy and what's not! Simply present your food, and our duck will let you know if you're about to make a good or bad decision. How we built it We used a Raspberry Pi 3 to power Goose’s brain and trained a machine learning model (a CNN) to distinguish between healthy and unhealthy foods based on image classification. Additionally, with a bit of hardware hacking, we created a fun, feathery friend that learned the difference between a kale salad and a Dunkin’ Donuts gazed donut. Challenges we ran into Training a duck to recognize food turned out to be trickier than we anticipated! We struggled with finding large enough healthy and unhealthy food datasets, and to make matters worse, our Raspberry Pi often refused to boot at critical moments, making us question not just the project, but our career choices! On top of that, we didn’t receive all the hardware we ordered, forcing us to improvise with the limited resources we had. But in the end, the duck was quacking, and working just as we’d hoped! Accomplishments that we're proud of Despite the setbacks, we built everything ourselves—from the coding to the hardware setup to teaching a duck to judge your lunch! It was a proud moment when Goose finally quacked out its first food decision. He's quite the food critic now! What we learned We learned that building a project at Hack The North is a lot like feeding a duck—patience and persistence are key! We gained a deeper understanding of food nutrition and the importance of datasets in training models. We also learned (form experience) the intricacies of Raspberry Pi and hardware troubleshooting. Everything related to coding a system to process the webcam display was as well learned on the spot. What's next for JustDuckIt Goose might not stop at food! Who knows, maybe Goose will soon help you pick your outfit, plan your day, or even make life decisions like “Should I binge-watch this series or be productive?” We also didn't have a camera available during development, so our next step will also include attaching a camera directly to Goose to make it more independent. We'd also like to add a cooling system to Goose's brain (the Raspberry Pi) in the future to prevent overheating. Unfortunately, we didn’t have a fan available this time around, but it's definitely on our list for next time! We're also excited to add quacky sound effects to Goose in the future, with cheerful 'good' quacks and grumpy 'bad' quacks to keep things fun and engaging! Stay tuned—Goose is always evolving!",
    "matched_prize": null
  },
  {
    "title": "Learn Lens AI",
    "link": "https://devpost.com/software/learn-lens-ai",
    "text": "Built With",
    "matched_prize": null
  },
  {
    "title": "Linkr",
    "link": "https://devpost.com/software/linkr-eucw6f",
    "text": "Inspiration This year, all the members in our team entered the first year of post-secondary and attended our various orientation event. We all realized that during orientation, we talked to many people but really only carried on a couple of friendships or acquaintances. As such, we decided to make an app so that people could quickly find others in their vicinity who share their common interests rather than talking to every single person in the room until they find someone who's a good match. What it does Linkr is an app which shows the people in your area and sorts them by how much they have in common with you. When first installing the app, Linkr asks a series of questions to determine your personality and interests. Based on this, it will find matches for you, no matter where you go. The app uses geolocation of the users to determine how close other users are to you. Based on this and using AI to determine how much they have in common with you, you will get a sorted list of people with the person who has the most in common with you being at the top. How we built it We planned to use Cohere to analyze the similarities between the preferences of different people, using Django for the backend and React Native for the front end. Additionally, we believe that music is a great indicator of personality. As such, we looked into using Spotipy to collect data on people's Spotify history and profiles. With Cohere, we made use of the get_embedded function, allowing us to relate preferences and interests to a numerical value, which we then could mathematically compare to others using the numpy module. Additionally, we used Cohere to generate a brief summary of users' Spotify data, as otherwise, there is quite a bit of information, some which may not be as relevant. In the end, we ended up using React Native with Expo Go to develop the mobile app. We also used Appwrite as a database and authentication service. Challenges we ran into Indecisiveness: We spent a lot of time choosing between and experimenting on two different projects. We were also indecisive about the stack we wanted to use (we started with wanting to use C++ but it took us time to realize that this would not actually be feasible) . Combining different components: While we were successful in developing the different parts of the project individually, we had a difficult time combining the different technologies together. Accomplishments that we're proud of We are proud of our perseverance. Working on this project was not easy, and we were met with many challenges. Especially considering how our indecisiveness cost us a large part of our time, we believe that we did well and completed a fairly large portion of the final product. What we learned Incorporating different API's together. Working with a completely new technology (all of us were new to mobile app dev). Cohere's excellent functionalities. What's next for Linkr Allowing more user customizability and ability to communicate with other matches. Precise finding of other matches",
    "matched_prize": null
  },
  {
    "title": "Advizr",
    "link": "https://devpost.com/software/advizr",
    "text": "Inspiration Advizr was sparked from the common problem of getting accurate course information fast. The recommended way at universities is to go to academic advising and wait hours and hours until you get to ask a simple question that could be answered in a sentence. Furthermore, if you go to ChatGPT for academic advising for your courses, it will “hallucinate” non-existent courses. With a combined desire to solve a major student issue of waiting hours for academic help and optimizing an LLM to become a deterministic model, Advizr was born. What it does Advizr is a specialized academic advising RAG model that leverages Cohere’s API to provide users with accurate and reliable course information. By developing a RAG model trained on course data from universities nationwide, our model delivers precise, deterministic information on prerequisites, corequisites, grades, professors, and course descriptions. With its scalable architecture, Advizr has the potential to impact student academic planning and evolve into a comprehensive tool for career guidance! How we built it Before we started development, we conducted a survey that gathered over 100 opinions, revealing public interest in our idea. This motivated us to approach this opportunity with a solid tech stack composed of Python and React. We first performed data integration by pulling data from three different APIs: UBC Course Explorer, UBC Grades, and Rate My Professor. After that, we cleaned the data and, with the help of Cohere’s API, built a Retrieval-Augmented Generation (RAG) model for the purpose of providing accurate course data in an accessible form. Additionally, we used re-ranking algorithms by Cohere that led to an 85% increase in efficiency of the product. Once we had a solid backend, we began designing our vision of the User interface with Figma and making it tangible with React. To bring it all together, we used API calls to fetch data from the front end to the back end and vice versa. Hence, throughout this build process, we have created a convenient web application backed by AI that provides accurate and relevant information. Challenges we ran into Since we are new to the LLM space, we had trouble determining how to get started and fill our knowledge gaps. Another problem we faced was how to implement persistent data transfer between the front end and back end. Nevertheless, we were able to get past these obstacles by investigating relevant documentation, learning from mentors, and collaborating. This has resulted in the completion of our initial goal to create Advizr. Accomplishments that we're proud of Producing the first-ever academic assistant LLM web app powered by Cohere’s API in under 36 hours. During this hackathon, we have overcome various technical and physical challenges like lack of sleep and unconfigured virtual environments, all while learning complex technologies. In particular, we implemented an API we were not previously familiar with and built a RAG model for the first time. What we learned At Hack the North, we picked up a new tech stack and skills: Flask, React, Figma, RAG models and LLMs. However, not only did we build on our technical skills, we also grew our ability to collaboratively work towards a goal and problem-solve when blocked. What's next for Advizr We plan to scale our web app to include more personalized features like AI-powered course scheduling and student profiles for a more connected feeling. In addition, we aim to partner with universities from all around the world to help students feel confident about their academic journey by providing course information in an organized and convenient way.",
    "matched_prize": null
  },
  {
    "title": "HandsOff Technologies",
    "link": "https://devpost.com/software/handsoff-technologies",
    "text": "Built With",
    "matched_prize": null
  },
  {
    "title": "LossEndFound",
    "link": "https://devpost.com/software/lossendfound",
    "text": "Inspiration Have you ever lost a valuable item that’s really important to you, only for it to never be seen again? The most commonly lost items include wallets, keys, and phones. While some are lucky enough to find their lost items at home or in their car, those who lose things in public often never see them again. The good news is that most places have a “lost and found” system, but the problem? It's manual, requiring you to reach out to someone to find out if your item has been turned in. What it does LossEndFound solves this problem by automating the lost and found process. It connects users who report lost items with those who find them. How we built it We built LossEndFound to make reconnecting lost items with their owners seamless: Challenges we ran into As first-time hackers, we faced a few challenges: Accomplishments that we're proud of We’re proud of how we pushed ourselves to learn and integrate new technologies: What we learned We learned several key lessons during this project: What's next for LossEndFound Moving forward, we plan to: These improvements will make the app even more efficient and user-friendly, keeping the focus on simplicity and effectiveness.",
    "matched_prize": null
  },
  {
    "title": "TrackBack",
    "link": "https://devpost.com/software/trackback",
    "text": "Picture this: Your world is familiar, yet fading. You reach for your glasses, but they’re gone. You check the usual spots for your keys, but they aren’t there. Every day, simple tasks become a puzzle, a battle against memory. What it does 🧠 TrackBack continuously records the environment using a camera and leverages object detection through YOLOv4 to identify and log objects, providing bounding boxes, the location, the label, and the most prominent colour. When users forget where an item was placed, they can type a query like \"Where is my brown mug?\" on our web app. An LLM (powered by Voiceflow) will access an image of the object along with object details stored by the YOLO model to provide instructions on where the object is. One of our key features is the chat function. We understand that seniors tend to have a hard time with technology, so having a chat feature that mimics real-life conversations could help them use this software. How we built it 🛠️ We used YOLOv4 for object detection, transforming and labelling detected objects in a real-time feed and their associated metadata (bounding boxes, labels, locations, colours) into a structured format accessible by Voiceflow. The data is stored in a tabular format to ensure it is compatible with Voiceflow's LLM, which handles the user queries and retrieves relevant object information. Our front-end was first designed in Figma and then implemented in React.js. The information displayed comes from calling the Voiceflow API. Challenges we ran into 🧗 Accomplishments that we're proud of 🌟 What we learned 🔍 What's next for TrackBack💡",
    "matched_prize": null
  },
  {
    "title": "Job Journey",
    "link": "https://devpost.com/software/job-journey",
    "text": "Built With",
    "matched_prize": null
  },
  {
    "title": "JeARVIS",
    "link": "https://devpost.com/software/jearvis",
    "text": "Inspiration JeARVIS was inspired by our fascination with JARVIS from Iron Man and our dream of bringing that level of interactivity to real life. We envisioned a desktop assistant that blends AI with immersive 3D animations and audio, making technology feel more intuitive, engaging, and alive. Our goal is to transform the way people interact with their desktops, turning everyday tasks into a dynamic and enjoyable experience. What it does JeARVIS is a desktop assistant designed to streamline your workflow and add a touch of entertainment. It provides real-time 3D animations and audio feedback, monitors your audio and desktop activity, and offers helpful reminders, system management, and interactive companionship. By combining productivity with fun, JeARVIS enhances your digital experience like never before. How we built it We developed the 3D models and animations using Unity, while the frontend is powered by ElectronJS and NodeJS. The backend runs on a Python server, integrating multiple services like Groq for image processing and VoiceFlow for handling user interactions. Challenges we faced Being new to ElectronJS and Unity, we encountered significant challenges with version control and compatibility between libraries, as well as difficulties in rendering Unity models seamlessly within Electron. Accomplishments we’re proud of We successfully integrated Groq for image and speech processing, and VoiceFlow to manage conversational interactions. These efforts resulted in a comprehensive assistant that offers personalized insights, such as sentiment analysis on your productivity. What we learned We gained valuable experience in working with new technologies and learned how to integrate multiple platforms cohesively. This project pushed us beyond our comfort zones but was ultimately rewarding. What’s next for JeARVIS Our future plans include enhancing the animations to make JeARVIS feel even more lifelike, and introducing camera-based features that enable real-world interaction and response.",
    "matched_prize": null
  },
  {
    "title": "Web Warriors",
    "link": "https://devpost.com/software/web-warriors",
    "text": "Inspiration a What it does How we built it Challenges we ran into Accomplishments that we're proud of What we learned What's next for Web Warriors",
    "matched_prize": null
  },
  {
    "title": "VoiScribe",
    "link": "https://devpost.com/software/voiscribe",
    "text": "Inspiration Communication is so important in today's world. Therefore, it is unfair that it may not be accessible to some parts of the population. We wanted to provide an easy solution in order to empower these individuals and build an inclusive environment. What it does and how we built it We used the Symphonic Lab’s voiceless API in order to interpret lip syncing movements into text/transcripts for people with speech impairments, which can be visualized on an application like google meet through closed captions. Once transcribed, we used google translate’s text-to-speech function to convert that text into speech, so that others can hear the intended words. Challenges we ran into We ran into a couple of challenges when developing the project. Firstly, there were bugs in the Symphonic API which slowed down our progress. However, we were able to overcome this challenge, with the help of our wonderful mentors, and create a working prototype. Accomplishments that we're proud of Despite multiple technical errors, we persevered through our project and successfully came up with an MVP. We collaborated effectively under time constraints and integrated feedback from mentors to constantly improve the code. What we learned We took so much away from this experience. Learning the tech was definitely one aspect of it, but in the process we developed other real-world skills such as critical thinking, problem-solving, building user-centric design, collaborating and so much more! What's next for VoiScribe In the future, we plan to make it capable of processing live feed. We also plan to we plan to incorporate a sign language predictor that can detect sign language when lip-sync to speech fails. Lastly, we plan to make it a chrome extension so that it is easily accessible to the public!",
    "matched_prize": null
  },
  {
    "title": "Shoppy",
    "link": "https://devpost.com/software/shopping-assistant-8rue4p",
    "text": "Built With",
    "matched_prize": null
  },
  {
    "title": "Peas in a Pod",
    "link": "https://devpost.com/software/hackpods",
    "text": "Inspiration Hackathons are a place much more than for hacking, it is an environment where all participants have the opportunity to build lasting collaborations. Unfortunately, many participants, especially first time participants, struggle to find their idea teammate despite having the necessary talents to create a beautiful project. What it does We have built an intelligent platform that will group hackathon attendees into synergetic groups based on their personal info, skills, and interests. Whether you are an engineer, a designer, back-end, or even someone who just enjoys basketball, we will find the right group of people for you and help you connect with like-minded individuals in a way that feels organic and natural. How we built it By utilizing React Native and Expo, our product is a cross-platform mobile experience, ensuring participants at hackathons can connect with one another regardless of their device. Having Convex as our backend and using Cohere's AI, we created a seamless algorithm that will recommend teammates based on their unique skills and project needs. Challenges we ran into Setting up all the dependencies. Accomplishments that we're proud of Leveraging Convex for real-time data synchronization and integrating Cohere’s AI for natural language processing, the high accuracy of our recommendations has been a big success factor that we're very proud of. What we learned Working with React Native and Expo gave us deeper insights into the nuances of cross-platform development. In addition, we saw firsthand how machine learning and natural language processing can significantly enhance user experience. What's next for Peas in a Pod We hope we can continue to meet the growing needs of hackathon participants and go beyond by taking our product to other events as well. We aim to expand our platform’s use beyond hackathons, enabling communities in academia, tech, and entrepreneurship to leverage our tools for networking and team-building in a variety of settings.",
    "matched_prize": null
  },
  {
    "title": "Mentor Matcher",
    "link": "https://devpost.com/software/mentor-matcher",
    "text": "Built With",
    "matched_prize": null
  },
  {
    "title": "AutoCAN",
    "link": "https://devpost.com/software/garbage-can",
    "text": "Inspiration In our cities and towns around us, it is extremely common for there to be multi-purpose trash bins, having multiple sections for different types of trash. Mainly, the use of these trash bins are to separate the recyclable from the non-recyclable. However, human error and laziness is inevitable as research showed that nearly 1/5 of all recyclable items are placed in the wrong bin. What it does AutoCAN, a smart trash sorting system, is used to save time and money. With normal trash bins, people often struggle and take time to decide which bin their trash should go in; and often, this decision is proven to be wrong. Using AutoCAN's smart trash bins, people can simply throw their trash in and it will automatically be sorted. Not does this just save time for ordinary people, but also relieve the labour costs for needing to manually sort offsite. How we built it The entire system can be split into 3 parts: Trash Classification Algorithm: For the classification algorithm we required datasets of trash/garbage as well as a machine learning model. The dataset was acquired from the public at Kaggle.com. We utilized Google's Teachable Machines web tool to create and train the ML model using the datasets. Hardware: We used an Arduino servo and motor to open the trash lid and spin the trash bin respectively so that the garbage drops into the correct category. To make the 3D model of AutoCAN's smart trash bin, we 3D printed every part. Challenges we ran into While debugging the code we were met with a lot of problems of the code not working properly and producing the wrong results. However, we were able to catch the problems by printing debugging information into the terminal. Another huge problem was achieving accurate and consistent image recognition of the different types of trash. However, we easily recognized that having more diverse and large amounts of datasets aided crucially. Training time and quantity also played a big factor in helping our model get better results. Accomplishments that we're proud of We are really proud of the 3D modelling and printing of the smart trash bin. It took a lot of hours and many redesigns and minor changes, but every small detail mattered at the end. What we learned Throughout this project, we have learned a variety of new things and skills. However, the most important thing that we have learned is how to use AI. Most definitely, we haven't mastered it, but we've learned its complexities and some of its operations. What's next for AutoCAN As of now, it is only currently in its prototype stage. If there existed next steps, then one of them would be enlarging the model into a real life sized one.",
    "matched_prize": null
  },
  {
    "title": "TED - Productivity App",
    "link": "https://devpost.com/software/hackdash",
    "text": "✨ Try out our Figma prototype here Inspiration We all procrastinate a lot—especially our team. We didn't do anything the first night, changed our idea five times, and finally decided on an idea at 5 pm on Saturday. We decided it would be a good idea to create a project that would help solve procrastination. What it does It tracks your open tabs on your computer and starts yelling at you if you're being unproductive. How we built it We created a mockup through Figma, a backend through Python, and our AI is completely powered by voiceflow. Challenges we ran into Accomplishments that we're proud of We are most proud of our rockstar UI/UX designer, Winston Zhao. He was able to go from wireframe to a full-fledge high-fidelity markup for two separate ideas ALL ON HIS OWN. We are also really proud of our idea. As first-year university students living away from home, we don't have our parents to berate us when we aren't studying hard enough. TED is a little taste of our parent's love that we didn't know we needed. What we learned One of the mistakes we made early on was not spending enough time formalizing and choosing an idea. We ended up spending the first half of the hackathon working on a completely different idea, only to scrap it and start from scratch. Luckily, we settled on an idea we liked and made TED. In the end, we learned the importance of choosing the right product and pivoting ideas when the necessity arises. What's next for TED We would like to switch technologies to a more convenient format: from a progressive web app to a desktop widget. We would also like to add voice and personality customization options.",
    "matched_prize": null
  },
  {
    "title": "devflow",
    "link": "https://devpost.com/software/devflow-n0kwpx",
    "text": "Inspiration In an era where agility is the key to success, especially for small businesses and startups, we noticed a gap in the market for affordable, easy-to-use project management tools that combine the simplicity of a Kanban board with the power of AI-driven insights. While enterprise-level tools like Jira are excellent for large teams, they tend to overwhelm smaller teams with unnecessary complexity, hindering productivity rather than boosting it. Our goal was to create a solution that empowers small businesses and development teams to optimize their workflow, unlocking the same level of efficiency that large corporations enjoy, and transforming the Software Development Life Cycle (SDLC) into a more streamlined, intuitive process. What it does The board is split into 4 key categories: In-Dev, In-Test, In-Review and Done. As the ticket progresses in GitHub from one development stage to the next, it will do so automatically on our board as well, containing essential information such as status, assignee, reporter and branch. How we built it Challenges we ran into Accomplishments that we're proud of What we learned What's next for devflow",
    "matched_prize": null
  },
  {
    "title": "Power Code",
    "link": "https://devpost.com/software/power-code",
    "text": "Links Frontend: https://github.com/Hack-The-North-2024/power-code Backend: https://github.com/Eurphus/PowerCode-Backend Inspiration Unleashing the flames of passion for programming is hard when you're grinding through endless streams of LeetCode questions to prepare for that one software interview. Let's be real, we've all been there. That's exactly why our project, Power Code, aims to make that experience way more fun and dynamic by bringing out the competitive nature in everyone's programming hearts to reignite their love for solving programming problems fast and efficiently. What it does In Power Code, programmers become gamers. That's right; our web application is a programming game that allows players to matchmake and duel opponents in a 1v1 to see who can solve a coding problem faster. These problems involve a selectable difficulty, providing all types of programmers to come and practice their skills through coding solutions as quick as possible. How we built it Power Code is a web application that utilizes React and Material UI for the frontend hosted on Netlify. Furthermore, we made use of Convex for the backend and implemented the multiplayer functionality using Convex's serverless functions. Additionally, code checks and validation were performed on a fast-api server hosted on heroku. Challenges we ran into Building Power Code wasn't always smooth sailing. One of the biggest obstacles we encountered was attempting to figure out how to use Convex to implement the game's multiplayer function. Accomplishments that we're proud of First and foremost, we're proud of being able to work on and build a project like Power Code. Having to learn and slowly work out how we were going to integrate Convex was definitely challenge, but a welcome one in that we were able to learn so much and accomplish the task as a result. What we learned While building Power Code, we learned that integrating new features using new methods like our usage of Convex is a great learning opportunity to help us develop projects later on. Our members learned about TypeScript, FastAPI, Convex, React, etc. Hack the North 2024 is a great place for programmers to gain more experience in developing projects, while giving them the luxury of being provided with an abundance of resources to further their knowledge and use them not only in their Hack the North project, but in the future as well. I also learned how to indulge on snacks What's next for Power Code Even though Power Code is a working project, there's still a long way to go for Power Code. The next step for our project is having a leaderboard to keep track of player points to further promote competition as well as implementing a compiler that can run a types of programming languages to be more accessible to programmers. Of course, our database for programming problems will expand and include a larger variety of problems later down the road. We plan to have full containerization to support super fast code running for majority of big languages.",
    "matched_prize": null
  },
  {
    "title": "UW Scavenger",
    "link": "https://devpost.com/software/uw-scavenger",
    "text": "Inspiration Our inspiration for creating UW Scavenger came from our own experience at Hack the North 2024. Navigating the sprawling E7 building was overwhelming, and finding quiet, comfortable spots for work and rest felt like an endless challenge. That's why we developed UW Scavenger - to transform the daunting task of exploring campus into an engaging adventure. With UW Scavenger, you and your friends can embark on three exciting scavenger hunts (ALL WITH AI-GENERATED RIDDLES AS CLUES!!) that will guide you to the best places on campus. Whether you’re searching for a quiet spot to focus or a vibrant place to unwind, our app makes discovering these spots both enjoyable and competitive. Plus, you can connect with other teams and new people who are also on the hunt. Just invite them to join the fun and let the scavenger hunts lead you to the perfect spots for work and play. By turning campus exploration into a game, UW Scavenger ensures that finding the ideal location is not just easier but also a lot more fun! What it does UW Scavenger transforms your campus experience into an engaging adventure by offering three dynamic scavenger hunts. Each hunt is designed to lead you to unique, hidden, and essential spots around campus. Here’s how it works: Discover Hidden Gems: Our app guides you to the best spots on campus—whether you're looking for a quiet corner to study, a cozy place to relax, or a lively area to socialize. Each hunt provides clues and challenges that reveal these hidden gems in a fun and interactive way. Compete and Collaborate: Join the scavenger hunts solo or with friends, and compete against other teams for the best times and highest scores. You can also team up with new people you meet along the way, making it a great way to connect and collaborate. Engage and Explore: With each hunt, you’ll uncover interesting facts, adding an educational twist to the adventure. The app also encourages exploration by leading you to less-traveled areas that you might not have discovered otherwise. AI-Generated Riddles: We use Groq's API to generate unique and engaging riddles for each scavenger hunt. These AI-created clues add an element of mystery and challenge, ensuring a fresh experience every time. Interactive Map: Our integration with Mappedin’s HTN 2024 indoor map helps you navigate campus efficiently. The map shows your proximity to your target spots, enhancing your scavenger hunt experience with real-time guidance. Track Progress and Achievements: Keep track of your achievements and see how you rank against other teams. The app features leaderboards and badges to celebrate your progress and motivate you to continue exploring. UW Scavenger turns the task of navigating campus into an enjoyable game, making your experience more engaging and memorable. A host chooses three items in three different rooms in the building. They then assign (by sticking a note or writing on/near the item) \"item codes\", which are numbers used to verify that players found the correct item based on the AI-generated clue provided by Groq's API. How we built it We used a Flask server to host the backend, and HTML/CSS/JS (with Bootstrap). We made use of Mappedin's HTN 2024 indoor map to show the players how far away they were from the room they needed to be in. We also used Groq's API, so that a description of the item could be turned into an AI-generated riddle to be solved as a clue by the players. We had no database, and all user data was stored in a dictionary within the server (and in user cache). Challenges we ran into We were unable to implement Genesys's API for a feedback system, since the documentation was difficult to interpret and mentors were unavailable, so we were forced to drop that idea and focus on bettering our AI riddles and map technology. Accomplishments that we're proud of We are proud of the fact that we were able to adopt a very innovative and subtle use of AI in our project. Instead of making a loud and fancy chatbot, we generated concise (yet an enjoyable exercise for the mind) riddles using Groq's API. This was helpful because it meant the host could choose ANYTHING they would like to be the item, and users didn't see the same boring riddle/clue every time. What we learned We learned how to implement Groq's API. This was exceptionally helpful for us because we found that Groq allows us to train the system to behave a specific way for the user. This makes it very easy for developers to launch LLM-based applications, instead of spending a large amount of time fine-tuning an LLM. What's next for UW Scavenger We hope that hackers continue to use our idea and benefit from the concept in future Hack the North events. This product has benefited us greatly during HTN 2024 - and we're confident this (and similar applications) could benefit many others. Outside of just HTN, Scavenger could also be used to introduce students to new campuses and ensure that they know important locations such as help desks or fire exits :)",
    "matched_prize": null
  },
  {
    "title": "SolEasy",
    "link": "https://devpost.com/software/soleasy",
    "text": "Inspiration Solana has come a long way from its humble beginnings, now being one of the top blockchains in terms of transactions and volume. But in my eyes one issue has always persisted and that is the steep learning curve with Solana's transactions and reading them. When I was a newbie it took me a while before I was comfortable using the Solana Explorer and Solscan and so I decided that its time to make adoption for new comers even easier through SolEasy! The easier way to view Solana transactions and also dispense transaction receipts to your friends or consumers. What it does It essentially provides a clean and very easy user interface to find the most important information regarding a Solana and SPL transfer transaction. It is also very easy to use due to its backwards compatibility with Solscan.io and it being open source. It even showcases a wallet's Solana name service identity for further ease and also updates the transactions status in real time! How we built it I built it using a Next.js, React, Go, Gin, Typescript, and Solana stack mainly. It runs off of a main net API which itself is hooked up to the Solana blockchain via RPC. Challenges we ran into Approaching this project solo due to the lack of interest in Crypto or Solana specifically this year at HTN was tough, and I had to learn some new things on the technical side and the blockchain side to complete the project. One of my largest hurdles was interacting with the on-chain data as initially I was going to use a RPC client entirely but changed my mind due to the unneeded complexity that would bring in. Accomplishments that we're proud of I'm really proud of the final product I think its a genuine tool that can be used around the world by either regular people who are new to the Solana ecosystem or businesses who need a easy way to dispense receipts to there consumers, as the Solana explorer and Solscan are not fit for such a use case. What we learned A lot needless to say, but one important thing I definitely learned was anything is possible if you truly believe in yourself. Going into this I did not think I could achieve this kind of product in just 2 days but here we are! What's next for SolEasy I'm now interested in adding in more transaction types specifically swap transactions and I wish to add an LLM agent that can explain those transactions to users so that users can use SolEasy for things like on-chain investigation if they aren't familiar with Solana instructions.",
    "matched_prize": null
  },
  {
    "title": "WakeUp",
    "link": "https://devpost.com/software/wakeup-cu1lhi",
    "text": "WakeUp. Have your friends Motivate you to Wake Up... Try WakeUp Now! A Social Media platform where your Alarm will not go off until you snap your friends with a morning task! Try it out: http://thiswebsitewillhelpyou.study/ src: https://github.com/d-huliu/WakeUp 💭Inspiration💭 With each new generation of students, it appears we are becoming \"worser\" at productivity, time management, and getting our day started. Let's not even bash GenZ yet; we (the developers) struggle to wake up to class (occasionally) too. There is no better way to solve modern problems, than to introduce modern solutions: Social Media. You wanna catch up with your friends? How about we try waking up first yea? 🤔What it does🤔 Never sleep through an alarm ever again. Instead, wake up so you can snap your friends! Wakeup forces you to wakeup by running an alarm until you complete the snap, given your defined prompt. Eg: prompt is \"Pic with Dog\", then Alarm will run until you snap a photo with your dog. The Camera AI is backed by GPT4o to ensure your snap contains the correct activity. View what your friend's morning routines consist of in the Feed home tab. You will see when they wake up, what they posted, and their caption for the day. Customize your morning routine under your Profile, where you can edit your Alarm and decide if you would like to manually choose your morning task or complete the automatically generated ones with your friends! Create your own WakeUp Account to then find/add your friends in the Chat tab. Message your friends about their tasks, and potentially organized shared morning routines that you can all stick to! 🔨How we built it🔨 We built WakeUp using a modern tech stack centered around React for the frontend, utilizing Chakra UI for a responsive and visually appealing user interface. Firebase was integrated for real-time database interactions and authentication, ensuring seamless data storage and retrieval for user profiles, friends, and chat messages. The project employs React Router for navigation between the various components, such as the alarm system, profile management, and chat interface. Framer Motion and GSAP were used to add interactive animations, enhancing the user experience. We also incorporated react-webcam for snapshot functionality within the alarm feature, and UUID for unique user identification. The app's backend logic, including tasks, alarm management, and chat functionalities, was tied together using Firebase's real-time database and cloud storage, making the app both fully functional and scalable. 💪Challenges we ran into💪 🏆Accomplishments that we're proud of🏆 🎓What we learned🎓 🔜What's next for WakeUp🔜 Future features to be worked on:",
    "matched_prize": null
  },
  {
    "title": "LowkeyPrepped",
    "link": "https://devpost.com/software/lowkeyprepped",
    "text": "Built With",
    "matched_prize": null
  },
  {
    "title": "EcoVision",
    "link": "https://devpost.com/software/ecovision-mu8pyx",
    "text": "Inspiration 💭 We were up late at night brainstorming after eating a bunch of snacks. Then, someone picked up a piece of trash from the table, and... we knew what to do. What it does 🤷‍♀️ It's a mobile app that identifies trash in an image and sorts it to determine which bin it should go into (recycling, garbage, etc.). How we built it 👨‍💻 Training the AI model: The AI model was originally the YOLOv5 pre-trained model. However, it was not built for garbage detection, so we fine-tuned it with the TACO (Trash Annotations in Context) dataset for 16 epochs.\nMaking the app: We used Swift to create the iOS app, which consists of a camera that returns the images with identification boxes around the garbage. Challenges we ran into 🤔 Finding a model and dataset: Finding an effective and accurate pre-trained AI model was very challenging. To solve this issue, we trained a not-so-accurate AI model with a dataset to increase the accuracy.\nTraining the dataset: Training an AI model took a long time. We found that using Google Colab's GPU would speed up that process.\nMultiplatform support: We originally wanted to support Android devices, but ran into many problems with React Native and Flutter Accomplishments that we're proud of 😄 We are very proud to be able to turn an inaccurate pre-trained model into a more precise version. What we learned 🏫 We learned a lot about mobile app development and object detection using machine learning. What's next for EcoVision❓ One major next step for EcoVision is to expand its accessibility further than iOS devices (eg. to Android users). We can add features such as using hardware to create specialized trash cans that sort the waste or creating a device to inform you of the category without using a phone.",
    "matched_prize": null
  },
  {
    "title": "GoDesk",
    "link": "https://devpost.com/software/godesk",
    "text": "Inspiration The inspiration behind Go Desk was to take AI chatbots to the next level for SMEs and startups. We wanted to help businesses focus on growth and innovation rather than getting bogged down by repetitive customer support tasks. By automating support calls, we aim to give businesses more time to build and scale. What it does Go Desk is a phone-based customer support AI agent that allows businesses to create intelligent agents for answering customer questions and performing specific tasks. These agents go beyond simple responses—they can cancel orders, book appointments, escalate cases, and update information, without requiring human intervention. How we built it Go Desk was built on Open AI's reliable APIs for conversational generation and intent comprehension. We integrated Twilio to handle phone calls programmatically, using speech-to-text for voice input processing. The backend was developed with Node.js and TypeScript, while the frontend was built with Vue.js. Challenges we ran into We faced a few challenges, especially in figuring out the right idea to implement. Initially, we planned a hardware project, but due to lack of components and other issues, we decided to pivot to an AI-based solution. Without a designer on the team, we had to get creative with the UI, and while it was a bit hacky, we’re proud of the result! Accomplishments that we're proud of This was our first project involving large language models (LLMs), and we pulled it off with almost no sleep in two days! We’re also proud of the fact that we managed to pivot the project successfully and deliver a fully functional AI-powered solution. What's next for Go Desk We plan to iterate on the platform, refine its features, and validate the idea by testing it with real customers. Our goal is to keep improving based on feedback and make Go Desk a go-to tool for businesses needing advanced AI-powered customer support.",
    "matched_prize": null
  },
  {
    "title": "Fleurish",
    "link": "https://devpost.com/software/fleurish",
    "text": "Inspiration There are a lot of text-to-image, image-to-text AI generators, only a few image-to-video AI generators, but from our research, we couldn't find an AI generator that converted live images to video (.gif). We wanted to fill this void, since it has a few useful applications, and so we created Fleurish! What it does When the user inputs an image, selects a theme and clicks the \"Imagine\" button, an AI-generated gif based on the inputted image will be generated after a few seconds. Alternatively, if the user connects their phone to their computer, they can also use the \"live image\" option to get the application to read images from the phone camera video in real time. How we built it We used a Convex template with a built-in Replicate integration as our backend, and then added on a Replicate https://replicate.com/lucataco/dreamshaper7-img2img-lcm in order to convert images to AI-generated images. For the live image option, we used RTC peer connections to send camera feed to a Python backend, which then compresses the image (to optimize the AI generation) and feeds to the front-end, built in Next.js with the WebSocket API. Challenges we ran into We spent a lot of time testing and searching for a model that gave us satisfactory results, and even then, it took us time to learn how to integrate that with our application. Also, we did not have previous experience with a lot of the technologies used, namely generative AI (with images), WebSocket API, and using the RTC peer connection to allow the communication between a phone and our application. Accomplishments that we're proud of Our application is highly optimized and generates the gif within 1-2 seconds. Also, the themes and the gifs produced give very accurate results. What we learned We learned a lot of new technologies, such as Convex, WebSocket API and RTC peer connection. We also gained experience with image generative AI, which is vastly different than text generative AI. Also, learning to integrate all of the technologies used together was a very insightful learning experience. What's next for Fleurish Better, faster models, user management (so there is no overlap between users).",
    "matched_prize": null
  },
  {
    "title": "dumpy",
    "link": "https://devpost.com/software/moment-o-us",
    "text": "Inspiration Now that everyone has gone their separate ways after high school, staying in touch with old friends isn't as easy anymore. Even with so many ways to stay connected, it's become rare to share the little, everyday moments that aren't important enough to be a post on Instagram or a reason to call. Dumpy allows us to highlight these small moments and is inspired by the simpler days in grade school, where we passed notes and secretly wrote on the classroom boards. What it does Dumpy helps friends stay connected in a simple, fun, focused, but also slightly chaotic way. No doomscrolling here! Users have the option of posting messages, pictures, voice notes, and drawings to a communal bulletin board. They can also give each other quests to complete. These quests may be motivational or just help spice up life. Users have complete control on what vibes they want to acheive for their boards! How we built it This project was designed using Figma and built with HTML, JavaScript, and CSS. Challenges we ran into Accomplishments that we're proud of What we learned What's next for Dumpy",
    "matched_prize": null
  },
  {
    "title": "AiroScents: The Automated Air Freshening Drone",
    "link": "https://devpost.com/software/airoscents",
    "text": "AiroScents: Project Story Inspiration The idea behind AiroScents was born from the desire to merge technology with environmental enhancement. We wanted to create a solution that could improve the atmosphere in indoor spaces using technology in a way that was both practical and unique. As we explored possibilities, we realized that scent plays a significant role in setting moods and enhancing spaces, whether in a home, office, or event setting. This led us to the concept of an autonomous drone that could navigate spaces while dispersing pleasant fragrances, creating a seamless blend of hardware innovation and software control. What We Learned Throughout the development process, we gained a deeper understanding of hardware-software integration. We learned about the complexities of working with Bluetooth modules like the HC-05, the challenges of creating stable flight systems for drones, and the importance of seamless communication between devices. On the software side, we explored React and TypeScript more thoroughly, learning how to integrate external APIs like Mappedin, and how to optimize user interaction for controlling a drone in real-time. The pathfinding research taught us the value of algorithmic efficiency and how crucial it is for navigating complex spaces autonomously. How We Built It The project was built in two main phases: hardware and software. Hardware: Software: Challenges We Faced One of the main challenges was ensuring stable communication between the drone and the web app. The Bluetooth module presented issues with range and reliability, which required us to troubleshoot and optimize its performance. On the hardware side, building a stable and efficient drone was a major task, as we had to balance weight, flight stability, and the addition of the scent-dispensing mechanism. On the software side, integrating the Mappedin API presented its own challenges, particularly with mapping complex indoor environments and ensuring the drone would follow precise routes. Conclusion AiroScents was a challenging yet highly rewarding project that pushed our skills. We are proud to have built a product that combines technology with a unique, practical application, and we look forward to exploring further improvements and features for future iterations.",
    "matched_prize": null
  },
  {
    "title": "Derek",
    "link": "https://devpost.com/software/derek-zm3w1g",
    "text": "Inspiration This year is our last year at Hack the North. Ever since 2020++, we have competed as hackers, making all kinds of projects from NFT scanners to mind-controlled cars. This year, we wanted to celebrate the unsung heroes. The ones left forgotten even though they're the backbone of society itself. We've all watched the movie WALL-E, but how many of us remember M-O, the robot who sacrificed everything for his beliefs and morals? Who is praising him for staying steadfast in his pursuit of cleanliness? We present DEREK, the Dynamic Efficient Robotic Expert for Kleaning, the lone robot causing the downfall of the Roomba itself. What it does DEREK successfully identifies contaminants and dirt particles, and shows them that it is no match for a of the robot the likes of it. It is a fully autonomous system that effectively removes contaminants from your home (or spaceship) and keeps you and your family safe. But you don't want a boring robot, do you? DEREK adds to the overall cleaning experience with the utmost sass. Whether it be snarky comments or lively facial expressions, DEREK has it all. How we built it DEREK is a Raspberry Pi 4 at heart. Supporting that board and the peripherals is Viam (a tool we learned about at our last HTN!). This made startup fast but also introduced complexities into our project. On the RPi, we have Python running our emotion engine, sassy response generator, and all our control elements. We use Cohere to create snarky, sassy - and at times - angry responses to seeing so much dirt to clean up. The body is a multicolour of 3D prints, laser cut parts, and last-minute duct tape patches. Animated emotions are displayed on an ipad and changed to fit the spoken word. Cohere Cohere was awesome! We were able to run sentiment analysis and generation with their generative text APIs and then correspond that with emotions and animations we created to fit the scene. This helped us to create a more \"human\" like-robot and give DEREK some personality! Challenges we ran into We seemed to run into trouble wherever we went. From the simplest of problems to much more complex ones; we probably had every problem possible during our hack this year. We had power issues, motor shearing problems, infinite code bugs, and mechnical fit issues. But, each time that we did, we were able to solve it or pivot to a different solution. Accomplishments that we're proud of We're proud of keeping up with the fun spirit of everything and pushing through all the challenges we ran into. We are especially proud of each other for the past years of school, friendship, and competitions. As it's our last year at UWaterloo, we wanted to make something fun, challenging, but more importantly, something we can do together. What we learned What's next for Derek",
    "matched_prize": null
  },
  {
    "title": "Firescape",
    "link": "https://devpost.com/software/gpeet",
    "text": "Built With",
    "matched_prize": null
  },
  {
    "title": "Integrating Multiple APIs Together",
    "link": "https://devpost.com/software/integrating-multiple-apis-together",
    "text": "Built With",
    "matched_prize": null
  },
  {
    "title": "Wakey",
    "link": "https://devpost.com/software/wakey-dwockq",
    "text": "Built With",
    "matched_prize": null
  },
  {
    "title": "Mint-Ur-Wellness",
    "link": "https://devpost.com/software/mint-ur-wellness",
    "text": "Inspiration In today's fast-paced digital world, we're all guilty of it—mindlessly scrolling through social media, captivated by content that adds little value to our lives. We know it harms our mental and physical well-being, yet the addictive nature of these platforms keeps us hooked. So I asked myself, why not turn the tables? What if mental and physical wellness could be just as addictive? What if improving ourselves became a game we couldn’t stop playing? That’s the bold idea behind Mint-Ur-Wellness—a revolutionary platform that transforms health into an engaging and rewarding journey, just like the games and apps we can’t put down. What it does Mint-Ur-Wellness is a next-generation wellness platform that merges physical fitness with mental well-being in a gamified, seamless experience. Picture this: physical exercises guided by smartglasses that track your every movement, providing real-time feedback and scoring your accuracy. On the mental wellness side, I’ve integrated activities such as journaling, to-do lists, goal setting, and self-check-ins. Every time you complete an activity—be it physical or mental—you earn points. But here’s the twist: those points can be used to mint NFTs, making self-improvement not just rewarding, but fun, and dare I say, addictive. How I built it The foundation of Mint-Ur-Wellness starts with the smartglasses I developed. Using an Arduino BLE, OLED display, and ESP32, I created a device that is not only lightweight but also highly efficient in transferring data via Bluetooth. The sensors inside track your movements with precision, sending the data to the smartglasses for real-time feedback, all written in the Arduino IDE. Choosing BLE and ESP32 was a deliberate move—they deliver seamless, real-time communication without compromising on speed or accuracy. On the software side, I built the platform using Next.js, layering in a suite of APIs to supercharge its functionality: To top it off, I leveraged the Verbwire API for the NFT minting process, allowing users to seamlessly turn their wellness journey into a collection of unique digital assets. Payments and transactions are handled through MetaMask on the Ethereum blockchain. Plus, users can describe their dream NFT to a bot, and it will create it for them. And for those who don’t have an image? No problem—there’s an image generator in place to make sure no one misses out on the fun. Challenges I ran into Creating a platform that makes mental and physical wellness as addictive as a game wasn’t easy. The toughest challenge? Building the smartglasses. They demanded a significant engineering push, ensuring the sensors were precise, and the data transfer fast enough to provide immediate, actionable feedback. Navigating the Bluetooth communication complexities, fine-tuning the OLED display, and bringing it all together took serious effort—but the results speak for themselves. Accomplishments that I'm proud of The fact that I’ve managed to make wellness not only fun but addictive is something I’m incredibly proud of. I’m also proud of the highly technical integration of Arduino BLE with real-time feedback through smartglasses—this was no small feat. Beyond that, the ability to blend AI, hardware, and blockchain into a single, unified experience where users can mint NFTs as rewards for their wellness journey is something I consider a technical triumph. The synergy of these elements is what truly sets Mint-Ur-Wellness apart. What I learned This project has been a deep dive into both hardware and software development. On the hardware front, I got hands-on experience with Arduino components, Bluetooth Low Energy, and OLED display technologies—pushing the boundaries of what’s possible in personal fitness devices. On the software side, building a tech stack with Next.js, OpenAI, Stripe, and Clerk has made me much more confident in creating scalable, monetizable applications. Learning to integrate blockchain technology and NFT minting has been a game-changer, giving me valuable insights into the future of digital assets and their role in user engagement. What's next for Mint-Ur-Wellness I’m incredibly passionate about the intersection of mental and physical wellness, and I firmly believe that Mint-Ur-Wellness is just the beginning. My vision is to scale this platform, bringing it to as many users as possible. The blend of NFTs, Blockchain, and AI has endless potential to revolutionize how people approach their health. I’ve already integrated Stripe to lay the groundwork for monetization and future growth. My next step is to build upon this foundation, reaching new users, and making wellness a lifestyle people want to stick to.",
    "matched_prize": null
  },
  {
    "title": "Spaces",
    "link": "https://devpost.com/software/spaces-fnt5yi",
    "text": "Built With",
    "matched_prize": null
  },
  {
    "title": "Survo",
    "link": "https://devpost.com/software/survo",
    "text": "What it does Survo is a first of it kind end-to-end dashboard tool that manages surveillance footage and performs AI video analysis with an integrated LLM to ask questions and monitor your footage. Want to ask questions like 'When was my package stolen?' or 'Did anyone come to my door last night?' without having to sift through footage? Survo is the solution. How we built it Survo leverages Google's video analysis tools and bridges the gap with Groq's quick API to allow for LLM chats regarding your footage. Moreover, through Auth0 your data is secure and with ChromaDB responses are constantly being refined for optimal performance. The app also uses AWS DynamoDB to reliably store and read data. Challenges we ran into We ran into issues with database usage to store the footage records of users. In order to forward analysis context to Groq for LLM conversation's we needed to re-adjust our schema to enable the ability to retrieve keys for any specific footage of a user. Accomplishments that we're proud of We're proud of our ability to deliver a first of it's kind product with results that achieve the overall goal we initially aimed to accomplish. What we learned We learned a lot about different API's and creating a fullstack app using Flask. What's next for Survo The next steps are to improve video processing time and aim for real-time processing by using a more efficient in-house video analysis tool.",
    "matched_prize": null
  },
  {
    "title": "Yap the North",
    "link": "https://devpost.com/software/yap-the-north",
    "text": "Inspiration i think the idea kind of just popped in our heads What it does Yap the North is a game where you have to speedrun getting 3 other teammates! The twist is that you have to actually speak to the characters, through your computer audio. The characters all have unique personalities and different specifications for whether they'll join your team or not, and will respond differently, depending on what you say. How we built it Each character is a custom prompt-engineered LLM with conversation history. To determine whether or not they wanted to join your team, we run each response through a secondary LLM and generate a value between 0-1. The interface is a React website built with Vite. It records audio and sends the Blobs to the backend, which then returns the responses from the different LLMs. Challenges we ran into Accomplishments that we're proud of This project was a ton of firsts for us, as we: also proud that we persevered through having to remake the game with a completely different stack. at 5am. What we learned game dev is hard. What's next sleep, probably.",
    "matched_prize": null
  },
  {
    "title": "Remy",
    "link": "https://devpost.com/software/remy-the-rat",
    "text": "Inspiration Our team was inspired by the beloved movie Ratatouille, a film that shaped our childhoods and sparked our love for cooking. As university students, we’ve faced the challenges of balancing cooking with busy academic schedules and knew there had to be a way to make it easier and more fun. That’s why we decided to bring Remy, the genius chef rat, from the screen into reality! With our project, anyone can cook—no matter how busy or inexperienced you are in the kitchen. Remy will be your guide, helping you whip up delicious dishes with ease. What it does Remy is your personal kitchen guide, sitting right on your head—just like in Ratatouille. Simply say, “What’s up, Remy?” to initiate a conversation. Not sure what to cook? Using his \"eyes\" and \"ears,\" Remy will fetch a recipe based on what you say, and guide you through every step. He’ll even give playful tugs on your hair to steer you toward the right ingredients and tools in your kitchen—just like in the movie! Feeling lost in the sauce? Ask Remy anything, and he’ll answer instantly. No more scrolling through recipes on your phone with greasy hands—Remy’s got your back, helping you chef it up with ease and fun. How we built it We built Remy using a multithreaded system on a Flask backend, running a total of five threads to manage real-time tasks. One thread constantly listens for audio input, which is captured using a mobile phone acting as both the microphone and camera due to hardware limitations with the Raspberry Pi. The audio is transcribed into text using speech-to-text APIs, and another thread handles text input, processing it through a fine-tuned OpenAI model optimized for cooking instructions. A separate thread replays responses using text-to-speech, while the fifth thread captures footage from the phone’s camera for analysis and uploading. For the physical interaction, we used a Raspberry Pi to control servo motors that power Remy’s animatronic movements. Although the original plan included complex servo movements, we simplified them due to hardware failures. Remy can still perform basic motions, guiding users in the kitchen. Additionally, we developed a dynamically updating frontend using React and Socket.io, allowing users to track the images captured by Remy, view the questions they’ve asked, and see the answers Remy provides in real time. We also leveraged TensorFlow machine learning models to help Remy identify ingredients for recipes, such as distinguishing between different fruits. Challenges we ran into Our project faced several challenges, particularly with hardware and integration. Initially, we planned to use a microphone and camera directly connected to the Raspberry Pi, but we couldn’t procure a working setup in time. As a result, we shifted to using a mobile phone for audio and visual input, which required adapting our system to handle external hardware. Additionally, we encountered issues with the servo motors used to animate Remy. During testing, multiple servo motors failed, and with limited hardware resources available, we had to simplify Remy's movements to basic gestures. We also had to learn sewing techniques to embed the servo motors into Remy’s skeleton, dealing with snapping strings and other complications in the process. On the software side, we had to manage the complexity of multithreading across five separate tasks on a Flask backend, ensuring that the real-time audio processing, transcription, and camera input didn’t cause delays or crashes. Balancing these threads while keeping performance smooth was a challenge, especially when processing multiple inputs simultaneously. We also had to fine-tune machine learning models using OpenAI and TensorFlow to help Remy correctly identify ingredients in real time. This required a lot of iteration, as the models needed to accurately distinguish between similar-looking ingredients, such as different fruits, which took considerable time and effort. Lastly, integrating all of these components into a coherent, user-friendly experience was a significant challenge. Developing a dynamically updating frontend using React and Socket.io, while syncing it with real-time data from the backend, took careful coordination. Despite these obstacles, we were able to successfully deliver a functional, interactive kitchen assistant. Accomplishments that we're proud of We are incredibly proud to have presented a coherent, functional solution made out of multiple machine learning models, showcasing our abilities of problem solving and working under pressure. When faced with complications, such as failed hardware and problems with our trained models, we were able to pivot efficiently and find solutions on the fly. Another achievement was successfully integrating all the hardware components into a cohesive system, overcoming the complexities of putting everything together. What we learned Throughout this project, we gained valuable insights into integrating diverse technologies and handling real-world constraints. We learned the intricacies of multithreading on a Flask backend, managing multiple simultaneous tasks while maintaining real-time performance. Working with machine learning models in TensorFlow taught us how to fine-tune models for specific applications, such as ingredient recognition. We also discovered the importance of adapting to hardware limitations and learned practical skills like sewing to integrate servo motors into animatronics. This project reinforced our problem-solving abilities and highlighted the significance of flexibility and innovation when facing unexpected challenges. What's next for Remy the Rat Looking ahead, we plan to enhance Remy's capabilities with additional features and improvements. We aim to refine the animatronics by integrating more advanced servo motor movements and exploring alternative materials to improve durability and functionality. Expanding Remy’s knowledge base with more comprehensive recipe databases and multilingual support will make him even more versatile in the kitchen. We also envision incorporating more sophisticated computer vision algorithms to better identify and interact with ingredients. Additionally, we want to explore potential integrations with smart kitchen devices for a more seamless and efficient cooking experience. Our goal is to continue evolving Remy into an even more helpful and interactive kitchen assistant.",
    "matched_prize": null
  },
  {
    "title": "Aisle Atlas",
    "link": "https://devpost.com/software/we-cooked",
    "text": "Inspiration Have you ever been frustrated looking for a grocery item, only realizing after that you passed by it multiple times? Ever wish you could receive store guidance without stepping out of your social bubble? We are introducing Aisle Atlas! An interactive, computer vision companion residing right on top of your head. With its AI capabilities and convenience of use, our device allows anyone to become an \"employee\" of a supermarket. Through SMS messages, localization and effective mapping of grocery items, we aim to increase the efficiency and shopping experience for all. What it does Imagine needing to buy an item/items but you're in a rush to be somewhere else. Maybe someone you know is already at the supermarket and only a text message away. Using a simple SMS text, Aisle Atlas allows you to send a grocery list that is automatically received. The items are then mapped immediately for the other shopper to find the shortest algorithm to grab all the components, with detailed instructions to arrive at each \"station\". We then use localization to determine our current position within the store and the required path to each item. Once an item has been \"completed\", there is a basic fingerprint sensor attached to the side of our device. With one tap, that item is no longer at the top of the queue. You can track with live feed the positioning of the shopper and updates in real-time. How we built it Our team wanted to implement a mix of both hardware and software. We 3D-printed a headband and support compartment, housing a Raspberry Pi, camera, batteries and a touch sensor. Our original idea was to attach the device to a hard hat, but we decided in the end to go with a sponsor bucket hat. This gave us more flexibility with materials and easier mounting conditions. We interfaced both firmware and software together to create a well-rounded project and demonstration. We also used vision-based localization and object detection, as well as MappedIn with live location tracking. Challenges we ran into In terms of challenges, our original plan was to use the Raspberry Pi Module 1.3 Cameras for our detection method. These were substantially smaller and convenient to place inside different types of headgear. We were to connect and see the camera availability but had increased difficulties taking a picture. In the end, we decided it would be simpler to implement a webcam for proof of concept, but its bulkier size was a new challenge on its own. Another issue was our SSH authentication originally. We wanted to film a video feed at a nearby convenience store, but this required both the laptop and Raspberry Pi operating on the same Wifi network. We had issues connecting the pi to hotspots and it made it difficult to wander anywhere other than HackTheNorth Wifi locations. Some other technical challenges were debating between stability vs latency tradeoffs. Tolerancing for 3-D prints was also difficult, as mechanical and electrical parts needed to integrate seamlessly for optimal performance. Accomplishments that we're proud of Our team had a lot of fun working together on this project. We all had accomplishments we were proud of. In terms of mechanical products, our team gained experience with 3-D printing and other manufacturing tools such as soldering. Our team integrated a Raspberry Pi and camera as the fundamental hardware of our project. It was great to see the impacts of our software in real life. For software, we discussed lots of problems together and worked through many backend and front-end integration issues. We completed vision-only localization and mapping using the SIFT algorithm. What we learned A huge step forward for us was learning about ngrok for rapid deployment. We gained well-rounded experiences in mechanical, electrical and software, and each worked on components that we enjoyed. There were lots of cool \"aha\" moments and we were excited to have fun together. What's next In terms of hardware, we weren't able to implement the finger sensor effectively within the deadline. It would be great to add more ways for the user to interact. Another issue was the method by which our instructions were posted. Currently, they are simply posted to a webpage, but we would love to explore mobile apps and audio instructions for increased convenience. Improving latency was one of the features we looked at from the software side. Long-term, our project originally stemmed from some automating convenient tasks. This would include using a robot instead of a human with computer vision capabilities to complete all shopping tasks. There are so many additional features such as gamifying, better AI and detection capabilities, and a wider range of items to explore.",
    "matched_prize": null
  },
  {
    "title": "Goose Guru",
    "link": "https://devpost.com/software/goose-guru",
    "text": "Inspiration 🚀 The idea for Goose Guru came from a need to bridge the gap between traditional mock interviews and the interactive, real-world feedback that aspiring developers crave. We wanted to create a platform that not only simulates coding interviews but also offers personalized, real-time feedback to boost confidence and skill. By combining AI with the familiar structure of coding interviews, Goose Guru helps users prepare more efficiently and effectively. What it does 💻🤖 Goose Guru is your AI-powered mock interview platform designed to elevate your coding interview prep! 🧠💬 Our AI agent guides (or judges) you as you code, offering a dynamic experience where you can code in your preferred language and engage in realistic interview scenarios powered by large language models (LLMs). Get detailed feedback on your technical execution and behavioural performance, complete with scores and clear improvement goals to drive your success! 🌟🎯 How we built it 🛠️ Challenges we ran into ⚠️ We faced challenges delivering a large project with 2 people. It was definitely less sleep than we wanted, but we are proud of our results! We also had some issues working with audio/speakers through WSL, but we somehow got it to work. Every challenge was a step closer to 8 hours of sleep. Accomplishments that we're proud of 🏆 We’re thrilled to have built a platform that not only simulates coding interviews but also provides real-time, actionable feedback. Successfully integrating LLMs for an engaging interview experience while ensuring smooth data flow through Convex has been a major win for us! For both of us this is our first time being exposed to Convex, so we were happy with how quickly we picked it up. Getting the LLMs interfaced properly was also a challenge worth boasting about! And we thoroughly enjoy the sleekness of our UI. What we learned 📚 We learned how to bootstrap AI models for educational purposes and integrate them with modern backend systems like Convex. For at least one of us, both cohere and convex are completely new technologies. Our experience taught us that not every 36 hours are the same! What's next for Goose Guru 🌟🔮 With growing interest in our idea, Goose Guru is exploring real-world entrepreneurial opportunities. We believe AI should enhance our abilities to learn faster and do better, not replace human creativity and expertise. Stay tuned as we continue to innovate and make AI a powerful tool for growth and success! 🚀💡",
    "matched_prize": null
  },
  {
    "title": "InPin",
    "link": "https://devpost.com/software/inpin",
    "text": "Inspiration While waiting for the Hack the North bus after flying in from California, we ended up talking to ~10 other contestants. But during our bus ride, we realized that we'd forgotten almost everybody's name, and since we hadn't gotten their LinkedIns, we effectively had no way to stay in touch. Realizing that the next three days were going to involve meeting hundreds of new people, we decided to make the InPin to have an easy way to connect with all of them. What it does Hang the InPin on your shirt and hit a button before you talk to somebody. The InPin focuses on key details like name, location, and place of work, while allowing you to focus on real conversation. As you walk away, hit the button again to stop listening and a fully automated and personalized connection request is sent straight to the person you just talked to. Look back at all the cool people you've connected with on the InPin Memories app. (and for those of you concerned with security, we promise we never save audio or video) How we built it Hardware: Raspberry Pi 4, Custom 3D Printed Casing, Breadboard, Button, Jumper Wires, DRW Sponsor Battery Pack (Rechargeable Power Source), some Superglue and Duct Tape Software: OpenAI, Cohere (robust query generation), Convex (InPin Memories app), Python, Flask, NextJS Challenges we ran into This was our first time building a hardware hack and we had no shortage of issues with everything from formatting our MicroSD card to wiring our breadboard. Most notably, from around 8 am on Saturday to 4 am on Sunday we spent almost all of our time trying to trying to SSH into our Raspberry Pi. Accomplishments that we're proud of As first time hardware hackers, holding our physical product in our hands is really a crazy feeling. We're proud that we stuck with the InPin through hours of failures and most importantly, we're proud of the fact that we definitely made the trip up from Cali worth it. What we learned Try new things! InPin wouldn't have been a thing if we didn't decide to take the chance and build some hardware. We had our ups and downs but this experience definitely taught us to be adventurous and step out of our comfort zone. What's next for InPin Make Memories searchable (for example you'd be able to look up something like \"who was that super interesting guy from Cali working on an automated connection wearable?\"). Lip reading to make audio processing more robust in noisy environments. Make the InPin smaller, cheaper, and more polished. The prototype we have right now is more of a proof of concept than anything, but we'd love to see this taken to a point where people can, for example, be handed out InPins at the start of a conference or networking event.",
    "matched_prize": null
  },
  {
    "title": "Recipe Radar",
    "link": "https://devpost.com/software/recipe-radar",
    "text": "Inspiration We wanted to build a tool that takes the stress out of finding new recipes to make. What it does Recipe Radar is a user-friendly app that allows you to search for recipes based on ingredients or meal types. It fetches recipe data from a large API database and displays the results in an easy-to-navigate interface. Each recipe comes with detailed ingredient lists, preparation steps, and even cooking tips. How we built it We built the app using a combination of Python and Flask for the backend and React for the frontend. We integrated a recipe API to fetch data and present users with results. Javascript and Bootstrap were used for the interface of the app. Challenges we ran into The biggest challenge we faced was learning how to work with an api Accomplishments that we're proud of What we learned What's next for Recipe Radar",
    "matched_prize": null
  },
  {
    "title": "KitchenVision",
    "link": "https://devpost.com/software/kitchenvision",
    "text": "Inspiration In the hustle and bustle of modern life, managing groceries and meal planning often feels like a daunting task. We wanted to create a solution that would transform these everyday challenges into a streamlined, enjoyable experience. Inspired by the growing need for smart kitchen management, we envisioned KitchenVision as a way to merge advanced technology with practical use. Our aim was to make it easier for people to track their grocery inventory, discover new recipes, and manage their meals—all from the convenience of their smartphones. What it does KitchenVision is an Android mobile app designed to revolutionize kitchen management with the following features: How we built it Receipt Scanning: Backend Processing: Recipe and Inventory Management: AI-Enhanced Recipe Suggestions: Challenges we ran into Accomplishments that we’re proud of What we learned Developing KitchenVision deepened our understanding of integrating diverse technologies into a cohesive system. We learned to manage complex data processing, handle AI-driven insights, and create a user-centric interface. The project underscored the importance of optimizing AI models and backend infrastructure for a seamless user experience. What’s next for KitchenVision KitchenVision is set to redefine kitchen management with future improvements, making meal planning and grocery management more intuitive and enjoyable. Sponsor-Specific Highlights Best Use of Databricks KitchenVision leverages Databricks to process and organize our extensive recipe dataset. We converted about 3000 complex categorical vectors from around 30,000 samples into a streamlined two-column dataset using PySpark modules and the Databricks Community platform. This integration optimizes data management and enhances the efficiency and scalability of our backend processing. Best Use of Cohere and Codegen We utilize Cohere’s LLM to process the text extracted from the CEIR model. Since all recognizable text information from the receipt is obtained, we apply dynamic attention to different components of the text for recipe recommendations and personalized cooking instructions. Cohere’s LLM selects grocery-item-related information from text files for future inference. Additionally, we applied Cohere’s LLM to process our recipe database, which contains about 3,000 example ingredients. A recipe recommendation system is constructed based on whether the items from the receipt can be used to prepare specific dishes. Some items may be operationally similar to ingredients in the dataset and serve as substitutes. To address this, Cohere’s LLM categorizes tokenized ingredients and relates grocery items to existing categories for generating recipe recommendations. For Codegen, Cohere’s LLM enables advanced text extraction and culinary semantic alignment between tokenized text systems. This interconnected processing system allows our project to perform more sophisticated tasks such as recipe recommendation. Working in an Air-Gapped Environment Our app is designed to function efficiently even when disconnected from the internet. The recipe search and personalized cooking instructions are localized on the phone, allowing the app to operate independently. If the mobile device has enough computational power, the deep learning OCR model can be offloaded, ensuring full functionality in an air-gapped environment.",
    "matched_prize": null
  },
  {
    "title": "Novel Novels",
    "link": "https://devpost.com/software/novel-novels",
    "text": "Inspiration We asked ourselves, where are all the picture books? What it does It allows the user to read their favourite books accompanied by AI-generated pictures. How we built it React, Node, and HTML. Challenges we ran into This was our first time working with React and Node. React and Node are both very confusing. Accomplishments that we're proud of Creating an interface and having some of the functionality we planned. What we learned We learned to use React and Node and to use git with care. What's next for Novel Novels We want to implement more educational features, like an AI voice reader, a dictionary, and a mobile app with an interface similar to Instagram Reels.",
    "matched_prize": null
  },
  {
    "title": "GooseGuard",
    "link": "https://devpost.com/software/gooseguard",
    "text": "About the Project We were inspired to create this project because, like everyone else, we are constantly bugged and frustrated by call scammers who seem to be more persistent than ever. However, the situation became personal when one of our team members walked in on their parents being scammed out of money. Shocked that even their tech-savvy parents could fall for such deception, we realized the severity of the issue and wanted to develop something that could effectively tackle these scams. How We Built the Project We built the program using React/Next.js for the frontend and Convex, a sponsor tool, for the backend. Using Convex presented its own set of challenges, as we initially found it difficult to learn under the time crunch of this hackathon. However, we persisted with it due to its ability to quickly set up a backend database and its built-in potential for Google API authentication. Unfortunately, we were unable to get the Google API authentication fully operational. For identifying scam content, we utilized the BERT (Bidirectional Encoder Representations from Transformers) Transformer Model. The model was fine-tuned on two large datasets of spam calls and spam texts found on Kaggle. We were asked to dream big for Hack the North and we did just that! Our model consists of 340 million parameters! It achieved an impressive accuracy rate of 96%. The model processes input by tokenizing it, running it through BERT, and determining whether the input is likely to be a scam based on known patterns and scam scripts. To transcribe audio from calls to text, we used another sponsor tool, Groq. Groq significantly decreases the translation time when compared to running on a large language model (LLM). This allows us to efficiently convert audio input into text, which is then processed by our BERT model. This integration significantly speeds up the workings of the language model, providing quick and accurate scam detection. GooseGuard's Multi-Layered Scam Detection We designed GooseGuard to provide the most robust and accurate scam detection across phone calls, text messages, and emails by implementing multiple layers of scam detection: Time permitting, we also planned on implementing the following layers of scam detection: These layers work together to improve the model's accuracy in identifying scam content, providing a more comprehensive protection mechanism. Challenges We Faced One of the main challenges was learning how to use Convex effectively. While it required a bit of a learning curve, we found it valuable for quickly setting up a backend database and exploring features like Google API authentication, which we worked to integrate into our project. Another challenge we encountered was understanding the limitations of mobile development. Initially, we spent a significant amount of time working towards developing a mobile app. However, we soon realized that the security restrictions on iPhone and Android would make retrieving emails, text messages, and phone calls a highly complex and slow process. This realization led us to pivot towards building a web application, a decision we made late into the hackathon. Finding a holistic dataset to fine-tune our BERT model presented another challenge. After researching and going through a lot of literature on relatable datasets we came across two that resulted in the best outcome. What We Learned Throughout this project, we learned a great deal about machine learning, backend development, and scam detection patterns. Utilizing BERT for natural language processing and Groq for audio transcription proved to be both highly efficient and rewarding. It has been an enriching experience to build a tool that can potentially help protect individuals from online scams, giving us a sense of positive social impact. Working together in a short 32-hour window brought us closer as a team. It was fascinating to learn about each other’s different experiences and perspectives, forming strong bonds that we will carry beyond this hackathon. Additionally, meeting a diverse array of students from across the globe—representing all populated continents—was inspiring and contributed to our growth. This collaboration and shared mission for positive impact have made this hackathon an unforgettable journey. Future Development Moving forward, we plan to continue implementing the layers of scam detection and fine-tuning our model for even greater accuracy. We also aim to focus on robust authentication features to further secure our platform. Most importantly, we intend to develop a mobile application better suited for our target population, as our phones are the primary contact point that scammers use through calls, messages, and emails.",
    "matched_prize": null
  },
  {
    "title": "Idetic",
    "link": "https://devpost.com/software/idetic",
    "text": "Inspiration Remember that moment from your sibling’s birthday party you wanted to share with your family? Now imagine sorting through hundreds of birthday videos and hours of footage to find it. That’s what inspired us to create Idetic. What it does Idetic is a video search engine. Upload your video to our video library, and we automatically categorize it into relevant chunks, allowing you to search through your videos as if they were text. How we built it We built Idetic through the use of a Convex & Next.js backend with integrations with Auth0 for authentication, and Convex's database for file storage. Our ML components use models like Whisper and clip running on a machine, layered with Convex and FastAPI for full-stack communication. Audio processing was handled with ffmpeg, and video processing with OpenCV2. We then performed vector similarity searches to the generated embeddings through Convex offerings. Challenges we ran into Accomplishments that we're proud of We're super proud of the fact we managed to string together a super complex project as far as we did within the 36 hours given. Vector embeddings for videos can get quite difficult and building recommendation systems alongside search engines is a challenge. What we learned What's next for Idetic Launch at some events and watch how people share memories!",
    "matched_prize": null
  },
  {
    "title": "Tenuto",
    "link": "https://devpost.com/software/tenuto",
    "text": "Inspiration Imagine being able to instantly find the perfect song to match any given moment… With Tenuto, this becomes reality, while also providing users with accessible mental health support in a new way. Tenuto utilizes music as a way to regulate mood in a way that is customized to the user’s specific emotional needs in real time. What it does Simply connect your Spotify and Fitbit accounts, then input your mood and situation into the app. Tenuto uses your emotional state, heart rate, and music preferences to provide the perfect song recommendation for any given moment. Tenuto is a musical therapy app designed to help users relax and improve their mood through personalized music suggestions. These recommendations are based on the user’s saved music from Spotify, combined with their heart rate variability, which provides a more accurate picture of stress levels than heart rate alone. Heart rate variability reflects the balance between your sympathetic and parasympathetic nervous systems, offering insights into your overall well-being. While a high heart rate could indicate stress, it could also result from excitement or physical activity, making heart rate variability a more reliable measure of your emotional and physical state. How we built it We used React Native for the front-end development, enabling cross-platform compatibility for both iOS and Android. On the backend, we integrated the Fitbit API to gather real-time heart rate data, which was processed using Convex for fast data management. We also leveraged the Spotify API to access the user’s playlists and music preferences. Our algorithm encompasses a wide array of nuanced human emotion, we used Cohere’s large language models to process the user’s inputted mood and emotional state, using Pinecone as a vector database. This enabled us to produce accurate personalized recommendations based on contextual inputs. Challenges we ran into The original plan was to integrate Apple Watch to analyze the user’s heart rate, but we ran into issues with accessing the information from Apple. It required using XCode while we had already built the remainder of our project in VSCode with React Native. Although integrating this data would be possible, we opted for using Fitbit instead, as their data is more easily accessible and updates faster. Additionally, fine-tuning our recommendation algorithm to effectively combine heart rate data, emotional input, and music preferences was complex, but Cohere’s API helped smooth out the process. Accomplishments that we're proud of We were able to build an algorithm using Cohere's embeddings, and generation and chat features that analyzes user input and heart rate data to deliver personalized music recommendations. The integration of real-time heart rate variability from Fitbit with Spotify’s extensive music library was a breakthrough, making the user experience more immersive and responsive to emotional needs. Additionally, we successfully created a seamless flow between various APIs and databases using Convex, ensuring the app functions efficiently across platforms. What we learned We learned how to interact with Convex for effective backend management, allowing us to handle real-time data efficiently. Additionally, we explored Cohere's language models and gained insights into leveraging sentiment analysis for mental health applications. We also explored Pinecone's vector database for optimizing music recommendation data storage and retrieval. The experience enhanced our understanding of integrating multiple APIs and managing them within a cross-platform app environment. What's next for Tenuto Our next steps for Tenuto include adding support for more wearable devices, such as Apple Watch, to expand the app's compatibility and reach a broader user base. We also plan to further refine our algorithm by incorporating machine learning to provide even more accurate and adaptive recommendations based on a user's evolving emotional and physiological states, as well as their music taste. Additionally, we aim to integrate a journaling feature, where users can track their mood over time and see how different music selections have affected their well-being. Finally, we hope to implement voice input features and implement sleep and study music features by incorporating customized binaural beats and other soothing soundscapes.",
    "matched_prize": null
  },
  {
    "title": "Project Tee",
    "link": "https://devpost.com/software/project-tee",
    "text": "Inspiration All three teammates had independently converged on an idea of glasses with subtitles for the world around you. After we realized the impracticality of the idea (how could you read subtitles an inch from your eye without technology that we didn't have access to?) we flipped it around: instead of subtitles (with built-in translation!) that only you could see for everybody else, what if you could have subtitles for you that everyone else could see? This way, others could understand what you were saying, breaking barriers of language, distance, and physical impairments. The subtitles needed to be big so that people could easily read them, and somewhere prominent so people you were conversing with could easily find them. We decided on having a large screen in the front of a shirt/hoodie, which comes with the benefits of wearable tech such as easy portability. What it does The device has three main functions. The first is speech transcription in multiple languages, where what you say is turned into text and you can choose the language you're speaking in. The second is speech translation, which currently translates your transcribed speech into English. The final function is displaying subtitles, and your translated speech is displayed on the screen in the front of the wearable. How we built it We took in audio input from a microphone connected to a Raspberry Pi 5, which sends packets of audio every 100 ms to the Google Cloud speech-to-text API, allowing for live near real-time subtitling. We then sent the transcribed text to the Google Cloud translate API to translate the text into English. We sent this translated text to a file, which was read from to create our display using pygame. Finally, we sewed all the components into a hoodie that we modified to become our wearable subtitle device! Challenges we ran into There were no microphones, so we had to take a trip (on e-scooters!) to a nearby computer shop to buy microphones. We took one apart to be less bulky, desoldering and resoldering components in order to free the base components from the plastic encasing. We had issues with 3D printing parts for different components: at one point our print and the entire 3D printer went missing with no one knowing where it went, and many of our ideas were too large for the 3D printers. Since we attached everything to a hoodie, there were some issues with device placement and overheating. Our Raspberry Pi 5 reached 85 degrees C, and some adapters were broken due to device placement. Finally, a persistent problem we had was using Google Cloud's API to switch between recording different languages. We couldn't find many helpful references online, and the entire process was very complicated. Accomplishments that we're proud of We're proud of successfully transcribing text from audio from the taken-apart microphone. We were so proud, in fact, that we celebrated by going to get boba! What we learned We learned four main lessons. The first and second were that the materials you have access to can significantly increase your possibilities or difficulty (having the 7\" OLED display helped a lot) but that even given limited materials, you still have the ability to create (when we weren't able to get a microphone from the Hardware Hub, we went out and bought a microphone that was not suited for our purposes and took it apart to make it work for us). The third and fourth were that seemingly simple tasks can be very difficult and time-consuming to do (as we found in the Google Cloud's APIs for transcription and translation) but also that large, complex tasks can be broken down into simple doable bits (the entire project: we definitely couldn't have made it possible without everyone taking on little bits one at a time). What's next for Project Tee In the future, we hope to make the wearable less bulky and more portable by having a flexible OLED display embedded in the shirt, and adding an alternative power source of solar panels. We also hope to support more languages in the future (we currently support five: English, Spanish, French, Mandarin, and Japanese) both to translate from and to, as well as a possible function to automatically detect what language a user is speaking. As the amount of language options increases, we will likely need an app or website as an option for people to change their language options more easily.",
    "matched_prize": null
  },
  {
    "title": "Not AI",
    "link": "https://devpost.com/software/not-ai",
    "text": "Built With",
    "matched_prize": null
  },
  {
    "title": "TechStyle",
    "link": "https://devpost.com/software/techstyle-acsutf",
    "text": "Built With",
    "matched_prize": null
  },
  {
    "title": "Build-A-Burger",
    "link": "https://devpost.com/software/hamburger-builder",
    "text": "Inspiration The central focus of our brainstorming session was: what are we all passionate about? And the undeniable, universal answer is obviously: FOOD! But the fact that we are all here at HTN means we are also passionate about building things. Food + engineering = cooking. With technology doing basically everything in our lives and the theme of this hackathon being DREAM BIG, we present: Build-A-Burger. What it does Build-A-Burger constructs a hamburger with your desired ingredients (buns, lettuce, patty). How we built it Physical burger builder contraption with recycled cardboard, 3D printed gears and gear rack tailored to specifications of the machine, motors controlled by ESP32 wifi microcontroller Challenges we ran into Accomplishments that we're proud of What we learned What's next for Build-A-Burger We can increase the features of the machine, for example, an extra condiments contraption. By adding and modifying various components we could also make it adaptable to a variety of foods, increasing its versatility.",
    "matched_prize": null
  },
  {
    "title": "Sips Don't Lie",
    "link": "https://devpost.com/software/sips-don-t-lie",
    "text": "Inspiration The idea for Sips Don’t Lie came from a very relatable problem: we often forget to drink water! 😅 Whether you're a busy college student or a working professional, staying hydrated is a challenge. But here’s something we all hate — public embarrassment! 😳 So, we thought, why not combine the two? What if your water bottle could publicly mock you every time you forget to take a sip? And that’s how Sips Don’t Lie was born, a playful solution to keep you hydrated, all while making hydration fun (and maybe a little embarrassing)! 🎤🥤 What it does Sips Don’t Lie is a smart water bottle that playfully calls you out if you haven’t taken a sip of water in 30 minutes. Using an ultrasonic range sensor, it tracks the water level in the bottle, and if you haven’t taken a sip, it generates a funny, publicly embarrassing reminder using the Voiceflow API. Meanwhile, the online dashboard adds a fun gamification element, letting you track and compare points with your friends based on how often you drink water! 📊🏆 How we built it We used an ultrasonic range sensor to measure the water level inside the bottle by calculating the distance to the surface. Multiple measurements are taken to ensure accuracy, even when walking. The data is processed by a microcontroller, and if no sip is detected, the Claude 3.5 LLM generates a humorous, voice-activated reminder through the Voiceflow API. The insult is then played via a Bluetooth speaker. To keep things fun, the system is linked to a real-time online dashboard, where users gain or lose points based on their water intake. 🎧💻 Challenges we ran into One of the toughest challenges was selecting the right sensor that would accurately track water levels without allowing users to cheat by just leaving the cap open! 🤔 After some research, we landed on the ultrasonic range sensor. Another challenge was fine-tuning the timing intervals and making sure the funny reminders felt fresh every time, thanks to Claude 3.5. Oh, and integrating the sensor with the Bluetooth speaker while keeping everything portable? Not easy! But we made it happen. 🔄🔊 Accomplishments that we're proud of We’re proud of developing a fun, engaging solution to a common problem — hydration! We successfully built a smart water bottle that not only tracks your water intake but also makes sure you stay accountable in a playful way. 💪 We’re especially proud of integrating the Voiceflow API and making sure each reminder feels fresh and personalized. Plus, adding a gamification aspect with real-time point tracking took the project to another level! 🚀 What we learned We learned a lot about sensor technology, specifically how to measure liquid levels accurately without margin of error. We also discovered the joys (and challenges!) of working with voice APIs like Voiceflow and how to craft a seamless user experience. Importantly, we learned how powerful a little humor can be in solving everyday problems! 😂 What's next for Sips Don't Lie! Next, we want to refine the voice prompts further, making them even more dynamic and personalized. We also want to improve the gamification dashboard by adding social features, allowing users to challenge their friends to hydration duels! 🔥💧 Eventually, we aim to introduce different bottle designs and integrate AI that tracks individual hydration needs, creating a fully personalized water-buddy experience.",
    "matched_prize": null
  },
  {
    "title": "HackerQuest",
    "link": "https://devpost.com/software/hackerquest",
    "text": "HackerQuest: Your Ultimate AI Interview Partner No LeetCode. No hassle. Just success. Introduction Ever struggled to find friends for a mock interview? Wondering why you still fail interviews despite solving over 1,000 problems on LeetCode? As a recruiter, have you ever lost top talent due to a shortage of available interviewers? HackerQuest is here to change the game. Our AI-powered interview platform is designed to eliminate the stress of interview prep and revolutionize the interview process for both job seekers and recruiters. Forget about mindlessly grinding LeetCode or coordinating mock interviews with busy friends. HackerQuest offers you not only instant, expert-level interview simulations with live feedback, but also communication skills training, and a variety of topics covering everything from coding to behavioral interviews and more. We believe in the power of equal opportunity. Our mission is to ensure that everyone has the chance to prepare effectively for their desired position. Stop wasting time on irrelevant problem-solving. With HackerQuest, you can focus on what truly matters for your career and your next role. Why Choose HackerQuest? Traditional interview prep is time-consuming and repetitive, with job seekers spending up to 80% of their job search time preparing for interviews. Finding available peers for mock interviews is hard, and they’re often not experts in your field. For companies, scheduling interviews with engineers for each candidate can cost $500+ per candidate and eat up valuable engineering hours. With HackerQuest, you save both time and money. Job seekers benefit from real-time, personalized interview simulations that provide actionable feedback, while companies streamline their recruitment process and reduce interview costs by over $1M per hiring cycle for large recruitment drives. Key Metrics: Key Features Real-time Interview Simulations: Get personalized interview experiences in multiple domains including technical coding, system design, behavioral, financial, and more. No more relying on static platforms like LeetCode or HackerRank. Multi-Agent AI System: A sophisticated AI ecosystem offering diverse interview types, driven by advanced language models and supported by real-time feedback. AI-Powered Live Conversation Bot: Powered by ElevenLabs for text-to-speech and Deepgram for speech-to-text, our system conducts live mock interviews, adapting to your responses in real time. Emotion Detection for Feedback: Using OpenCV’s emotion prediction, your facial expressions are analyzed frame-by-frame during the interview, enhancing your feedback to help you improve communication. Instant Problem Selection: Get the most relevant coding challenges tailored to your needs via Ollama’s fast, local inference models, and a database stored on ChromaDB hosted on AWS. Detailed Interview Evaluation: Powered by Cohere and Langchain, HackerQuest provides detailed, actionable feedback using a hidden rubric that evaluates your performance like an expert. Why HackerQuest? HackerQuest: Revolutionizing interview prep, so you can focus on what really matters: success.",
    "matched_prize": null
  },
  {
    "title": "Hackd",
    "link": "https://devpost.com/software/hackd-r8osly",
    "text": "Inspiration Finding teammates can be incredibly stressful. The Hack the North Slack had 1,710 members in the \"looking for teammates\" channel, and taking the step to message someone when you're a solo hacker can be terrifying. Everyone on our team had to take that leap, but some can't. That's why we created Hackd—an easier way to find the right hacker for the job. What it does Hackd analyzes your GitHub profile to understand what kind of coder you are and the complexity of the projects you've worked on. It also conducts sentiment analysis to identify people who can help meet your goals. But we didn't want to stop there. Recruiters come to hackathons to meet thousands of people. With Hackd, we can help them find the best hackers to join their teams or contribute to exciting projects. How we built it BACKEND Convex to run our database and store the user information, this includes authentication for github and all the information we gather from their account, the sentiment analysis linked to the github account, as well as the AI algorithm results of matches Cohere Would run the analysis of the github results and the sentiment scores to see who the matches were Python to scrape github users of their public repos, tech stack, and profile pictures FRONTEND React for our front-end development ... Challenges we ran into We had to learn new technology very fast here, Convex's dashboard and AUTH system gave us a lot of issues on figuring out the linking. Being able to manipulate Cohere to be able to read through such heavy files such as a git repo was also quite a challenge, we had to make an insanely fine-tuned model to ensure that it avoided all unnecessary information to speed up processing. Accomplishments that we're proud of We completed the project !! Hackd is polished and well made web app that is able to connect users attending hackathons with each other and ensure a more clean way to find your next collaborator. Being able to offer a solution that is DEI friendly, our app is able to connect users without the walls, until you find the person for the job you don't even know their name, and the lovely part about github is that once we show their icon it automatically isn't them. All they know is their work, work ethic, and how much they want to go. What we learned I think we should start with collaboration, this Hackathon was a physically demanding challenge and we pulled through together as a team, we all have each other to thank for it and brought us all closer. Looking solely at the techstack, we learned a lot about Convex and different data base companies out there, how they work and how we can manipulate them into our product. Parsing github was a huge learning curve, as well as how easy it is to get information out of it, the company wants us to have it, it's up to us to make it as beneficial for others as possible. What's next for Hackd Continuing the recruiting side of it. I believe we have a lot to grow in people able to connect top talent at hackathons with users, this includes being able to invite for a coffee-chat and having access to information like a resume at a click of a button. As this is a proof-of-concept our AI is not perfect, even with our fine-tuning it is still slow especially in larger repos. Finding a more efficient way to go through and grade it. We have also noticed at hackathons that there is no way to give feedback to teammates after. We think that kind of knowledge is a powerful thing that we should be able to do. A post hackathon, or coworking review could allow our algorithms to sort even better",
    "matched_prize": null
  },
  {
    "title": "LoveScenario",
    "link": "https://devpost.com/software/lovescenario",
    "text": "Inspiration After uncountable rejections and endless swiping, our team dreamt up a new approach to online dating. What it does Simply rate a couple of bios, and let AI do the matching. How we built it React Native for the frontend, C# for the backend. Challenges we ran into Learning new frontend technologies, using AI to match users, and deploying the backend onto the cloud. Not getting enough sleep. Accomplishments that we're proud of Using unfamiliar technologies, collaborating with new people. What we learned Start with more background research, and easy to use technologies. What's next for LoveScenario More advanced matching algorithms.",
    "matched_prize": null
  },
  {
    "title": "Hackademics",
    "link": "https://devpost.com/software/hackademics",
    "text": "Inspiration A couple of us started using Anki to study last term, and we quickly realized that one of the slowest parts of the process, especially when cramming for exams, is manually generating flashcards. The process is often repetitive, and when it involves media like PowerPoint slides or lecture recordings, it becomes even more time-consuming. While web services exist to automate the generation of Anki flashcards from text and other formats, we found that most of them aren't free. We believe students deserve a tool like this without any cost, as efficient studying shouldn't come with a price tag. What it does Our tool, Hackademics, automatically converts various file formats—such as text files, PDFs, PowerPoints, Word documents, and even audio recordings—into ready-to-use Anki flashcards. Whether it's a transcript of a lecture or notes from a presentation, Hackademics takes the input and generates an Anki deck to help you study more effectively and efficiently. This tool empowers students to focus on learning, not on tedious card creation. How we built it We developed Hackademics using Python and Flask to build the web interface and handle file uploads. For file conversion, python-docx and python-pptx were used to extract text from Word and PowerPoint files, respectively. Pdfplumber was used to extract text from PDF's, and audio transcriptions were handled using a custom transcription service. We integrated Cohere's LLM API to format the extracted text and create meaningful question-and-answer pairs for flashcards. This step ensures the content is well-structured for efficient studying. Cohere’s rerank() feature was used to identify and prioritize the most relevant questions based on the extracted content, ensuring that only the highest-quality question-answer pairs are included in the final deck. Once the text is processed and the question-answer pairs are generated, the output is converted into an Anki-compatible format (.apkg), which are automatically downloaded and imported directly into the Anki app, allowing immediate use for studying. The website was deployed for free with Render, and we used the free domain provided by GoDaddy + MLH. You can find Hackademics online at https://www.hackademics.study/. Challenges we ran into Handling audio transcription was another challenge. Converting speech into coherent text that could be transformed into effective flashcards proved difficult with most open-source packages due to limitations in accuracy and noise reduction. We wanted to ensure that the transcripts were not only accurate but also clean and concise, making them useful for generating high-quality flashcards. After evaluating various solutions, we chose to integrate AssemblyAI's API, as it provided the level of precision and reliability needed for this task. This allowed us to maintain a high standard of transcription quality, ensuring that the flashcards generated from audio content were as effective and meaningful as possible. Accomplishments that we're proud of We're incredibly proud to have developed a tool that students will actually use. Throughout the hackathon, many people who walked by our booth asked for the website link, which gave us confidence that there is genuine demand for Hackademics. Knowing that our tool can make a real difference in helping students study more efficiently is deeply rewarding. Additionally, by offering this service for free, we’re not only saving students time but also potentially saving them money compared to other paid services that provide similar functionality. Above everything, it feels great to know we've built something that has immediate, practical value for the student community. What we learned Throughout the development of Hackademics, we not only deepened our technical skills but also gained valuable experience in Flask development and project management. Building a full-stack application taught us how to handle file uploads, manage server-side processes, and deliver a seamless user experience, all while optimizing performance for large datasets like PDFs and audio files. In addition, we honed our project management skills, coordinating tasks across multiple team members, managing deadlines, and ensuring that we balanced feature development with quality assurance. This project reinforced the importance of clear communication, effective collaboration, and adaptability in a fast-paced environment like a hackathon What's next for Hackademics Expanded File Format Support: We plan to add support for more file types, such as HTML, Markdown, and images, allowing for even more flexibility in content creation. Enhanced AI Capabilities: We aim to refine the AI's ability to generate even more nuanced and subject-specific flashcards, potentially incorporating auto-generated hints, tags, and categories for more organized study sessions. Integration with Cloud Storage: We intend to allow users to upload files directly from popular cloud platforms like Google Drive and Dropbox, making it easier to access materials from any device. Collaborative Study Features: We’re exploring the idea of allowing users to share flashcard decks with others, encouraging collaborative learning and crowdsourced study materials.",
    "matched_prize": null
  },
  {
    "title": "Guardian",
    "link": "https://devpost.com/software/guardian-et4n25",
    "text": "Inspiration 💡 The inspiration behind creating Guardian, a machine-learning-powered patient monitoring system, stemmed from both personal experience and an acute awareness of the growing crisis in the healthcare industry. With a projected shortfall of 151,000 caregivers by 2030—rising to an alarming 355,000 by 2040—the need for innovative solutions has never been more pressing. Our team has felt this challenge firsthand: Adon’s sister works in healthcare research, providing us with a clear view of the immense difficulties caregivers face in maintaining the quality of patient care. On a more personal level, my father underwent heart surgery a few years ago. After being discharged from the hospital, he suffered a severe collapse at home due to complications with his medication. Luckily, we were present to intervene, but the incident raised a vital question: What happens to patients who are alone in such emergencies, without the immediate support of family or caretakers? This unsettling thought became the catalyst for Guardian, a system designed to bridge this gap, ensuring that patients and caregivers alike are better supported during moments of critical need. What it does 🚀 Guardian operates through two integrated components designed to streamline fall detection and patient care. The first component is an automated Python script that continuously monitors patients for falls using advanced computer vision technology. When a fall is detected, Guardian takes a multi-step approach to ensure a comprehensive response. The system first engages with the patient through a smart assistant, asking if assistance is needed. This interaction is processed using speech-to-text technology, combined with sentiment analysis to assess if the patient is in distress. If emergency help is required, Guardian automatically contacts emergency services, providing all essential details. Additionally, the system allows caregivers to notify close friends and family, ensuring swift communication. The smart assistant is equipped to understand and converse in multiple languages, thanks to its integration with Speecify and Google’s API. Furthermore, this entire process from fall to phone-call takes less than 10 seconds compared to the several minutes, potentially hours, that this process usually takes. The second component of Guardian is a web application designed for caretakers, close friends, and family. Through this platform, they can monitor live patient video streams, review fall logs, and access important metrics on patient activity levels at a glance. Guardian’s overall goal is to ease the healthcare and post-operative experience, offering peace of mind to both healthcare professionals and patients. How we built it 👨🏽‍💻 We built Guardian by carefully integrating a suite of technologies designed to enhance patient care through automation and real-time responsiveness. For fall detection, we utilised Python to automate key processes, leveraging Roboflow's SDK Inference for computer vision to accurately identify falls in real time. For the smart assistant functionality, we employed Speechify’s API to handle text-to-speech conversion, enabling seamless communication. On the input side, we incorporated Google’s API for speech-to-text, allowing the system to process verbal commands. We then utilised OpenAI’s API to perform sentiment analysis, ensuring the system could gauge the emotional tone of interactions. To complete the workflow, Twilio’s API was integrated to automate emergency calls and messages, ensuring that in the event of a detected fall, help is immediately alerted. For the monitoring component, we used Figma to create the intuitive design, developed a web application using NextJS, used TailwindCSS for styling, and MongoDB for the backend to manage data. Challenges we ran into 🕚 When building Guardian, we encountered several significant challenges that required careful problem-solving. One of the more technical hurdles was learning how to effectively implement WebSockets to stream webcam footage from one device to another, ensuring that live video feeds could be seamlessly displayed on the web app’s front end. This required a deep understanding of real-time communication protocols and overcoming latency issues to maintain a smooth user experience. Another key challenge was integrating multiple APIs within the Python script, each serving a unique function—computer vision, text-to-speech, sentiment analysis, and emergency notifications. Stitching these together into a cohesive and efficient system demanded rigorous debugging and orchestration, as ensuring smooth interaction between disparate services proved to be both complex and time-consuming. Accomplishments that we're proud of 🥇 When we entered this hackathon, our primary goal was simply to learn as much as possible, create a modest project, and enjoy the experience. As the 36-hour challenge drew to a close, we not only achieved these goals but exceeded our own expectations. We've walked away with an immense amount of new knowledge, shared countless moments of fun, and built Guardian—a project that we are genuinely proud of. The journey was as rewarding as the outcome, and what began as a learning experience evolved into the creation of something impactful and meaningful. What we learned 🧠 Throughout the development of Guardian, we gained valuable experience working with a diverse array of APIs, including Roboflow for computer vision, Speechify for text-to-speech, Google’s speech-to-text, and Twilio for automating emergency notifications. Beyond technical skills, we deepened our understanding of how to efficiently read and interpret documentation, troubleshoot bugs, and collaborate effectively within a team. We also learned some lighter lessons along the way—like discovering the upper limit of how many fruit snacks one can reasonably consume during a hackathon!",
    "matched_prize": null
  },
  {
    "title": "Ferney",
    "link": "https://devpost.com/software/ferney",
    "text": "Ferney Ferney is a plant companion that talks. The inspiration behind the project The idea for this project stems from a shared interest in making plant care easier and more interactive for users. Many people struggle with knowing when to water their plants, often either overwatering or underwatering them, leading to unhealthy plants. Additionally, caring for plants can feel like a chore, so we thought, why not add a fun, interactive element to the experience? That’s how the concept of an AI plant companion was born—a bot that not only helps with plant care but also adds a conversational, playful aspect to it. What it does Our project monitors soil humidity in real time using sensors. When the moisture level is either too low or too high, it triggers a water pump to either stop or start watering the plant. A built-in LED light blinks to alert the user when the water level is out of the optimal range. In addition to this, we have an AI-powered plant bot that acts as a companion to the user, providing them with real-time updates about their plant's status and even holding conversations through natural language processing. How we built it The project combines hardware and software elements. The sensors are connected to an Arduino board, which measures soil humidity and controls an LED light to indicate water levels. When the soil is too dry, the Arduino triggers a water pump to hydrate the plant. For the AI companion, we used Voiceflow to develop a chatbot that communicates with the user about the plant's status, interacts conversationally, and offers tips or updates. We also implemented front-end development to display sensor data in a user-friendly interface, connecting the Arduino and Voiceflow via APIs. Ferney is built around VoiceFlow, which powers most of the plant agent features including text-to-speech. The agent flows were built to facilitate interaction between the plant and the human. Additionally, the plant agent is aware of key metrics of the plant such as soil moisture and air humidity. The agent achieves this by fetching data from a sensor API that is powered by an Arduino board with sensors, which integrates directly with the plant. The user interface was built to facilitate interaction between the plant and the user by voice. This is done by extending VoiceFlow capabilities using the Whisper speech recognition model. The user interface listens to user speech, converts it to text and propagates it in text form to VoiceFlow. All together, this architecture makes the plant \"aware\" of its state and enables it to \"talk\" to humans. Challenges we ran into We initially had an idea that we add a speech to text feature on Voiceflow such that the user can speak to the chatbot. We ran into challenges trying to integrate the speech to text feature, and after multiple tries and tight deadline, we decided to scrap the idea. What we learned Throughout this project, we learned a lot about integrating hardware with software, especially how to connect real-time sensor data to an AI chatbot system. We gained experience working with Arduino, sensors, water pumps, and LEDs, as well as deepening our understanding of AI development through Voiceflow. Additionally, working with APIs to connect different platforms taught us valuable lessons in web development and real-time data transmission. What’s next for Ferney Moving forward, we aim to enhance the AI plant companion by training it on more diverse plant datasets, making its interactions even more informative and personalized. We also plan to improve the system by adding more sensors, such as temperature and light sensors, to provide a more holistic approach to plant care. Moreover, we're looking to refine the UI and explore machine learning algorithms to predict watering schedules based on historical data.",
    "matched_prize": null
  },
  {
    "title": "Sooper Tooder",
    "link": "https://devpost.com/software/sooper-tooder",
    "text": "Inspiration Several of us have been TAs and have considered tutoring, but being able to market yourself to those who need to seek for such services can be difficult. On the other hand, it can be difficult to find a qualified and helpful tutor. Sooper Tooder (prounounced Super Tutor) is here to bridge that gap to empower educators and learners. What it does With personalized login flow and feed, Sooper Tooder is tailored to all educators and learners from democratizing the educational landscape to inspiring an online community of teaching talent. Leveraging natural language processing (NLP) technology, our platform aims to provide personalized feedback for both students and teachers through a mobile application. How we built it Challenges we ran into Accomplishments that we're proud of What we learned What's next for Sooper Tooder",
    "matched_prize": null
  },
  {
    "title": "Code Senpai",
    "link": "https://devpost.com/software/code-senpai",
    "text": "CodeSenp.ai - Your AI GF & DSA Tutor! 🌸✨ (づ｡◕‿‿◕｡)づ💕\nFeeling lonely? Struggling to crack those job interviews? 😞 Well, CodeSenp.ai is here to solve both your problems! 🎉 We've dreamed up a world where your AI girlfriend is not just a partner in love, but also your personal mentor in Data Structures and Algorithms. 🌈 What's CodeSenp.ai? Imagine having a cute, caring senpai who not only cheers you on but also teaches you how to ace coding problems on LeetCode! 💻💪 That's right! Our \"girl\" will motivate you, explain problem-solving techniques, and make you fall in love with both her and coding. We dream big. With CodeSenp.ai, we aim to revolutionize the creator market by building relationships without the creators needing to be actively present. Let our AI take care of building those bonds... Just like we did! (〃＾▽＾〃) 🗒️ How to Get Started? 🍡 🌸 Onboarding 🌸 🌸 The Problem We’re Solving 🌸 There are so many people who just need someone to cheer them on, wait for them after a hard day, and guide them through the challenges of adult life. 🥺 With CodeSenp.ai, we’re here to give you both emotional and educational support. She’ll remember your birthday 🎂, your preferences 🌈, and your coding progress! 📈 🛠️ Our Solution: An AI Anime Girlfriend! 🌸💖 Your new AI girlfriend will: She's here to be there for you, every step of the way! 🌸💕 📈 Go-to-Market: Students and Creators! 🌟 Let’s be real – almost everyone at Hack the North is single and looking for internships! 🙃 CodeSenp.ai is here to fill that void in both coding support and emotional connection. (✿ ♥‿♥) The content creator market is next! Influencers can use our AI GF to build develop their personas and foster deep relationships with their audience – without needing for in-person presence! 🎥✨ 💖 Features 🛠️ How We Built It Our CodeSenp.ai was built using the MERN stack with a sprinkle of ✨ TypeScript ✨ magic! Here’s the breakdown: Frontend: Developed with React (TypeScript) to create a smooth and interactive user experience. 🖥️💕 Backend: Built using Node.js and Express (JavaScript) for handling all the API calls and interactions with the database and AI. 🌐📡 Database: We use MongoDB to store user information, preferences, and chat history, making sure your AI GF remembers everything important to you! 🗂️🎀 AI Power: Integrated Claude AI and Eleven Labs APIs to bring our virtual girlfriend to life, providing her with the intelligence to teach, chat, speak and support you on your coding journey! We have made so many prompts and fine-tunements.. just to make her perfect! 🧠🌸 🎯 Future Plans! We envision CodeSenp.ai evolving into the ultimate AI companion and mentor for coders everywhere. With the ability to foster relationships, save creators time, and provide personalized motivation, she's more than just a coding tutor – she's a revolution in human-AI interaction! 🌸💻🌟 Ready to Meet Your Code Senpai? 💖👩‍💻 Join us on this journey to make learning DSA fun, motivating, and a little bit romantic. (✿❛◡❛) Go ahead, give her a try – she’s waiting to teach you AND steal your heart!",
    "matched_prize": null
  },
  {
    "title": "GooseGains",
    "link": "https://devpost.com/software/goosegains",
    "text": "GooseGains is perfect for anyone who struggles in reaching and maintaining their goal pace when running. Whether you aim to set a personal record or are just hoping to preserve your fitness level, GooseGains makes reaching your running goals more fun and attainable than ever. Inspiration The inspiration for GooseGain comes from our personal experience of struggling to find the motivation to run and keep pace while doing so. Despite hoping to improve our pace while running, What it does GooseGains is a mobile app designed to help runners stay on pace. It uses a speed tracker that triggers custom audio clips based on your performance. Dropping below your average speed? GooseGainscan remind you of an embarrassing or upsetting moment. What about when you hit your goal pace? GooseGainswill reward you with positive affirmations encouraging you to maintain your speed. By simply uploading positive and negative prompts to the app, your GooseGains will return AI-generated motivation that’s completely tailored to you. How we built it Over the course of 32 hours, we brainstormed and brought our idea to life! We used the React Native framework to develop our Android app and Expo to make development smoother, all new to us; which was part of the fun. Through using git version control we were all able to collaborate on one code base, and each deploy on our phones through Expo. Challenges we ran into With all team members being new to the frameworks and tools being used, we ran into several technical difficulties. However, these periods of trouble often meant innovating our ideas in new ways! At first, we wanted to include a very ambitious amount of features in our project, but through trial and error, we were able to deliver our end project. Accomplishments that we're proud of Something our team is specifically proud of is the ability to implement our AI mascot, Honk. This project was the first experience that the majority of our team had working with AI, which proved to be a great challenge, especially in the beginning. However, little by little, our team worked together to overcome this challenge and deliver an end product that we can take pride in. What we learned Through the creation of GooseGains, our team became well-familiarized with the process of trial and error, as well as being flexible with our ideas. While our original concept is one that is considerably different from our end product, we learned that projects should be something continuously improved upon rather than rigidly structured, and that we should not be afraid to stray from a set path if a more optimized one becomes available. What's next for GooseGains The next step for GooseGains would be to expand upon our methods to motivate our users. Many find encouragement in music, while others may be better motivated by different coaching mascots. Including more audio options for our users would be the next option for the app, as well as potentially breaking into exercise fields beyond running (e.g. biking, climbing).",
    "matched_prize": null
  },
  {
    "title": "Kiwi",
    "link": "https://devpost.com/software/kiwi-qy8kv5",
    "text": "Built With",
    "matched_prize": null
  },
  {
    "title": "Spark!",
    "link": "https://devpost.com/software/spark-4qvl2u",
    "text": "Inspiration As UW students, we've been immersed in the buzz of startup culture. However, when trying to dive deeper, we encountered a significant hurdle: connecting with startups was challenging. This struggle to streamline the research process when seeking information about companies, particularly for networking and job opportunities, led us to create Spark. ✨ Often, users need to scour various platforms for key details like company size, location, or industry—a time-consuming process. We aimed to build a tool that consolidates this information, allowing users to access relevant data faster and connect with potential contacts effortlessly. By leveraging personalized information, including a user's resume, we're semantically empowering connectivity and fostering networks. What it does 🛠️ Spark is a powerful, user-friendly platform designed to revolutionize how people interact with the startup ecosystem. Here's a breakdown of its key features: Intelligent Query Processing: Users can input queries ranging from broad requests like \"Fintech companies in NY\" to highly specific ones such as \"GenerativeAI firms in SF with < 100 employees in their first round of funding.\" Our advanced natural language processing system interprets these queries accurately. Comprehensive Data Retrieval: Leveraging vector databases, Spark identifies and presents the most relevant companies based on the user's query. It provides crucial information including: Interactive Playground: This feature enhances user experience and functionality: Real-time Data Visualization: Our platform offers dynamic charts and graphs that update in real-time as users explore different queries and datasets. 📊 Personalized Recommendations: By analyzing user interactions and preferences, Spark provides tailored suggestions for companies, roles, and networking opportunities. 🎯 Spark isn't just a search tool; it's a comprehensive ecosystem designed to empower founders, investors, job seekers, and anyone interested in the startup world to make informed decisions and meaningful connections. How we built it 🔧 Spark leverages a powerful, modern tech stack to deliver its innovative features: This combination of cutting-edge technologies allows Spark to offer a seamless, efficient, and feature-rich platform for exploring the startup ecosystem. 💡 Challenges we ran into 🚧 Accomplishments that we're proud of 🏆 What we learned 📚 Our journey with Spark has been an incredible learning experience: This project pushed us to expand our technical skills while also broadening our understanding of the startup world, preparing us for future innovations in this space. 💡 What's next for Spark Ultimately, Spark is designed to be a versatile tool that caters to a wide range of users within the startup ecosystem: Releasing this as a potential product in the future is something we will look into. No matter where you fit in the startup world, Spark is here to illuminate your path and ignite meaningful connections. Whether you're building, investing, or joining the next big thing, our platform is designed to streamline your journey through the dynamic landscape of innovation. 🌟",
    "matched_prize": null
  },
  {
    "title": "EyeMAP",
    "link": "https://devpost.com/software/eyec-6egat2",
    "text": "⁉️ Why? In 2020, there were 43.3 million blind individuals and 295 million people with Moderate or Severe Visual Impairment (MSVI) worldwide. Navigating large, complex buildings can be overwhelming for those with visual impairments, limiting their independence. Existing tools like canes or guide dogs are helpful but can't provide detailed, real-time navigation in indoor spaces. EyeMap was created to address this gap, offering a tech-driven solution that empowers visually impaired users to navigate confidently and independently through voice guidance and object recognition. 🌟 Inspiration We often get lost in large, complex buildings like E7 🏢, and it made us wonder 🤔 how visually impaired individuals handle such situations without assistance. This challenge inspired us to create a solution that empowers them to navigate independently, providing real-time guidance and object recognition to simplify their experience in unfamiliar environments. 🧭 What it does EyeMap is an accessory that \"sees\" 👁️ the world around you. We created the AI agent \"Joe,\" which provides voice-guided navigation 🗣️, helping users move through complex buildings by detecting obstacles 🚧 and describing what's in front of them. It is also able to accurately detect people. Whether it’s understanding the space around them or navigating to hard to find areas, EyeMap offers a seamless experience for users to navigate independently. 🛠️ How we built it We developed EyeMap using React ⚛️ for the front-end, Python 🐍 for backend processing, and FastAPI 🚀 for API management. Firebase 🔥 handles user data storage, and the Mappedin SDK powers the indoor navigation 🗺️. OpenCV was also used for image processing and Google Speech was used to transform speech to text. We used the Mappedin's provided 3d render for the frontend UI, using it to calculate closest routes, combining algorithms to detect the closest rooms, as well as tracking where the user is. Although the tracking is solely a simulation, we see great potential to improve upon the technology, such as 3D triangulating each user's location within the building using technology like wireless beacons, allowing the visually impaired to have access to where everyone is and how they would go about navigating towards them. 🚧 Challenges we ran into One of the biggest challenges was figuring out efficiently integrating the Mappedin SDK for indoor navigation. Mapping out complex venues with accuracy and real-time processing ⏱️ was tough. Dealing with the SDK on client side JavaScript made it hard for our back-end to receive and process data. Additionally, having the speech recognition to process many different types of commands with consistent accuracy proved challenging with all the variables that could go wrong. 🏆 Accomplishments that we're proud of We’re proud of creating a working solution that can guide visually impaired individuals through buildings 🏨 with real-time feedback. EyeMap successfully integrates voice navigation 🎙️ and object recognition 📦 to provide users with both orientation and awareness of their surroundings. Seeing the prototype perform in a complex environment is a huge accomplishment for our team. 📚 What we learned We learned how crucial it is to consider accessibility in technology development. This project gave us insight into user-centered design 🎯, especially the importance of balancing features with simplicity for the end-user. We also deepened our technical knowledge in API development 💻, navigation SDKs, and how to make various technologies work together seamlessly. 🚀 What's next for EyeMap We plan to refine EyeMap's voice recognition and improve its ability to navigate even more complex environments 🌍. Another important improvement is to increase the speeds of the LLM's to provide more responsive feedback. Future iterations might include more voice customization 🎙️, and support for multiple languages 🌐. We also hope to test EyeMap in real-world scenarios with visually impaired users to ensure it meets their needs effectively.",
    "matched_prize": null
  },
  {
    "title": "Number Predictor AI",
    "link": "https://devpost.com/software/number-predictor-ai",
    "text": "Built With",
    "matched_prize": null
  },
  {
    "title": "DevelopersAssemble",
    "link": "https://devpost.com/software/developersassemble",
    "text": "Inspiration During Hack the North, we realized how difficult it was to find suitable teammates to collaborate with. Browsing through Slack channels and random project ideas felt inefficient, leading to frustration. This struggle inspired the idea for Developers Assemble — a platform where developers can quickly find others based on skills and project needs, using a simple swiping model. What it does Developers Assemble connects developers looking to collaborate on projects. Users create profiles highlighting their skills (e.g., frontend, backend, full-stack), and can post or swipe on projects looking for teammates. The platform helps developers match with projects or other developers based on their specialties, making the process of forming a team easy and efficient. How we built it We built Developers Assemble with a focus on real-time interactions and scalability. The frontend was developed using React, Tailwind and Vite to create a dynamic and responsive user interface. For the backend, we used Django REST, with SQLite as our database. Challenges we ran into One of the biggest challenges we faced was integrating the backend and frontend for the first time. Integrating the React frontend with the Django backend posed difficulties, particularly in ensuring that the APIs and real-time matching features communicated seamlessly. This process taught us the importance of efficient communication between the frontend and backend, laying the groundwork for future scalability.Moving forward, we aim to add more features that will enhance the user experience and make Developers Assemble even more effective for connecting developers. In addition to team messaging, project management tools, and GitHub integration, we plan to introduce a ranking system where developers can be rated based on their contributions and collaboration skills. This feature will help teams identify the best matches not just based on technical skills, but also on teamwork and reliability. We also aim to refine the matching algorithm further to factor in these new ranking metrics, ensuring even more accurate and successful matches between developers and projects. By continuously improving these core functionalities, we hope to create a comprehensive platform that truly supports collaborative development. Accomplishments that we're proud of We’re proud of creating a seamless, intuitive platform that developers can use to find and connect with others for collaborative work. Successfully integrating real-time matching and building a system that scales with user growth were significant achievements. Additionally, building the platform from the ground up taught us important lessons about matchmaking algorithms and user experience. What we learned We learned that user experience is key — developers want a fast, easy-to-use platform that makes finding collaborators painless. We also learned a lot about building real-time systems and ensuring server efficiency. Fine-tuning the matching algorithm taught us how crucial it is to accurately pair users based on skills and project needs. Overall, the development process helped us gain a deeper understanding of collaboration dynamics in the tech space. What's next for DevelopersAssemble The next steps for Developers Assemble include adding features like team messaging, in-app project management tools, and integrating with popular developer platforms like GitHub and GitLab. We also plan to improve the matching algorithm further to offer even more refined matches, and expand the platform to cater to larger developer communities worldwide.",
    "matched_prize": null
  },
  {
    "title": "snip.",
    "link": "https://devpost.com/software/snip-395suc",
    "text": "Inspiration In our daily lives we noticed that we were spending a lot of time shopping online and weighing many options. When I was looking to buy a mechanical keyboard last month, I spent over two days visiting countless sites to find a keyboard to buy. During this process, it was frustrating to keep track of everything I had looked at and compare them efficiently. That’s why we built snip. It’s an ultra easy to use tool that extracts important features from your screenshots using AI, and automatically tabulates your data for easy visualization and comparison! What it does Whenever you decide you’re about to take a series of related screenshots that you would like to have saved and organized automatically (in one centralized place), you can create a “snip” session. Then, whenever you take a screenshot of anything, it will automatically get added to the session, relevant features will be extracted of whatever item you took a screenshot of, and will be automatically added to a table (much easier to view for comparison). How we built it We used Tauri to create a desktop app with a Rust backend (to interact natively with the device to monitor its clipboard) and a React frontend (to create the user interface). We also used the shadcn UI component library and Tailwind CSS to enhance our design. Challenges we ran into Our team is comprised primarily of roboticists, infra, or backend people, and so trying to create a visually pleasing UI with React, shadcn, and Tailwind CSS (all technologies that we’re not used to using) was quite difficult - we often ran into CSS conflicts, UI library conflicts, and random things not working because of the smallest syntax errors. Accomplishments that we're proud of We were able to finish creating our hack and make it look like an actual product. What we learned We learned to prefer to use technologies that give us more speed, rather than perhaps better code quality - for example, we decided to use typescript on the frontend instead of javascript, but all the errors due to typing rules made it quite frustrating to go fast. Also, we learned that if you want to build something challenging and finish it, you should work with technologies that you are familiar with - for example, we were not as familiar with React, but still decided to use it for a core portion of our project and that cost us a lot of precious development time (due to the learning curve). What's next for snip. Provide more powerful control over how you can manipulate data, better AI features, and enhanced ability to extract data from snapshots (and also extract the site they came from).",
    "matched_prize": null
  },
  {
    "title": "personalnote",
    "link": "https://devpost.com/software/personalnote-study",
    "text": "Inspiration As students, you probably all have played around with AI chatbots and (possibly) ran a couple homework problems through them. But, as you might have noticed, traditional chat bots have their drawbacks: 1. sometimes providing inaccurate information, and 2. not referencing the course material we want... Our team believed in a solution where we could easily feed in course notes, textbooks, and a variety of personalized source material to learn and practice off of. This way, we can create much more accurate, course-based answers and practice material on the go! What it does Personalnote is an application that allows its users to feed in .pdfs, text files, and images into its system as datasets. Then, each file is interpreted and trained off of. Once trained, users will be able to create practice questions using flashcards, study note summaries, and solutions to problems. For example, a user could upload their course textbook for calculus and ask for practice problems for an upcoming test. Like a personal tutor, Personalnote will respond with worked examples using extremely similar approaches as referenced in the textbook. As each response is pre-trained by each user, users can be more prepared for assessments on course material and receive significantly more accurate information. How we built it The majority of our file processing/training was done through OpenAI's new file search and vector stores feature. Challenges we ran into Our team was formed at the icebreaker events late Friday, and almost all of us were first-time hackathon attendees. Originally, we wanted to build our solution using Voiceflow's apis for their knowledge-based agents. But, we ran into a lot of trouble setting them up so ended up switching gears to OpenAI with only around a day left... It was definitely a time-crunching project! Accomplishments that we're proud of What we learned Pretty much almost everything we worked was new to us! From learning the basics of web like communicating between frontend and backend to just having fun with LLMs, working on a project like this really allowed us to touch on many new skills. Also, OpenAI's file search feature was really cool and easy to use and we plan on trying it out more in the future. What's next for personalnote.study We'd like to try out mobile app development which could make it much easier to use our image scanning feature.",
    "matched_prize": null
  },
  {
    "title": "Yoga Yogi",
    "link": "https://devpost.com/software/yoga-yogi",
    "text": "Inspiration Just Dance was probably my favorite game as a kid. Not only because I got to listen to Psy a million times a day, but because I loved to run and hop and dance around. At this hackathon, it was an inspiration for us to try and build something similar for everyone: a pose guiding app that helped people get active. What it does Yoga Yogi is the everyman’s yoga guide. Using pose recognition, Yoga Yogi recognizes your yoga poses and gives you individualized feedback on how to improve! Not only do we feature a login, you can design your very own yoga routine individualized to poses and duration goals. Once you are ready to start, Yoga Yogi will walk you through a guided yoga session, where you will receive live feedback and helpful tips. How we built it Our project connected multiple moving parts, including a React frontend, a pose recognition model, and a Flask backend. Using Flask had huge importance in our project as it connected our Voiceflow and AI backend to our frontend React elements. Voiceflow was fun to use as it was very easy to create user help agents as well as AI responses specific for programmed API calls. Collecting our data and training our model took several steps. To collect our data, we used mediapipe and cv2 to identify the major nodes of people in yoga poses and transformed these into graphs. We then used this dataset to train a graph neural network, which allowed us to classify pose images into distinct classes. Lastly, we used Voiceflow to generate customized feedback based on real time video. Challenges we ran into Since this was our first time working together collaboratively on Github, we struggled quite a lot with figuring out what merge meant and what branches were and whatever rebase meant. But after we got the hang of that, the next obstacle we faced was smoothly running the front and backend together, but not because of technical difficulties, but because of our difficulties with communicating with each other. All of us had unique skill sets, and we oftentimes struggled with communicating what our programs needed and outputted. This slowed us down a considerable amount, but despite this, we were able to make a project that was greater than our sum. Accomplishments that we're proud of Sean: As a returner hacker, I am very happy that this project was quite functional and was impressed with all the stuff we were able to fit in these few days utilizing the right planning, skills and even a bit of luck. I'm also glad that I played a crucial part in the project as well as being able to help the team in countless ways through my experience. I feel that we worked very well as a team and really appreciate HTN for giving me the chance to learn and develop another amazing project! Chris: I’m really happy with how simple and streamlined our UI is! It feels like a real app you would find online. This was the first time for me to work in a large group at a sprint pace, and although at some times I felt really tired, I really enjoyed my experience and would come again. Steven: I am really happy with the learning progress I have made during the 36 hours. I had almost no experience coming in with web design and with using React and other frameworks. I was able to learn so much on the spot and reflecting on the past 2 days of work, I feel accomplished and thankful for this opportunity. I want to also thank my team members for helping me with so much and teaching me new pieces of coding knowledge that I would not have known. Nirvan: I am very proud of the team effort and collaboration that we had the entire time. We worked really hard and supported each other whenever we had an error particularly those between the react frontend and flask backend. I am particularly proud of overcoming the issue of having multiple setIntervals in different components by passing props to a parent component and having one common useEffect to run all the setIntervals. What we learned A lot of front end and APIs! While the more technical parts are usually documented thoroughly, making a clean and satisfying frontend that we liked was much more creatively difficult than we thought. We each all had our own different thoughts and ideas about what we liked and what we wanted to put in, but making a project we were all proud of meant that we had to compromise on things that we didn’t want. What's next for Yoga Yogi We want to put Yoga Yogi on phones! This would allow Yoga Yogi to guide you through his practices, anytime, and anywhere. We also want to train Yoga Yogi on more poses, allowing Yoga Yogi to give you step by step feedback rather than broader statements.",
    "matched_prize": null
  },
  {
    "title": "Whisker Wheel",
    "link": "https://devpost.com/software/whisker-wheel",
    "text": "Inspiration Cats can be a toss-up... Some people love them, while some people hate them. But just like any other pet, cats need and deserve the opportunity to release their energy. Some owners may play with their own cats with different toys, while others may spend agonizing hours training their cats to only scratch some arbitrary pole within their homes. We realized that there could be an interactive avenue for cats to expend their energy at home that could come naturally to our kitten friends' instincts. What it does The Whisker Wheel is a quick, fun robot that constantly runs about with fluttering whiskers. Its speed, quick change of motion, as well as flashy whiskers are sure to activate the neurons of any feline friend. With its two DC motors, ultrasonic sensors, and IMU, the Whisker Wheel is capable of extremely unpredictable movement all while avoiding imminent obstacles. All these sensors communicate with an ESP32, which can be connected via Bluetooth on any device to gain insights as to your pet's activity. How we built it The Whisker Wheel contains the HC-SR04 Ultrasonic Sensor, a motor driver, two 48:1 DC motors, MPU6050 IMU, and an ESP32, all connected with many many jumper cables and two mini breadboards. On the software side, we were able to write all the firmware through Arduino using PlatformIO. Though a simple-looking bot, the firmware involved was actually quite extensive, with complex operations like Kalman filters performed on the accelerometer and gyroscope readings of the MPU6050 in order to extract meaningful data on our cat bots. Challenges we ran into Well... where to get started? The entire build process was a roller coaster ride from start to finish. Arriving late to Hack the North due to our flights, we were a a loss as to what to build while many other teams had already started fleshing out their dream projects. We actually went through many iterations of different project ideas before finally settling upon our final Whisker Wheel idea, and even then, the path was DEFINITELY not smooth. Wireless communication, for example, had been one of our goals, and while we had initially built up the entire project on an Arduino UNO, we soon came to realize that the Bluetooth module (HC-06) we had was completely faulty. With less than a few hours left, we had to bite the bullet and switch everything over to ESP32 for its built-in wireless capabilities. Accomplishments that we're proud of First and foremost, we are extremely proud to have built up this project to such an extent. We wanted to make something novel and fun throughout the entirety of our brainstorming projects, but we are most proud of how we were able to retain the project's novelty while managing to fit in many more technical elements to various aspects of our projects (e.g. wireless communications, Kalman filtering, etc.) What's next for Whisker Wheel During this hackathon, we were quite limited in our access to various sensor modules. We had to use Kalman filtering on an IMU to find many fields like orientation and position, when there were many better sensor modules for the job. With better sensors, we hope to be able to provide a more extensive array of gathered data. With more data, the ways we can entertain and play with our pets become endless.",
    "matched_prize": null
  },
  {
    "title": "Nibbles",
    "link": "https://devpost.com/software/nibbles-6pcd5f",
    "text": "Inspiration For Nibbles 🍽️ At Team Nibbles, our inspiration stems from our journey as first-year engineering students grappling with the challenges of a demanding new path. Balancing our academic workload with the desire to maintain a healthy diet, we faced the common struggle of finding time to prepare nutritious meals amidst our busy schedules. 🍴✨ Driven by the need for efficient meal planning and healthier eating, we envisioned Nibbles as a solution to simplify and enhance our culinary experiences. Our goal was to harness technology to transform the way we manage our pantry and meal prep, making it easier to cook with what we have and reduce food waste. 🚀🌟 The idea for Nibbles emerged from our own daily challenges with time constraints and the desire for a more organized approach to meal preparation. We wanted to create a tool that would not only streamline meal planning but also make nutritious eating more accessible, turning the complexity of managing ingredients into a seamless and enjoyable experience. With Nibbles, we aim to empower ourselves and others to make the most of their ingredients and maintain a healthy lifestyle, even amidst the busiest of schedules. 🥗🔍 What it does Nibbles is an intuitive food inventory manager designed to streamline meal planning and reduce waste. By focusing on key pantry ingredients, Nibbles transforms them into versatile \"STAR\" ingredients, offering tailored recipe suggestions based on what you have. As your inventory decreases, Nibbles dynamically adjusts recipes and generates smart shopping lists, providing timely recommendations on what to buy and where. This ensures you make the most of your ingredients while keeping your pantry efficiently stocked for future meals. How we built it Front End: We engineered an engaging and interactive front end for Nibbles using React. This technology allows us to design a dynamic user interface where users can easily input pantry ingredients and preferences. The responsive design provides tailored recipe suggestions and real-time inventory updates, ensuring a smooth and enjoyable experience as users manage their meals and pantry. 🌟🖥️ Backend: Our backend infrastructure is built on the robust MERN stack, incorporating MongoDB, Node.js, and Express to efficiently manage and process data. We maintain a comprehensive database of recipes and track real-time data from our hardware, including ingredient quantities and nutritional information. By integrating Redox and Axios for API interactions, we ensure seamless communication between the hardware and our application, delivering accurate inventory tracking and personalized recipe recommendations. 🚀🔧 Hardware: Nibbles employs advanced hardware to monitor pantry inventory with precision. We use ESP32 microcontrollers combined with load cells and amplifiers to measure ingredient weights accurately. The load cells provide precise measurements, and the amplifiers enhance signal accuracy. Through 3D printing and rapid prototyping techniques such as breadboarding and soldering, we’ve created and refined our hardware setup, ensuring reliable inventory tracking and smooth integration with our app. ⚙️📦 AI Integration: We leverage Cohere, a sophisticated AI platform, to enhance our recipe data management. Cohere's powerful language models enable us to analyze and extract relevant recipe information, offering a diverse range of culinary options based on users’ pantry inventories. By harnessing Cohere’s capabilities, we ensure that the recipes and suggestions provided by Nibbles are diverse, relevant, and tailored to individual tastes. 🤖🍲 Challenges we ran into Merging Front and Back End: Integrating the front-end interface built with React with the backend services powered by the MERN stack proved challenging. Ensuring a seamless data flow and maintaining a smooth user experience required meticulous coordination and synchronization to keep data consistent and performant across both layers. 🔄💻 Setting Up Physical Components: Designing and assembling the hardware, including load cells and amplifiers, while managing real-time updates to the MongoDB database via the ESP32, presented significant challenges. Ensuring accurate data transmission and effective integration between the physical components and the software required innovative problem-solving. ⚙️🔗 Setting Up the AI: Implementing Cohere for recipe data extraction involved overcoming complexities in configuring and fine-tuning the AI models. Achieving accurate and relevant recipe suggestions while ensuring smooth integration with our system was a critical challenge, requiring detailed adjustments and optimizations. 🤖📊 Accomplishments that we're proud of Bite Size Accomplishments at Nibbles 🎉 🚀 Seamless Hardware-Software Integration 🌐 One of our major accomplishments was successfully integrating our hardware components with the software platform. Designing and assembling the hardware, including the ESP32 microcontrollers and load cells, while ensuring smooth communication with our backend and frontend systems, was a complex task. We navigated this challenge by meticulously synchronizing the hardware with the MERN stack and React interface, resulting in a reliable and efficient system for tracking pantry inventory and delivering recipe suggestions. 💻🔧 🔍 Advanced Recipe Recommendations with AI 🤖 Implementing Cohere AI to enhance our recipe suggestions was a significant achievement. We tackled the complexities of configuring AI models to accurately interpret and process recipe data, enabling Nibbles to provide highly relevant and diverse culinary options. This accomplishment demonstrated our commitment to leveraging cutting-edge technology to offer personalized and dynamic recipe recommendations based on users' pantry inventories. 🍲📈 ⚙️ Efficient Real-Time Data Management 🗂️ Successfully managing real-time data from physical scales and integrating it with our MongoDB database was a key milestone. We ensured accurate and timely updates of ingredient quantities and nutritional values, overcoming challenges related to data transmission and synchronization. This achievement was crucial in delivering a seamless user experience and maintaining an up-to-date inventory and recipe system. 📊🔄 🌟 Enhanced User Experience with a Dynamic Interface 🖥️ Creating an engaging and responsive user interface with React was another major accomplishment. We developed a dynamic platform where users can easily input data, view personalized recipe suggestions, and manage their pantry efficiently. This accomplishment reflects our dedication to providing a user-friendly experience and making meal planning and inventory management more enjoyable. 🎨🚀 What we learned Implementing Cohere AI: We mastered the integration of AI models for recipe data extraction and processing with Cohere. By leveraging its advanced capabilities, we significantly improved the relevance and variety of our recipe recommendations, enhancing the overall user experience. 🤖📚 Using Load Cells: We navigated the complexities of load cells and amplifiers, mastering their calibration and integration. This expertise was essential for achieving precise measurements of ingredient quantities, ensuring reliable inventory tracking. ⚖️🔧 Communicating Between Backend and Hardware: We gained in-depth experience in bridging hardware components with our backend services. This expertise enabled seamless data flow and consistent performance, ensuring that our system operated smoothly and efficiently. 🌐🔄 Efficient Use of Git: We advanced our skills in using Git for version control, refining our development workflow. Improved code management and documentation streamlined our collaboration and enhanced the efficiency of our development process. 🛠️📂 What's next for Nibbles",
    "matched_prize": null
  },
  {
    "title": "LeetCoach",
    "link": "https://devpost.com/software/leetcoach",
    "text": "Inspiration LeetCoach was born out of the urgent need to bridge the gap between technical skill and communication under pressure, which often defines success in coding interviews. Traditional coding platforms overlook the importance of articulating your thought process, but that's where LeetCoach shines. We envisioned a platform that immerses developers in a high-stakes, interactive environment, preparing them to not only solve problems but to explain, adapt, and thrive in the most challenging interview settings. What it does LeetCoach offers an unparalleled mock coding interview experience, where an AI-driven interviewer interacts in real time to replicate the pressure of actual interviews. But it doesn’t stop at coding: the AI engages users, asking them to justify their decisions, probing for clarity, and offering strategic hints, all designed to refine both problem-solving abilities and communication skills. It’s more than practice; it’s the next best thing to a real interview. How we built it LeetCoach is the result of cutting-edge innovation and seamless integration of multiple technologies. At its core, the AI model processes real-time audio, transforming every session into an interactive, lifelike conversation. Using GROQ for computational efficiency, Cohere for advanced natural language processing, and Convex for a robust serverless backend, we created a dynamic platform. The system’s speech-to-text capabilities are powered by Google Cloud, while Auth0 ensures secure, effortless authentication. Challenges we ran into Our biggest hurdle was integrating real-time audio interaction with the large language model (LLM). Ensuring the AI could respond swiftly, with minimal delay, while preserving the high quality of responses, demanded significant optimization. Achieving this balance of speed and intelligence was no small feat, but solving it was critical to the user experience we envisioned. Accomplishments that we're proud of The real-time interaction between users and the AI interviewer is our proudest achievement. We overcame technical complexities to create a seamless experience where users engage in authentic, fluid mock interviews. It’s this level of realism that sets LeetCoach apart and prepares users like never before for the unpredictability of technical interviews. What we learned This project pushed us to the cutting edge of modern development, exposing us to an exciting array of APIs and technologies that each played a critical role in shaping LeetCoach. GROQ enabled efficient processing, while Cohere's natural language processing brought conversational depth to our AI. Convex allowed us to build a serverless backend with remarkable flexibility, and Auth0 ensured our authentication system was both seamless and secure. With Google Cloud Platform powering real-time transcription, we learned not only to integrate these technologies but also to harness their full potential in creating a cohesive and powerful user experience. What's next for LeetCoach LeetCoach is poised to take the next big step. With the official launch on the horizon, we’re gearing up to scale the platform into a thriving business. The vision extends far beyond just mock interviews; we’re working on expanding the feature set, refining the AI, and introducing more nuanced scenarios to mimic real interview dynamics. Our mission is to equip even more users with the skills they need to succeed in the high-pressure world of technical interviews, creating a platform that becomes the go-to resource for career advancement.",
    "matched_prize": null
  },
  {
    "title": "Stealth Startup",
    "link": "https://devpost.com/software/stealth-startup",
    "text": "⚙️Inspiration The inspiration for Stealth Startup came from the idea of automating the entire process of building a company. We wanted to create intelligent agents that simulate the roles of a CEO, CTO, and Marketer, each contributing to building out a startup from scratch, mirroring the experience of launching a real-world tech startup and seeing how a particular idea would play out in a given market. 💻What it does We were able to create the first fully self-functioning startup run purely by AI, there's no human interaction to re-iterate the business plan, refactor some of the code, or even change up the colors on the logo AI-generated — it's the agents that are 100% in control of the iteration of all aspects of the startup. Stealth Startup is a simulation platform where AI agents act as key team member agents—CEO, CTO, and Marketer—who collaboratively build a startup. The CEO agent handles market research, generates business strategies, and designs the business plan — note that we connected it to the web to get the latest insights. The CTO agent directly modifies the codebase and pushes updates to a real GitHub repository — it's able to go through the entire codebase of the startup the agents are building, make actual changes to the code, and commit these changes to their personal repository. The Marketer agent designs and creates logos and branding materials for the startup — it's able to literally generate relevant logos based on the startup idea and iterate on the design based on consulting with the other agents. These AI agents communicate on Slack wherein they can have discussions between team members and provide updates to one another in various channels. The agents came together and ended up building a 911 dispatch assistant web application built fully in React by the CTO agent. The Video In the video, the platform you see at the start is what it has been iteratively building from scratch. When we run the agent, the CMO realizes the logo should be changed, designs a new one, and then sends it to Slack. Then, the CTO understands this change and updates the codebase to use the new logo, then pushes this code to its GitHub. This process is entirely self-automated. 🛠️How we built it We built Stealth Startup by developing specialized AI agents using Cohere's language models to generate realistic business strategies, technical code, and marketing content. Each agent was given distinct tasks: the CEO generates business plans, the CTO agent integrates with the GitHub repository to make real-time code changes, leveraging Groq for fast AI inference, and using headless browsers on Vercel v0 for versioning and deployments. The Marketer agent uses Flux 1 to create logos and branding materials. All these agents are orchestrated through a Dictator class that manages the workflow and ensures smooth communication between the agents, simulating the creation of a full startup. Additionally, we gathered 40+ applicants via Linkedin to apply to our company's intern posting that was generated by the CEO agent and had it select candidates to hire. Sponsor Appreciation We used Cohere to make our agents smart and reliable. Cohere Connectors for access to the internet and tool calling, structured outputs for consistent information retrieval with agents and Command R for better reasoning. We used Groq for fast Llama inference in our SWE agent that powers our CTO. ⛓️‍💥Challenges we ran into One of the main challenges we faced was coordinating interactions between the different agents to ensure they communicated effectively and followed a logical workflow. Another hurdle was implementing the CTO agent's ability to make real-time code changes and push them to a separate GitHub repository without errors, along with ensuring the Marketer agent generated high-quality, relevant logos. 🎉Accomplishments that we're proud of We're particularly proud of successfully integrating multiple agents into a cohesive, collaborative environment. Our CEO agent can generate a detailed business strategy, while the CTO agent actively modifies and pushes code to GitHub, and the Marketer agent designs branding materials—all of which function together as if they were real startup team members. This simulation not only automates key startup processes but also demonstrates how AI can work together in a structured environment with proper structuring of instructions. 🔍What we learned Through this project, we learned the complexities involved in automating the roles of startup team members and how to orchestrate various AI agents to simulate real-world tasks. We also deepened our understanding of API integration with tools like GitHub and Replicate, as well as managing workflows between different AI-driven agents. 📢What's next for stealth startup Our next step is to expand the capabilities of our agents, allowing for more complex decision-making and interactions, such as hiring new agents for additional roles like Product Manager or UX Designer. We also plan to refine the CEO's strategic decision-making and the CTO’s ability to manage larger codebases, while expanding the marketer's role to include full branding strategies and marketing plans.",
    "matched_prize": null
  },
  {
    "title": "Silent Voice",
    "link": "https://devpost.com/software/silent-voice-nmosfx",
    "text": "Built With",
    "matched_prize": null
  },
  {
    "title": "FrontendKevin",
    "link": "https://devpost.com/software/frontendkevin",
    "text": "Inspiration We wanted to help people who have awesome ideas but don't necessarily speak \"code.\" So, why not let them speak... literally? By using speech-to-text, we’re making website creation as easy as having a conversation—no typing, no fuss, just talk and watch your site come to life! 🎤✨ What it does Kevin utilizes ONLY your voice to generate picture-perfect websites. Speak your vision, and Kevin handles the rest—turning your words into fully functional, responsive websites with accurate layouts, stunning visuals, and clean code. People with visual impairment or physical disabilities can discover the magic of coding with our hands off approach to dev tooling. How we built it Using Web Speech API, we turned your words into the website of your imagination by integrating it with generative AI to instantly create and design responsive, user-friendly web pages. Our amazing agent Kevin is powered by the high performance architecture of Groq. Challenges we ran into We encountered several challenges during development, including fine-tuning the speech-to-text system to reliably detect trigger words, accurately parse spoken instructions, and recognize the final trigger word to complete the process. Additionally, we struggled with gathering and structuring the necessary data to send to our API for seamless AI-driven website generation, requiring extensive troubleshooting to ensure smooth, uninterrupted functionality. Accomplishments that we're proud of We’re incredibly proud of getting the app to function as intended, from accurately recognizing voice commands to generating fully functional websites using generative AI. Overcoming the technical challenges to deliver a smooth and reliable user experience is a significant achievement for our team. What we learned Never use a serverless backend with a server. What's next for FrontendKevin Next, we plan to expand the range of design options, add support for more complex website features as well as improve the accuracy of the website generation. We’re also looking to enhance user customization and explore integrations with other AI tools to further streamline the development process.",
    "matched_prize": null
  },
  {
    "title": "LockedIn - Vision Pro",
    "link": "https://devpost.com/software/lockedin-4rvey8",
    "text": "Inspiration Thanks to the generous support of vGIS Inc., we had the incredible opportunity to work with a physical Apple Vision Pro at Hack the North! We developed a VisionOS app that transforms reading into an interactive and engaging experience, making textbooks come to life! What it does LockedIn goes beyond a standard PDF viewer by leveraging AR to render pages as immersive 3D objects. It brings diagrams to life through interactive models, allowing you to engage deeply with the material. Our core feature “Lock in” gives a focused, distraction-free reading environment that enhances your experience—whether it’s a challenging research paper or your favorite book. If there’s an equation, you can graph it in your interactive AR space. Reading about chemistry? Turn your reactions to life with 3D models of molecules. Love physics? Space out about it in a virtual observatory. It’s time to lock in. The app also simplifies content with page summaries, making complex information easy to grasp. Plus, there's an interactive voice-activated chatbot that answers any contextual questions you have along the way, based on the page you’re reading. And for added convenience, it includes a Shopify store where you can purchase and add books directly within the app. How we built it To bring LockedIn to life on VisionOS, we utilized a combination of SwiftUI and UIKit in the Swift programming language for display and AR/VR interactions. 3D models were created using Reality Converter and rendered into immersive environments with RealityKit. To optimize the immersive space, we developed a custom LSTM machine learning classification model using Python, PyTorch, NumPy, and Pandas to determine a suitable environmental setting. For the database, AWS S3 handled book files, while Convex managed metadata. We also implemented a seamless file transfer pipeline using MASV. Our backend was powered by Flask, serving as the REST API, initially deployed on Defang before transitioning to ngrok for better accessibility. Cohere was used to build a context-based chatbot via VisionOS voice-activation. Challenges we ran into We ran into a few challenges understanding the differences between VisionOS view layouts, like with volumetric windows. We also had to learn how to use Reality Converter and 3D model design. Trying to make our 3D models interactive took a lot of time and teamwork to figure out, especially since the VisionOS is such a new platform! It also took a lot of practice to get used to the controls of the Apple Vision Pro… Accomplishments that we're proud of This was our first time developing on Vision Pro, and we successfully built the project using a lot of new tech! We integrated multiple purpose-built databases for file storage and metadata, implemented several AI/ML models, and managed to seamlessly bring everything together. Despite working with unfamiliar technologies, we created what we believe to be a truly game-changing application! What we learned We gained valuable experience in Swift and understanding of Vision Pro and the intricacies of AR/VR development, along with a ton of cool stuff about integrating different likes of technologies into a cohesive platform! What's next for LockedIn (Vision Pro Application)",
    "matched_prize": null
  },
  {
    "title": "EchoMoods",
    "link": "https://devpost.com/software/echomoods",
    "text": "Inspiration Millions of people with communication deficits face daily challenges in expressing themselves clearly. Many turn to professional help, such as speech therapists, to navigate these difficulties. However, there’s an undeniable power in being able to improve on your own terms, practicing and refining communication in a safe, personalized environment. EchoMoods was inspired by the desire to offer people the autonomy to master their communication skills independently, with instant, constructive feedback. We wanted to build a tool that could truly make a difference in enhancing the quality of life for those with communication challenges. What it does EchoMoods enables users to record a video of themselves mouthing a sentence along with a facial expression. The app then analyzes the video to provide feedback on three core aspects: By offering this multi-dimensional feedback, EchoMoods helps users understand where their communication might be misunderstood and offers insight into improving both verbal and non-verbal cues. How we built it We developed EchoMoods using a combination of cutting-edge technologies: This integration of multiple APIs allowed us to provide real-time, actionable feedback to users, making EchoMoods both powerful and versatile. Challenges we ran into One of the major challenges we faced was integrating the backend with Cohere AI. Initially, we underestimated the complexity of setting up a backend server to manage the API calls efficiently. Debugging the process of connecting all the APIs smoothly also took significant time and effort, but we persevered and overcame these roadblocks through collaboration and problem-solving. Accomplishments that we're proud of We’re incredibly proud of successfully combining multiple APIs into a cohesive, functional application. The ability to create a unique solution that brings real value to people’s lives, while utilizing innovative technology, is something we cherish. The user-friendly interface and seamless feedback process are significant achievements that we believe set EchoMoods apart from other tools. What we learned Throughout the development of EchoMoods, we gained a deeper understanding of the nuances involved in API integration. We learned that every API has its own set of requirements and constraints, which must be carefully navigated. We also gained valuable insights into optimizing backend services to handle multiple API calls efficiently, ensuring a smooth user experience. What's next for EchoMoods The future of EchoMoods lies in improving the accuracy and depth of its feedback. We aim to: By continuing to refine EchoMoods, we hope to further empower individuals with communication challenges and make the world a more inclusive place.",
    "matched_prize": null
  },
  {
    "title": "HaBits",
    "link": "https://devpost.com/software/habits-wnt9id",
    "text": "Inspiration We were inspired to create HaBits because we wanted to make health tracking both engaging and personalized. With so many fitness and health apps available, we noticed a gap in how they engage users with their data and motivate healthy habits in a way that fosters community. By integrating data from the iOS Health app, we wanted to give users an easy way to monitor their progress across key health categories—like sleep, meditation, and steps—in a visually intuitive and competitive way. Our goal is to make health tracking more social, personal, and fun, helping users improve their habits while connecting with their friends and competing on a leaderboard. We believe that building good habits becomes easier when you can track your progress and share the journey with others. What it does HaBits is a personalized health tracker that syncs with the iOS Health app to monitor users' sleep, meditation, and steps. It visualizes this data using progress circles, giving users an easy way to understand their daily goals and how well they are doing. Additionally, the app features a leaderboard where users can compare their progress with friends, fostering healthy competition and community support. We are also developing a feature where users can scan pictures of their food, and our machine learning model will estimate calorie intake based on the food’s appearance. This will allow users to track their calorie consumption relative to their BMI, ensuring they're meeting their nutritional needs. How we built it We built HaBits using React Native, TypeScript, and Expo to create a cross-platform mobile app that works seamlessly on both iOS and Android devices. We integrated Apple HealthKit to gather users' health data, including their sleep, meditation, and steps. For real-time updates and leaderboard functionality, we used Firebase, allowing users to stay connected with their friends and track their progress live. The food scanning feature, which is under development, uses a machine learning model to detect calorie intake based on photos of food. We designed the app with a focus on user experience, leveraging tools like Figma for prototyping, ensuring a sleek and intuitive interface. Challenges we ran into One of the main challenges we encountered was connecting to the iOS Health app and accurately retrieving health data. Ensuring that we could securely access users' sleep, meditation, and step data while respecting their privacy and permissions required careful navigation of the Apple HealthKit API. We also had to handle potential data syncing issues and ensure the app functions smoothly across different devices. Accomplishments that we're proud of We’re proud of successfully creating a social aspect with our leaderboard feature, which encourages users to stay motivated. We're also excited about our plans for the food scanning feature, which demonstrates how AI can play a role in helping users monitor their nutrition with minimal effort. Another accomplishment was our design; we received great feedback on how intuitive and visually appealing the app is. What we learned Building HaBits was a valuable learning experience, especially since it was our first time working with React Native and Expo. We quickly adapted to these new technologies and learned how to build a mobile app that could seamlessly function across both iOS and Android platforms. The experience also taught us how to work efficiently as a team, coordinating between frontend, backend, and machine learning tasks to bring all the components together smoothly. Collaborating on complex features like integrating HealthKit data and Firebase for real-time updates required strong communication and problem-solving. Overall, we learned the importance of flexibility and teamwork when tackling unfamiliar technologies and how to combine our skills to create a cohesive product. What's next for HaBits Next, we plan to expand the app’s functionality by integrating more health categories, such as hydration and stress levels, for a more holistic health tracking experience. We are also focused on improving the food scanning feature by enhancing the accuracy of calorie detection and expanding it to recognize portion sizes. Additionally, we plan to introduce features like personalized challenges and wellness tips based on the user’s data, making the app a more comprehensive health assistant. Finally, we’re looking to implement more social features, like group challenges and wellness goals, to further engage users in improving their health together.",
    "matched_prize": null
  },
  {
    "title": "CollabCart",
    "link": "https://devpost.com/software/collabcart",
    "text": "Inspiration As university students ourselves, we understand how overwhelming the increase in responsibility can be, especially when it comes to managing what we eat. Transitioning to independence can be challenging, and CollabCart is designed to alleviate this stress by leveraging artificial intelligence to streamline the grocery shopping process. What it does CollabCart is a grocery shopping platform that functions as both a grocery list and a tool for AI-assisted purchasing decisions. This is particularly beneficial for individuals with dietary restrictions, as the AI, powered by Groq, offers fast and reliable suggestions, providing quick information on-the-go. How we built it We used Tailwind CSS for the front end and MongoDB Atlas for the back end, designing and developing the application in VS Code. To power our AI, we used Groq's API to create our chatbot that acts as a personal informant when grocery shopping. We also integrated Auth0 to create user accounts, enhancing security through an authentication system. Challenges we ran into Given the personal nature of the problem we aimed to solve, we faced a continuous flow of feature ideas that risked overcomplicating the platform and hindering the completion of our prototype. After spending considerable time on ideation, we found ourselves racing against the clock to finalize our project. Accomplishments that we're proud of Even though we faced challenges in finalizing a solid idea, we are proud to have worked on a project that addresses issues personally relevant to our lives. Through this process, we’ve grown as developers, applying our skills to solve real problems we encounter daily. Our ability to create a practical solution that improves our grocery shopping experience reflects our commitment to using technology to enhance our own lives and those of others. What we learned As mentioned in our challenges, we discovered that the ideation process was particularly difficult, with developing a solid idea proving to be arguably harder than any of the technical tasks required. Our group was only able to successfully agree on our idea 15 hours after the hacking period began. What's next for CollabCart At CollabCart, we believe there are still problems to address in group shopping. For university students, shopping in groups can be complicated, hard to track, and an annoying hassle. By incorporating a collaborative function, our app allows users to manage their own shopping carts alongside a communal basket. This significantly simplifies the process and gives university students more time to focus on their studies.",
    "matched_prize": null
  },
  {
    "title": "Sign Engine",
    "link": "https://devpost.com/software/sign-engine",
    "text": "Inspiration For over eight years, I tried learning multiple languages, from Sanskrit and Spanish to Hindi and French, yet I could barely maintain a fluent conversation in any of them. When I moved to Vancouver in 2021, I joined Burnaby South Secondary School, which shares its campus with the British Columbia Secondary School for the Deaf (BCSD). This gave me the unique opportunity to study a new kind of language – a visual language – in high school. ASL wasn't like any of the other languages I had attempted to learn before: It wasn't just about words or pronunciation, but rather learning how to fully express yourself without the tools you typically use. Over the last three years, our ASL class has shown me how I take communication for granted and also helped me notice the many hurdles that are faced by the Deaf community in our hearing-centric society. From my very first week at Burnaby South, I have had many experiences that suddenly remind me of the reasons we learn about Deaf culture and accessibility in ASL class. The mission below is ultimately what I hope to achieve with this project. What it does Sign Engine has three main components: How we built it Challenges we ran into Accomplishments that we're proud of What we learned What's next for Sign Engine The most important things are:",
    "matched_prize": null
  },
  {
    "title": "NeuroGuard",
    "link": "https://devpost.com/software/neuroguard-1cbg2d",
    "text": "Project Inspiration: The inspiration for this project stems from recognizing the often-overlooked challenges faced by caregivers and families of Alzheimer's patients. While much focus is placed on the patients, the immense responsibility, emotional toll, and hard work of caregivers are frequently neglected. Our full-stack application, powered by AI/ML and video recognition, aims to ease this burden by providing fall detection and other supportive features, ensuring not only the safety of the patients but also the well-being of the caregivers. We were motivated by the belief that caregivers, too, deserve care and support with their duties. NeuroGuard was created to keep Alzheimer patients safe and support their caregivers. What it does: NeuroGuard is a full-stack application with a website/browser option that has multiple features like video recognition for fall detection, personalized medical agent, fall history and medical records, an alert message system, (and much more) to aid caregivers in their duties for their patients. Since Alzheimer's patients are at much higher risk of falling and serious injury, video recognition technology can quickly identify when a patient falls and alert the caregiver and emergency services if needed. As well, the personalized medical agent utilizes an AI API and is tailored to each individual user to allow swift usage of the application, access to information, and support to the caregiver. How we built it: Back-end: Implemented advanced computer vision techniques coupled with image analysis and processing to precisely detect sudden changes in human motion. Utilized OpenCV for real-time image processing and motion analysis. Integrated the Pose Detection algorithm, part of Google's powerful MEDIAPIPE machine learning pipeline, to track and interpret human movement with high accuracy. Conducted research into incorporating habit pattern recognition using the RoboFlow library and the Pixela public API, a promising addition that would have significantly enriched the system's functionality, though it was not fully realized within the project timeline. Aimed to apply object-oriented programming (OOP) principles to design a modular and efficient timer class, which would further optimize system performance and enhance flexibility. Personalized Medical Agent: We developed an AI chatbot using Voiceflow to support caregivers in Alzheimer’s care, integrating external patient data through Voiceflow APIs, including Zendesk Ticket APIs, to deliver personalized interactions. The system combines workflow automation, knowledge base features, and LLM integration for specialized training, ensuring accuracy in healthcare scenarios. It dynamically processes real-time data from patients, streamlining communication between caregivers and healthcare providers. The chatbot operates in two modes: an AI assistant powered by a custom GPT-4 model that provides context-specific responses to healthcare-related questions, and an incident reporting feature that guides users through submitting detailed reports. The system pre-fills the user's email, requests a minimum input for incidents, and sends reports via email to healthcare providers while offering the option to continue the session or conclude it. This solution enhances caregiver support by automating information flow and improving communication efficiency. Front-end: Developed dynamic and visually appealing web interfaces using React.js to showcase project functionality. Utilized the Axios library to seamlessly integrate the Flask backend with the React frontend through API communication, enabling efficient data flow. Overcame challenges such as implementing robust user authentication logic and establishing a live video stream from the backend to the frontend for real-time display. The result is an interactive and responsive frontend that enhances user experience while ensuring smooth backend integration. Challenges we ran into: The challenges we faced were numerous, but each obstacle helped us grow and refine our approach. Initially, we struggled with setting up the video recognition system, which required extensive troubleshooting. Without access to an external camera, we improvised by using an iPhone-to-PC connection to capture video. Training the model to accurately identify falls also proved difficult, so we incorporated MediaPipe to create nodes and leveraged the relationships between them to improve recognition accuracy. Additionally, integrating the Flask backend with the React frontend posed some challenges, but through persistence and problem-solving, we were able to overcome these hurdles and push the project forward. Accomplishments that we're proud of: Creating a fully function video recognition application without prior experience and perseverance through several challenges. Also, pushing ourselves to challenge ourselves with new technologies and combine multiple tech techniques and tools. What we learned: Life doesn't always go as expected, especially during a time-crunching hackathon. Having a solid plan, taking reasonable risks, and challenging ourselves with tech tools and techniques can be very rewarding! What's next for NeuroGuard: Extracting, plotting, visualizing, and predicting behaviour changes through graphs from a trained ML model can help provide helpful insight into medication and environmental changes to healthcare professionals. Therefore expanding our technology to the healthcare sector that is currently needing more help.",
    "matched_prize": null
  },
  {
    "title": "RemberU",
    "link": "https://devpost.com/software/orientu",
    "text": "💡 Inspiration When meeting new people, especially in huge numbers, it is not uncommon to forget people’s names immediately after–or even while–making small talk with them. As first year students who just went through Orientation, we can confidently say that we are no strangers to such situations. Social events can often be massive, intimidating, and overwhelming, so we wanted to help make such events feel a little cozier! ⚙️ What it does RemberU is an AI powered device that attaches itself directly to your Waterloo hardhat to make your people-meeting experience as smooth as possible. By reading the lips of those you’re speaking to, RemberU ensures that you’ll never forget or miss important information or have to sheepishly ask “what’s your name again?”. Our app stores essential aspects of each conversation and takes notes for future reference. Users can access these notes at any time to brush up on conversation starters, and each friend’s file will be updated using facial recognition after subsequent interactions. 💻 How we built it Hardware: RemberU runs on a Raspberry Pi connected to a hardhat, with a webcam strapped to the front to measure lip movements and record interactions. We guarantee you’ll have the coolest hardhat on campus! Backend: Our backend runs on Firebase, managing each account’s personal information and conversations. We used Symphonic Labs’ lip-reading API and OpenCV to record conversations, ensuring that our app works even in loud event environments. The Gemini API then processes the conversation, summarizing it into relevant facts and information to be displayed using Python and Flask in the app’s main feed. Frontend: Our frontend is built using Flutter, and features aesthetically pleasing log-in and register pages, as well as the main feed containing all of the user’s conversation data. Our very own mascot, a teddy bear named RememBear, greets users at our home page and represents the very message of coziness we want to convey. ⚠️ Challenges we ran into We struggled a lot determining what exactly we wanted RemberU to accomplish. We eventually settled with a scope wide enough to be useful, but not so wide that it would be impossible to complete. Learning Flutter was also a big hurdle for us, as it had a steep learning curve compared to using HTML and CSS, and most of us had little experience in it. Small challenges such as implementing APIs and fixing buggy libraries and hardware quickly turned into multi-hour nightmares. Putting it simply, Firebase + Flutter cooked us. Our Raspberry Pi also just decided to not work, so we had to improvise last minute. Being ¾ of us’ first times at Hack the North, we were of course faced with the dilemma of sleep or no sleep, but here we are! 🏆 Accomplishments that we're proud of Integrating the Gemini, Symphonic Labs, and OpenCV APIs together. Figuring out how to use Flutter and Firebase in such a short period of time. Building our own model for facial detection. Submitting this project. 📚 What we learned Technically, we had very little knowledge of front end development, specifically using Flutter and Firebase together. We also had little experience with the APIs we used such as Symphonic and OpenCV, so we’re very proud of how the final design and functionalities turned out. Thanks to HTN, we’ve had the opportunity to develop our technical and team skills, which we hope to take advantage of in our upcoming coops and careers. We will certainly look back fondly on this event, and we will cherish the bonds we’ve made, the people we’ve met, and the RBC food tent. 🚀 What's next for RemberU If we had more time, we would probably consider building a mobile app for RemberU so that it could be used even easier by people at events. This would allow us to use notifications to alert people of any potential updates to content or information. We could also incorporate the RemberU hardware into other kinds of headwear, as our current design targets UWaterloo Engineering Undergraduates. More professional designs could include making a RemberU tie, a name tag, or glasses.",
    "matched_prize": null
  },
  {
    "title": "Dashboard_reverse_engineering",
    "link": "https://devpost.com/software/dashboard_reverse_engineering",
    "text": "Built With",
    "matched_prize": null
  },
  {
    "title": "K.I.J.I.J.I. S.P.A.M.M.E.R.",
    "link": "https://devpost.com/software/price-whisper",
    "text": "Built With",
    "matched_prize": null
  },
  {
    "title": "IntroSpectacle",
    "link": "https://devpost.com/software/introspectacle",
    "text": "Inspiration 👓 The inspiration for this project stems from the challenges of meeting many new people at once and struggling to remember names, conversations, and hobbies. For students like us first-year university students, this is common during events like frosh week, while for adults, it may occur when transitioning to a new job. The project is also designed with older adults in mind, such as all our grandparents, who may experience memory difficulties. It aims to assist them in _recalling names, past interactions, and personal interests of those they meet, making social interactions easier and more meaningful. The inspiration behind the project is for people of every age, from young university students to wise old grandparents that people have! What it does 🤖 Our hack utilizes OpenCV and mediapipe to recognize faces of new friends and acquaintances. No more struggling to remember names or hobbies—our system displays each person’s name and key information right next to their face, making it easier to connect and engage with others. Additionally, we used the Cohere to summarize conversations, generate keywords, and extract names, helping you recall important details about each person in the future. How we built it 🏗️ We built a real-time facial detection system using OpenCV and MediaPipe for video input, paired with PyAudio for synchronized audio recording. MongoDB stores captured images and metadata. This project is designed for head tracking, face recognition, and efficient multimedia data storage and retrieval in real-time applications. Challenges we ran into 🎯 Initially, we've had way too many ideas, however, we ended up scraping off every single idea we could think of for the first night because they were either hard to accomplish, or we thought they weren't good enough. And it was frustrating not being able to think through and finally pin down on a single clear idea we could work towards. And especially since this is the first time at a hackathon for most of us, there are a lot of things we still didn't know, and had to chat with mentors and volunteers for advice, but as we kept trying, we started picking up the learning curve even more, despite many challenges we're still facing, the experience is all worth it. Accomplishments that we're proud of 🥇 For us, as most of us are first time hackers, we were proud of making tiny little progress through every hour of the hackathon, powering through every single stepback and challenge by working and collaborating together to solve that one little bug in our code. Especially since it's the first hackathon most of us have attended, we're proud of the fact that we were willing to put ourselves out of our comfort zone, to learn, grow, meet new people and immerse ourselves in the technology world, from tiny steps, to giant leaps. What we learned 💡 Thug it out. Typing this at 5:30 really shows us what it means to successfully finish a hackathon with a working project. From learning OpenCV's library to MongoDB's Pymongo all the way to just trying to integrate an audio and video file at the same time in Visual Studio; it was quite an experience for all of us, an unforgettable one we must add. What's next for IntroSpectacle 🔮 Our mission for IntroSpectacle is to develop it into something useful and applicable in real life. By converting this webcam to a smaller and sleeker device we want to improve the look as well as the features of the product. A large part of our inspiration started from the challenges of not being able to remember things about people, and with different and new features we will be able to take off!",
    "matched_prize": null
  },
  {
    "title": "CoTrace",
    "link": "https://devpost.com/software/cotrace-rf4atx",
    "text": "Inspiration Language preservation has become more prevalent recently, yet opportunities to engage with it hands-on are still limited— especially for endangered alphabets. Inspired by Chinese character tracing books, our team developed a way for language users to share their knowledge with aspiring learners in a way where every click connects past to present. What it does By uploading images of written words, community. contributors provide resources for learners to practice tracing over and the opportunity to refine their writing skills in endangered languages. Users are able to select their target language, navigate between user-submitted words and phrases, and create vocabulary lists. How we built it CoTrace is hosted on a React framework powered by a Convex backend. We used Typescript to construct the UI. Challenges we ran into Accomplishments that we're proud of What we learned What's next for CoTrace We also wanted to recognize Nunavik-IcE for providing us with the Inuktitut words used in our build!",
    "matched_prize": null
  },
  {
    "title": "ScanBites",
    "link": "https://devpost.com/software/scanbite",
    "text": "Inspiration As college students, we often struggle with balancing a busy schedule, coursework, and maintaining a healthy lifestyle. Cooking meals from fresh produce isn’t always an option due to time constraints, and constantly eating out can be too expensive. So, we end up turning to pre-packaged or processed foods. However, it's difficult to tell which processed foods are safe and healthy. With nutrition goals in mind, we wanted to create a solution that would help students make informed food choices effortlessly while saving time and reaching their dietary goals. What it does ScanBites is a mobile app that makes it easy to analyze the nutritional quality of prepackaged food. Users simply take a photo of the nutrition label, ingredients list, or the food itself, and ScanBites processes the data, identifying potentially harmful ingredients and assessing the overall healthiness of the product. The app uses AI and image recognition to break down calories, proteins, sugars, fats, and other nutrients, offering a health score based on user goals and dietary restrictions. How we built it Utilized Flutter Framework for the app development. Next.js for backend. The flow is as follows: Challenges we ran into Accomplishments that we're proud of What we learned What's next for ScanBite",
    "matched_prize": null
  },
  {
    "title": "Medisense",
    "link": "https://devpost.com/software/medisense-x3f5ul",
    "text": "Inspiration We discovered an API called OpenFDA, which provided data on various kinds of prescription drugs. We decided to build an app that would help users make safe choices with common over-the-counter medication without needing to add to the workload of their doctor or pharmacist. What it does Medisense scans the barcode on a medication’s packaging and uses the info it provides to answer the user’s questions about that medication. It can be easily augmented to work for other industries, such as the grocery industry. How we built it We used Flask for backend and FlowByte for frontend. Dynamsoft Barcode Reader scanned the bar codes, and FlowByte provided the speech to text input. We used exa-py to gather the most relevant search data on the medication and used a Llama 3 instance hosted on Groq to answer the user’s questions. Challenges we ran into Integrating flask with the JavaScript frontend was very challenging for this project, as was barcode scanning with our initial libraries. Also, the OpenFDA API ended up not providing data that was useful to us, so we had to pivot to exa-py fairly late in development. This pivot ended up greatly improving the product and enabling a new level of scalability and adaptability. Accomplishments that we're proud of What we learned What's next for Medisense This proof of concept could eventually become a mobile app, or with some reactive changes, be adapted to work as a mobile website. It can easily be adapted to other industries, and could see a home as part of an app for grocery stores to help customers learn more about their products.",
    "matched_prize": null
  },
  {
    "title": "procrasti-NATION",
    "link": "https://devpost.com/software/procrasti-nation",
    "text": "Inspiration At one point in our lives, we’ve all been procrastinators, so we figured, why not create a site that brings out the laziness in all of us? You no longer need to feel pressured into doing something you dread, procrasti-NATION understands you. 💻 What it does ⚙️ How we built it To build procasti-NATION, we used REACTJS and Tailwind CSS for the frontend, and we used MongoDB as well as Cohere's Chat and Classify APIs for the backend. Cohere's LLM was mainly used to prompt custom excuses for specific tasks of a user. However, we also trained it with data to classify the urgency of a user's task. MongoDB Atlas was used in order to store the history of a users prompts and answers with their urgencies and dates which makes it simple to sort and search for data. Let it also be known that a Costco family-sized bag of All-Dressed Ruffles was especially crucial to this building journey. 🤔 Challenges we ran into 🔭 Accomplishments that we're proud of We all took turns staring into space out of confusion while creating the excuse engine, so you can imagine our joy when it finally gave us an excuse other than ‘Sorry, I need to walk my pet goldfish’. We worked hard at prompt engineering for the Cohere Chat API Model to get it to work the way we wanted it to, and we were really satisfied with how it turned out. Needless to say, successfully implementing this model into our project was one of the best parts of this journey. What we learned On the technological side, we learned how to integrate backend and frontend code, as well as how to implement APIs. We learned how to use prompt engineering to do a DIY version of training an LLM. Otherwise, we learned that procrastinating a hackathon project maybe wasn't the best idea (is it too soon?). On a serious note, we learned how to better work with each other and the importance of planning out projects before executing them. We definitely look forward to applying what we learned here to future hackathons, projects and other endeavors. What's next for procrasti-NATION We believe there’s a pretty bright future ahead of procrasti-NATION. We look forward to introducing procrastinator-only forums, implementing more technologies to diversify the site, partnerships, monetization opportunities, and more! Our target market is widespread across demographics, and so we look forward to exploring what’s in store for us (is procrasti-UNIVERSE next? Stay tuned…).",
    "matched_prize": null
  },
  {
    "title": "TapIt",
    "link": "https://devpost.com/software/tapit",
    "text": "Inspiration Our inspiration for TapIt came from the potential of bus tickets, where a simple single-use ticket that would otherwise be thrown away (how wasteful!) can be configured to store information and interact with cell phones through Near Field Communication (NFC). We were intrigued by how this technology, often associated with expensive systems, could be repurposed and made accessible for everyday users. Additionally, while exploring networking features at Hack the North, we recognized the need for a more seamless and efficient way to exchange information. Traditional methods, like manually typing contact details or scanning QR codes, often feel cumbersome and time-consuming. We saw an opportunity to not only drastically simplify this process but also to reduce waste by giving disposable objects, like bus tickets, a new life as personalized digital cards. Our goal was to democratize this powerful technology, allowing anyone to easily share their information without the need for costly hardware or complex setups. What it does TapIt turns any NFC-enabled object, such as bus tickets, NFC product tags, or even your student card, into a personalized digital card. Users can create profiles that include their contact details, social media links, and more, which can then be written onto NFC tags. When someone taps an NFC-enabled object on their phone, the profile information is instantly shared. This makes networking, sharing information, and staying connected easier and more intuitive than ever. Just tap it! How we built it We used React Native and Expo to create a mobile app for Android and iOS. We used npm packages for NFC writing, and we used Flask to write a backend to create short profile URLs to write onto the NFC cards. Challenges we ran into We had issues with device compatibility and NFC p2p software restrictions. We also had trouble setting up an auth0 authentication system. It was very difficult to compile applications at first with React Native. Accomplishments that we're proud of We learned a lot about mobile application development and React Native in a very short period of time. Working with NFC technology was also really cool! What we learned NFC/HCE technologies and mobile development were our main focuses - and we're proud to have created a product while learning about these things on the fly. What's next for TapIt Features to support a wider range of NFC-enabled tags! We want to create an ecosystem that supports quick contact exchange with repurposed but readily accessible materials.",
    "matched_prize": null
  },
  {
    "title": "Timeless",
    "link": "https://devpost.com/software/timeless-qndjg6",
    "text": "Inspiration Every image tells a story; so does every song. In the rush of our fast-paced lives, it's easy to lose sight of the present. As students, we become so consumed by our ambitions and goals that we often forget to pause, take in our surroundings, and appreciate the moment. We tend to overlook the beauty of where we are, who we are, and how far we’ve come. When do we ever stop to reflect on life in the midst of our relentless pursuit of success? That’s why we created Timeless. It’s a space designed for those moments when you want to slow down, revisit forgotten memories, explore feelings of nostalgia, or even imagine the future. As students, we understand the importance of finding balance between ambition and reflection, and Timeless allows you to immerse yourself in those meaningful experiences. What it does Timeless is an AI-powered application designed to deliver immersive experiences by generating personalized photos, music, and videos based on user prompts. Leveraging cutting-edge AI models such as Cohere, Groq, DALL-E 3, Luma AI, and Suno AI, Timeless crafts unique audio-visual journeys that capture the essence of specific moments in time. Each experience is enriched with a photo summary, allowing users to fully engage with the atmosphere of the moment. By seamlessly blending creativity, nostalgia, and advanced technology, Timeless redefines how we experience history and memories in the digital age. How we built it For the front end, we began by designing each page in Figma, using React.js, Tailwind CSS, and Lucide libraries to enhance the visual aesthetics and overall appeal. We then integrated the front end with the back end by making RESTful API calls and utilizing the Axios package to facilitate smooth communication between both ends. Our backend consists of Python with the Flask framework, MongoDB Atlas for the database, and Postman for building and testing APIs. We carefully engineered a pipeline among the AI models including Cohere, Groq, DALL-E 3, Luma AI, and Suno AI APIs, ensuring each one generated the input for the next stage in the process. Thanks to Hack the North and their extensive technology stack, we were able to create our backend using these APIs and technologies as efficiently and effectively as possible. Challenges we ran into Initially, we had difficulties connecting the front end and back end. After consulting with a few mentors, we realized the best way to integrate them was through APIs and HTTP requests. Developing a simple, functional API was a significant challenge for our team at first. Timeless relies heavily on AI and requires the integration of numerous APIs. Therefore, prompt engineering and efficient API usage were crucial. Providing each AI model with sufficient information and designing a pipeline to utilize all of them was a time-consuming and difficult task. As a result, our team spent a lot of time fine-tuning prompts, experimenting with different prompt lengths, and testing various conditions to achieve the best results in the shortest possible time. Accomplishments that we're proud of Our user-friendly and simple interface, carefully designed to make users feel welcomed and relaxed. Our elegant design for pipelining various globally recognized AI models and APIs to create a more powerful and cohesive application. Our application works end-to-end, and we successfully implemented exactly what we envisioned. The final product closely aligns with our initial goals for this hackathon. Our ability to work quickly and stay united as a team, supporting each other and having fun while accomplishing a common task. What we learned We learned about new APIs that will simplify future projects, especially in the field of artificial intelligence. We developed the ability to work quickly as a team, staying united to complete a shared goal, no matter the challenges. We gained insights into prompt engineering, optimizing AI performance, and reducing time and resource consumption. What's next for Timeless Moving forward, we aim to spend more time refining and potentially training our models to make them more customizable and efficient for the project’s purposes. While Timeless introduces a significant innovation, it still needs further development to meet industry standards in terms of quality, design, and security (e.g., reducing long wait times for AI video generation). We plan to implement a more robust authentication system, such as Auth0, which we couldn’t use due to time constraints. Additionally, we aim to introduce new features, like defining events, cultures, and objects for inclusion in users’ final generated videos. We also plan to produce higher-quality videos by using more advanced APIs, potentially beyond Luma AI."
  },
  {
    "title": "Hermes",
    "link": "https://devpost.com/software/hermes-73azk0",
    "text": "❓Problem & Opportunity Do you have a relative whose primary language isn’t English? Access to communication, care, and travel is full of blockers that without personal assistance, can inhibit their daily lives. Recognizing a need in the healthcare, tourism, and foreign language journalism market, we wanted to create a tool that could break through that language barrier and create an easier experience for service providers. ✅How it helps Hermes creates a conversational translation experience that removes the broken and frustrating process that Google Translate presently offers. By keeping a conversational log of what's been said by both parties, we're able to simplify the recording flow into a constant ongoing process, rather than a one sentence at a time model. We’ve removed slow and clunky human intervention to the translation space. 🛠How we built it Thanks to the generous sponsors of Hack the North 2024, accessing the technologies needed to launch Hermes was a breeze. We used React, Google Cloud APIs, Convex Backend, Speech Recognition, Firebase, Cohere API. 💪Challenges With the short time constraint, as well as financial constraints, we had to pull back on features we wanted for our MVP. One challenge that arose was understanding which APIs we had available had the best use case for our project. The biggest and probably most relatable challenge, was matching up the variety of skill sets up to work in a consistent environment. Understanding each others proficiencies in JavaScript, TypeScript, React, and CSS was crucial in ensuring we would have an equitable division of labour to make sure we'd be able to deliver on time. 😤What we’re proud of First and foremost, we are proud to be given the chance to solve a problem that can impact many Canadians, and people worldwide. With the advent of AI and LLMs paving the way for process optimization and innovation, we are proud of our ability to learn and utilize resources like Convex, Cohere, OpenAI, and countless others in such a short period of time. 🔜What’s next We were building Hermes with the Canadian market in mind, but the app itself delivers on an experience that can have a global impact. These days borders are becoming more of a social construct than a literal one and access to communication across communities is becoming more and more vital. Even within countries with multiple languages across large populations, the opportunities are endless. Scaling this app into a robust and mature product would service a larger demographic and allow anyone to communicate with their community. With that, we have three main goals in mind to grow Hermes: 1) Add more languages. For our MVP we limited ourselves to only a couple of languages to keep the systems simple for our launch. We want no gaps in our service, and expanding to add more languages is crucial.\n2) Expand into more markets like education and athletics.\n3) Move away from English as the primary language. Only 15% of the world speaks English. That's 85% of potential customers left unserved and unable to communicate. Hermes will bridge that gap by allowing the app to be provided in multiple languages, as well as allowing translations from language to language,"
  },
  {
    "title": "RealTalk",
    "link": "https://devpost.com/software/realtalk-7pojgr",
    "text": "🌟 Inspiration Learning a new language can be overwhelming, especially when trying to recall past lessons in real-world situations. We wanted to create a solution that not only helps users communicate with their surroundings but also retrieves and builds upon their past learning experiences. The idea of combining voice interaction with advanced AI technologies like Retrieval-Augmented Generation (RAG) came from the need for a more personalized, immersive, and dynamic approach to language learning. 💬 What it does RealTalk is a voice assistant that enhances language learning by: 🛠️ How we built it We built RealTalk using: 🚧 Challenges we ran into One of the biggest challenges was integrating RAG with real-time voice input to retrieve relevant past experiences without causing delays. We also faced issues with ensuring that translations were contextually accurate, as direct translations often lacked nuance. Another challenge was tracking user progress and storing interactions in a way that allowed for seamless recall and updates. 🎉 Accomplishments that we're proud of We are proud of successfully integrating RAG into a voice assistant with the help of Cohere Rerank in a way that enhances personalized learning without the complexities of a vector database. The creation of dynamic flashcards from real-world interactions was a key milestone, as it provides users with a more engaging and effective way to retain new vocabulary. 📚 What we learned We learned a lot about the challenges of real-time voice interaction and language translation in an AI context. Understanding how to use RAG effectively to recall past learning experiences was a key takeaway, as it added depth to the user’s learning experience. We also gained insights into how to personalize learning tools to make them more effective for users at different stages of their language journey. 🚀 What's next for RealTalk Moving forward, we plan to:"
  },
  {
    "title": "Sixth Sense",
    "link": "https://devpost.com/software/smart-cane-38wkcv",
    "text": "Inspiration Around 85% of the world will go about their daily lives not knowing their privilege of being able to navigate several aspects of their lives with ease. Society, as it stands, is designed for the majority—those without disabilities. But what about the other 15%? What about the individuals who, despite already facing their challenges, have to live in a world that is not designed for them? 0.5% of these people are visually impaired. While 0.5% may seem like a small number, let’s put that into perspective; that’s around 45 million people, more than the population of all of Canada. We wanted to help make the world more accessible for people who may struggle to navigate their lives due to their visual impairment. Because everyone deserves to feel safe and secure when they go outside, regardless of their abilities. What it does Our smart cane constantly scans the surroundings of the user and sends a warning in the form of a vibration of the cane when obstacles enter the threshold and are getting too close to them. How we built it We began with an initial brainstorming process, where many of our ideas converged on creating an accessibility device. For the design and 3D printing phase, we used Autodesk Inventor to design custom 3D-printed parts to create a prototype cane. The process involved sketching and brainstorming concepts, creating CAD models, making revisions, and 3D printing the parts for assembly. In the electrical phase, we created a sweeping sensor using a servo motor and ultrasonic sensor connected to an ESP32, and also wired a DC motor with a motor driver to the ESP32 to create a vibration effect. For the programming, the servo motor continuously turned 180 degrees, the ultrasonic sensor checked for objects within a threshold, and the DC motor created a vibration when objects were detected within the threshold. The system also detects when the cane is close to the ground (therefore the user has fallen to the ground) and notifies the user accordingly. Challenges we ran into Accomplishments that we're proud of What we learned What's next for Sixth Sense"
  },
  {
    "title": "Goose Painter",
    "link": "https://devpost.com/software/goose-painter",
    "text": "Inspiration We were inspired by the concept of skribbl.io, a popular online multiplayer game where players have to draw their given words and other players take turns to guess them to get points. We wanted to know if AI fares better at this sort of image recognition than humans, thus we made Goose Painter! What it does Goose Painter works by getting an LLM to judge how well you can draw to describe a theme, with a twist. For a given theme, each player will be able to draw three images that describe the theme and its characteristics. Once completed, the app will generate a keyword or short phrase describing what it perceives in the images. With these keywords, the app then generates a short story building around them. Finally, each player's drawings are judged by comparing the generated story of each player against the original theme phrase using a defined rubric. The player with the highest similarity score is deemed the winner of the game! How we built it We built our application on the React framework, using JavaScript for our website and FastAPI for our endpoints. We leveraged GroqCloud and its API to access their vast array of LLMs to generate the keyword descriptions of each player's drawings, which is then sent back to our backend using the FastAPI endpoints. To generate the unique stories for each player's drawings, we send the drawings into LLAVA model via Groq which gives us the key words for the app, which then sends the keyword descriptions to Cohere's Command R LLM. Finally, we also use Command R to generate a story that communicates the drawings. And then those drawings are sent into Command R where we do a similarity scoring to evalute how close it fits to the initial. The rubric that we fed Command R follows for a criteria based on relevancy, tone, flow and creativity. Each player's drawings are ranked based on this score, and the player with the highest score is crowned the winner! Challenges we ran into We ran into a lot of issues as we developed Goose Painter, especially with the backend side. We initially had difficulty with integrating the Groq API within our code, as well as stitching the drawing keyword descriptions into a single input. However, we were able to overcome these challenges through perseverance and the help of our wonderful mentors! Accomplishments that we're proud of We are really proud to have set up our sophisticated backend correclty, as we have to ensure that each API call is being done in a sequential manner. We are also really proud of our UI and website design, which provides users with a fun way to get their drawings in! What we learned We learned a lot on how to leverage APIs and LLMs, especially across various clould partners and bringing them all other in sync under one application. What's next for Goose Painter We want to make it a more customizable experience where users can set certain themes, change the LLMs prompting, choose how many pictures they can draw, and overall ways to enhance the user's ability be more creative via our tool. Also, we want to implement this system into other similar games such as Dress to Impress such that there wouldn't be human biases in games that require judgement. Additionally, we want to explore new ways and tools to improve the accuracy and robustness of the similarity score generation."
  },
  {
    "title": "Hidden",
    "link": "https://devpost.com/software/first-impressions",
    "text": "Built With"
  },
  {
    "title": "What's your loo!",
    "link": "https://devpost.com/software/what-s-your-loo",
    "text": "Built With"
  },
  {
    "title": "YapBot",
    "link": "https://devpost.com/software/yapbot",
    "text": "Inspiration We got into a debate about our hackathon idea. One thing lead to another, and we built YapBot! What it does How we built it First we started out with experimenting with the Cohere API to have a working local script that we could use in a Flask backend that we needed to build for our hack. We then moved on to designing the REST API that our React frontend would ultimately communicate with. Once we had the API spec ironed out, our team split off into frontend and backend until we were done with YapBot. Challenges we ran into Accomplishments that we're proud of This was our first time working with the Cohere LLM API, so we were particularly proud of our prompt engineering that enabled the core functionality of YapBot. What we learned LLM hallucinations are very common without careful prompt engineering. We think that this is a particularly applicable skill now that LLMs are becoming more commonplace in all facets of society. What's next for YapBot"
  },
  {
    "title": "Cli-Change AI",
    "link": "https://devpost.com/software/cli-change-ai",
    "text": "Inspiration What It Does How We Built It Challenges We Ran Into Accomplishments What We Learned What's Next For Cli-Change AI"
  },
  {
    "title": "Blackbox",
    "link": "https://devpost.com/software/blackbox-x6gch5",
    "text": "Inspiration My experimentation with financial modelling made me curious to see if I could translate common formulas into stock. What it does Blackbox allows users to perform complex bond call price calculations with a limited number of inputs. Accomplishments that we're proud of Being able to translate equations into code and minimizing the required variables. What's next for Blackbox Looking forward to building a more in-depth user interface and including more models into our platform."
  },
  {
    "title": "TravelBite",
    "link": "https://devpost.com/software/travelbite",
    "text": "Inspiration Jeffrey Piccolo filed a lawsuit against Disney in February 2024, four months after his wife, Dr. Kanokporn Tangsuan, ate a meal a restaurant in Disney Springs that staff guaranteed was safe for her dairy and nut allergies. Disney lawyers said he cannot pursue a court hearing because he signed up for a free trial of Disney+ in 2019, where he agreed not to sue Disney in the small print. How scandalous. Infuriating. Utter legal tomfoolery. Our app cannot fully prevent tragedies like this, but the story made us realize the gravity of allergies. Especially with how diverse the North American culinary landscape is, we know friends who have difficulties eating out in places with dishes they can't pronounce. We want to make that experience a little bit easier for them. What it does How we built it Challenges we ran into Accomplishments that we're proud of What we learned What's next for TravelBite"
  },
  {
    "title": "PlanetShare",
    "link": "https://devpost.com/software/planetshare",
    "text": "Inspiration Charles had a wonderfully exciting (lol) story about how he got to Hack the North, and we saw an opportunity for development ^-^! We want to capitalize on the idea of communal ridesharing, making it accessible for everyone. We also wanted to build something themed around data analytics and sustainability, and tracking carbon footprint via rideshare app seemed like a great fit. What it does We want to provide a better matchmaking system for drivers/riders centered around sustainability and eco-friendliness. Our app will match drivers/riders based on categories such as vehicle type, distance, trip similarly, and location - but why not just use Uber, or other rideshare apps? Isn’t this the exact same thing? Yes, but here’s the catch - it’s FREE! Wow that’s so cool. How we built it Our two talented web UI/UX designers created multiple iterations of wireframes in Figma, before translating it into a beautiful frontend web app. Our very pog backend developers built all the database APIs, real-time chat system, and search/filter functionality. Challenges we ran into The most time-consuming part was the idea generation. We spent a lot of time on our first night (we stayed up past 4 a.m.) and the entirety of Saturday morning until we landed on an idea that we thought was impactful, and had room for us to put our own twist on it. We also faced our fair share of bugs and roadblocks during the creation process. Backend is hard, man, and integration even more so. Having to construct so many React components and ensure a functional backend in such a short timespan was quite the challenge. Accomplishments that we're proud of We were definitely most proud of our designs and wireframes made in Figma, which were then translated into a beautiful React frontend. Our team put a lot of emphasis into making a user-friendly design and translating it into an app that people would actually try out. Our team also put a lot of hard work into the backend messaging system, which required a lot of trial-and-error, debugging, and a final great success. What we learned In an era dominated by AI apps and projects, a pure software project is becoming increasingly rare. However, we learned that creating something that we are passionate about and would personally use means more than developing a product for the sole purpose of chasing industry hype. We learned a lot about matching algorithms for social platforms, technologies that enable real-time instant messaging, as well as the challenges of integration between frontend, backend, and databases. What's next for PlanetShare Building our web app into a mobile app for people to download and use on-the-fly. Also obtaining more metrics towards helping users improve their carbon footprint, such as integrating with public transit or other sustainable modes of transportation like cycling. Also, we can: Integrate rewards points by partnering with companies like RBC (Avion rewards) to incentivize customers to carpool more Extend our functionality beyond just carpooling - we can track other forms of transportation as well, and recommend alternative options when carpooling isn’t available Leverage machine learning and data analytics to graph trends and make predictions about carbon emission metrics, just like a personal finance tracker Introduce a healthy dose of competition - benchmark yourself against your friends!"
  },
  {
    "title": "Bullsift",
    "link": "https://devpost.com/software/bullsift",
    "text": "Inspiration 👴🏻 Our parents/grandparents have been exposed to harmful misinformation online, which has negatively impacted our families. This inspired us to create a tool that not only protects our loved ones but also empowers others to navigate the web more safely and critically. What it does 🚨 Bullsift acts as a \"firewall\" against fake news. It assesses the safety and credibility of websites, alerts users if a site is untrustworthy, and offers a game to explain why the content is flagged, providing users with factual information for a deeper understanding. With this tool, even tech-savvy grandkids will be asking grandparents if the news is real or fake! How we built it 🔧 We developed a Chrome extension that seamlessly integrates into your browsing experience, offering real-time content analysis for fake news detection. Powered by Cohere's Command R Model, our multi-layer detection system ensures users are equipped with tools to verify the credibility of online information. Each layer of detection is carefully engineered to deliver comprehensive insights, ensuring a smooth and intuitive user experience. The layers include: Challenges we ran into 🏁 Accomplishments that we're proud of 🌟 What we learned 📖 What's next for Bullsift 📈 Our next step is to implement a feature that allows parents to monitor their children's exposure to fake news, offering a safeguard against repeated exposure to harmful misinformation and encouraging healthier online habits."
  },
  {
    "title": "Sessions.AI - Your New Favorite Study Buddy",
    "link": "https://devpost.com/software/sessions-ai",
    "text": "Inspiration 🧑‍🎓 Imagine: your friend tests you on a quiz, right before it starts. All of a sudden, you're asked questions you never even thought of, studying alone. Oftentimes, having an outside perspective helps prevent acute tunnel vision when studying for exams. However, students might not always have their trusty pal to help them revise for tests. With sessions.ai, we sought out to give every student a study buddy- no matter the time or place. What it does 📖 Like a study buddy that studies the same thing you do, sessions.ai watches over your shoulder and keeps you accountable while you study- all the while quizzing you and filling the gaps in your knowledge. It does this through active recall, a studying technique which involves taking a topic the student wishes to learn, creating questions based on that topic, and then repeatedly testing the student on those questions. The student is thrust into 20-30min long sessions, followed by short and succinct questions made to simulate an exam scenario, testing their knowledge on what they have learned/retained on the topic they have studied in the session. How we built it 🔨 The building process of sessions.ai can be broadly broken down into three sections: 1. Frontend, UX Our goal in creating the frontend was a playful, straightforward experience making it easy for anyone to use the platform. The web application is built using NextJs App Router, along with TailwindCSS for a refined user interface and Zustand to manage global state. This combination allowed for quick development, iteration, and state management, letting us fine-tune our product every step of the way. 2. Creating the sessions.ai question engine Our primary objective in creating the question engine that would ultimately power sessions.ai was to be able to generate relevant questions to the study material. On opening the app a PDF of the syllabus is passed into the backend server, which is then parsed and sent to Cohere to serve as context for the subsequent queries. ↓ As for capturing text off the screen, the method we landed on was utilizing OpenCV's \"ImageGrab\" function to continuously stream the user's desktop by capturing a series of images. Next, with the help of OCR software (Tesseract OCR), all words are extracted from the student's study material of choice (PDF, slideshow, textbook, etc.). These will aid in the creation of curated questions for the student. ↓ After some intermediate parsing and cleaning up of the OCR text, the notes are sent over the wire to the backend server which processes and feeds them into Cohere, which in turn returns a list of relevant multiple-choice, short, and long answer questions/answers to study with. ↓ These questions are then displayed to the user! 3. Engines/Frontend connections Challenges we ran into 🏃‍♂️ OpenCV, the software we used for screen recording was often tricky to work with. Thankfully, ImageGrab from PIL came into the picture and made the screen recording aspect of sessions.ai far simpler. Our project plan seemed easy at first, but in practice implementing a native screen reader that closely interacts with a web application is tricky. We overcame this problem using a local hosted web app. We could then achieve a close to native feel to our desktop application with the flexibility and ease of iteration that comes with a web app. Prompt engineering is difficult! Getting Cohere to process our data in a satisfactory manner and return an object which in turn could be parsed by our backend was difficult, especially considering the fact that there were a few bugs in the Cohere API which made JSON validation difficult to implement service-side. Thankfully, we were able to mitigate this on our end. Phew! Ensuring API consistency between the frontend and backend required a lot of communication between all of us, especially considering the nested nature of the format (and the often cryptic ways these errors can show up, especially in weakly typed languages cough cough js) We had a few native components (most notably a border that indicates screen recording is active), and getting the app to look well enough on macOS (our target platform for now) required quite a bit of tinkering. We eventually had to go a bit low-level and interact with the native Objective-C Cocoa/AppKit framework to achieve the results we wanted. These were just a few of the challenges we faced; if we were to elaborate on all of them we would likely run out of space 😅 Suffice it to say there were definitely hardships, but these pale in comparison to the satisfaction of finishing our project and getting it to a well-functioning state. Accomplishments that we're proud of 🏆 Creating sessions.ai was quite difficult, especially considering how we were tying together so many different frameworks and services (many of which we only became comfortable with in the past 36 hours). They say necessity breeds innovation; considering the many umm, exotic ways in which we managed to tie things together, we can attest to this. Towards the end, we made sure to gather feedback from mentors, volunteers, and people external to the team. Their suggestions and help made sessions.ai **even better, and we are very, very grateful! Outside of those, being able to create a product that will make a change in the way students study is exciting. We thrive on being able to make change, and we're super excited to see what students will do with sessions.ai. What we learned Again, so so much; if we were to elaborate on everything we learned, we would probably run out of space! But nonetheless, we all learned quite a bit about the numerous languages, frameworks, and APIs we used: Cohere, Next.js, Tailwind, Python, Flask, etc, and how to bring them all together to create a coherent (get the pun) product. What's next for sessions.ai Validation, validation, validation! (In terms of the questions and answers). We were about to implement this; unfortunately, we ran out of time :( But this is of utmost importance, and is definitely something that will be added in the near future. We also plan on growing its capabilities even further with LaTeX parsing. The Tesseract OCR software would ever-so-slightly have trouble with mathematics-based problems, but we expect with a LaTex-parsing integration, these troubles will no longer exist. We would also implement user auth and create a native app in Tauri/Electron. Here's a fun reel we made about the experience: https://youtu.be/5O64DHUfKjE Thank you for reading!"
  },
  {
    "title": "Formulate",
    "link": "https://devpost.com/software/formulate-ncmu28",
    "text": "Inspiration In today's fast-paced digital world, the reliance on paper forms persists across various industries. From healthcare intake forms to legal documents and government tax forms, business entities and organizations still spend considerable resources on managing physical forms. This slows down workflows and creates challenges in storing, accessing, and analyzing data efficiently. Enter Formulate, a web application designed to bridge the gap between physical forms and digital efficiency. Formulate aims to revolutionize how businesses, government organizations, and nonprofit entities handle forms by digitizing them seamlessly and efficiently, allowing users to easily upload, customize, and manage forms. What it does Formulate is a powerful, no-code solution to the problem of paper-based data collection, utilizing technologies such as Convex and OpenAI to streamline the digitization process. At its core, Formulate allows users to upload paper-based forms in various formats (e.g., PDFs or images) and seamlessly convert them into structured, interactive web forms without any technical expertise required. This transformation happens instantly, creating both the frontend interface and backend infrastructure automatically. By digitizing these forms, users can easily store and manage data, making it accessible for future use, analysis, or integration into existing systems. Formulate eliminates the need for manual data entry and offers a complete, end-to-end solution for modernizing data collection workflows. How we built it 1. File Upload and Conversion The user journey begins with the ability to upload a file (PDF, image, etc.) that contains the desired form. This could be a paper form that has been scanned or a digital document that needs to be made interactive. The file is stored securely in the backend using Convex’s file storage system, ensuring privacy and data protection. 2. Automated Form Processing Once the file is uploaded, Formulate utilizes OpenAI's machine learning model to analyze the document. The application extracts essential form data, such as questions and answer types, and converts it into structured JSON format. This process enables the identification of question types, including free-response, multiple-choice, checkboxes, and more. For example, a form may contain a question like \"State your name,\" which would be categorized as a free-response text field, or \"Select your state,\" which would be converted into a multiple-choice field. 3. Dynamic Web Form Generation Once the form data is processed into JSON, it is passed back to the frontend, where it is dynamically converted into a responsive web form using React components. Each question type (e.g., text input, radio buttons, checkboxes) is rendered as an appropriate interactive element, allowing users to fill out the form digitally. 4. Form Submission and Backend Integration Once a user fills out and submits the digitized web form, the data is captured and stored in the backend using Convex’s serverless platform. Convex provides a highly scalable, dynamic backend that enables real-time updates and integrations, ensuring that the submitted form data is processed and stored efficiently. Challenges we ran into During the hackathon, one of the key challenges we faced was node compatibility between Convex and Shopify Polaris. This incompatibility led to issues in setting up the development environment, causing delays as we had to troubleshoot, downgrade, and configure dependencies to ensure both systems could function together smoothly. Another challenge we encountered during the hackathon was formatting the JSON files for dynamic form generation. We faced issues with ensuring consistency across different fields and data types. This required a lot of debugging to ensure that the JSON schema was correct and that it aligned with the expected input for both the frontend and backend. Accomplishments we're proud of We're really proud of Formulate overall! We had a smooth ideation process, with an amazing prototype designed by Andrew :). We're also really proud of the creation of the backend using Convex, and the front-end of our landing page. Formulate can also do some additional fun things, such as generating a form using a rough sketch (with neat handwriting). What we learned Throughout the hackathon, we all pushed ourselves beyond our comfort zones. Many of us took on roles aimed at expanding our skill sets, rather than sticking to what we were already familiar with. This included deepening our understanding of React, which we successfully navigated by working together and learning as we went. What's next for Formulate Looking ahead, one improvement we would have liked to implement is integrating Auth0 for secure user authentication. This would have enhanced the security of Formulate by ensuring that only authorized users could access the web forms and its associated data."
  },
  {
    "title": "my-react-app",
    "link": "https://devpost.com/software/my-react-app",
    "text": "Built With"
  },
  {
    "title": "Traura.ai",
    "link": "https://devpost.com/software/traura",
    "text": "Inspiration What is one of the biggest motivators at hackathons? The thrill of competition. What if you could bring that same excitement into your daily life, using your speech to challenge your friends? Our app gamifies everyday conversations and interactions, letting you and your friends compete to see who projects the most positive or negative aura. What it does Challenges we ran into Accomplishments that we're proud of What's next for Traura"
  },
  {
    "title": "XRSZE",
    "link": "https://devpost.com/software/xrsze",
    "text": "Inspiration 🌟 Exercising is one of the many elements to a balanced and healthy lifestyle, especially in our profession as some of us tend to slack in it. We want XRSZE to make you get off your butt and do things! What it does 📝 XRSZE counts your reps for various exercises while keeping you accountable for your fitness goals. We also have an AI Chatbot (made using Groq) that can give you a specialized meal plan, workout routine and even motivation for your specific height, weight, age and dietary restrictions! How we built it 🛠️ We built XRSZE using various web technologies and APIs. The front-end interface was designed with HTML/CSS/JS for simplistic beauty. For the backend, we used Node.JS and integrated it with MediaPipe for the Computer Vision and movement tracking while using the Groq API for the Chabot. We fine-tuned our Groq AI Chatbot to specifically work greatly with fitness and food consumption and to give advice to improve your health. We also specifically calibrated the MediaPipe recognition model to detect accurate body movements to count and recognize workouts and count/time the activities accurately. Challenges we ran into 😓 One of the biggest challenges was ensuring the accuracy of movement tracking with MediaPipe. Fine-tuning the model to reliably count and recognize various exercises required extensive testing and adjustments. We also faced difficulties integrating the Groq API seamlessly with our backend, which led to several iterations before achieving smooth interaction between the chatbot and user inputs. Additionally, optimizing the front-end performance to handle real-time data without lag was crucial for a smooth user experience. What we learned 📚 We learned a lot about integrating complex technologies, like MediaPipe and Groq, to work together in a unified system. The process taught us the importance of iterative testing and refinement, especially when fine-tuning AI models for specific applications. We also gained valuable insights into optimizing real-time data processing and enhancing user experience through thoughtful design and responsive feedback mechanisms. It was also some of our first times using Node.JS and it was a great learning experience to learn and improve in real time. What's next for XRSZE 🚀 We’re excited to expand XRSZE into a mobile app, and we are already working on the Flutter code to make our platform more accessible and convenient for users on the go. We also plan to enhance our workout tracking capabilities by adding support for a wider variety of exercises and workouts, ensuring that users have a comprehensive tool to meet their fitness goals."
  },
  {
    "title": "PlayBook",
    "link": "https://devpost.com/software/playbook-3zyb9h",
    "text": "Built With"
  },
  {
    "title": "goosechase",
    "link": "https://devpost.com/software/goosechase",
    "text": "So. What were all those weird posters about? What in the honk is goosechase? And where are the hot single geese in my area that I was promised? Great question: what's goosechase? Goosechase is a massive game of tag, of sorts. We littered hundreds of cryptic QR code posters all around Hack the North. Over time the crowd SNOWBALLS as unsuspecting bystanders wonder what all the ruckus is about and join in. Along the way, everyone makes new friends, has a load of fun (the video is proof of that!), and scores points to rank them on the goosechase leaderboard. And the whole time, everyone is still wondering \"who made this weird goosechase thing?\" How we built it Stack-wise, we converged on tech we were all familiar with that let us build really quickly... Next.js, Prisma, Postgres, Twilio. We did some really cool technical things, though. For example, we reverse-engineered Hack the North's internal API and abused the \"connection\" feature to auth and get info (name, social media profiles, etc.) on users just by scanning their badges. (As a side effect of this, Lexi now has probably the most \"connections\" of anyone at Hack the North.) We then went a step further by scraping profile pictures from said social media profiles to show in the leaderboard. Challenges we ran into We (semi-accidentally) spent about $140 on the project... ~$40 to print our posters at the Waterloo library and for rolls of tape (*cough cough* \"marketing budget\") and another ~$100 on Twilio fees for the THouSANDS of game-related text messages we sent out: (Yes, we meticulously traced the stonks guy at 4am for the bit.) Then we learned the hard way that SMS is not the ideal platform for running games when we got blacklisted by several major cell carriers for being too... suspicious, or something. (Ouch :/) After this experience, we iterated on the game to offload as much game-related communication as possible into our web client. Also, game design is hard! It took a lot of talking and iteration to ensure that everyone had the right incentives and the game was actually fun. (We ended up making some counter-intuitive game design decisions, for example, being tagged earns you points.) In-person games are hard to design and hard to bootstrap from few players. Making a good scoring algorithm and finding reliable ways to extract profile pictures and user profiles were also fun challenges. (Plus, dealing with the quirks of a very broken QR code scanning library.) Accomplishments that we're proud of We ran a game on Saturday night. It started out quiet and we were worried nobody was going to participate, until the most magical moment happened: people we had never met before started streaming into the room from the staircase, answering the call of their phone. We ended up running 12 rounds and it was an absolute blast. We spent an hour running around with others as a crowd built up and we chased elusive players such as the infamous Gregory Gu. By the end of Saturday, over 300 hackers had already signed up. At the end of the day, it's all about the friends you made along the way. It was a great weekend :) Addendum: messages you sent us Bet you didn't know we could read these through Twilio's dashboard, huh?"
  },
  {
    "title": "Umless",
    "link": "https://devpost.com/software/memory-bricks",
    "text": "Inspiration behind Umless💡 Growing up, many of our parents used the classic swear jar to curb bad language. Each time someone used a \"banned\" word, they had to drop a nickel into the jar—an effective motivator since nobody likes losing their money! Many of us have also struggled with public speaking. When we're speaking under pressure we often forget what to say and subconsiously \"fill in\" the space using filler words. However, the \"ums\", \"uhs\" and \"likes\" can be distracting and undermine the impact of our message, but it’s tough to notice them in the moment. By combining these concepts, we created Umless—a digital swear jar that helps you track and reduce filler words while practicing speeches. Every time you use a filler word our software will catch it and donate a nickel to a charity of your choice. Not only will this help recreate the pressure of speaking publicly, but it will bring more awareness of a user's speaking habits and financially motivate them to change. It’s a modern twist on an old idea, designed to improve your speaking skills and hold you accountable in a fun and impactful way. What it does 🫙 Umless is a powerful web app that reimagines the concept of traditional swear jars. Our platform enables users to record both video and audio of themself, and transcribes text in real time to catch filler words. It then keeps count of the number of no-no words, and donates the appropriate amount of money to charity after each recording. Not only that, but the software records and saves an accurate transcription with filler words highlighted so users can be more aware of instances they use these words. Finally, to further improve public speaking abilities, Umless calculates your speaking pace in real time, and compares the values to professional standards to give you a recommendation on how you can adjust your pace. This all-in-one public speaking tool will enable users to speak more fluently and confidently. How we built it 🏗️ Umless is a React.js web app built on Django and the Deepgram AI Voice Transcription API. We used the interim results technique to provide live transcription and analysis 3x faster than the native API provides itself. We used MediaDevice's web API to record video and audio, as well as Auth0 for user sign-in Challenges we ran into ⁉️ The greatest challenge we faced was live transcription. Most APIs automatically filter out filler words, which is counterproductive to our product. Not only did it take large amounts of experimentation to find an API that fits our needs, but we found that the default real-time transcription was either too slow or too inaccurate. To help resolve this, we had to tap into interim results, which will provide preliminary results and adjusts it as necessary before providing a final transcription. Accomplishments that we're proud of 🏆 1) Creating a functional web application that is able to accurate transcribe and analyze results in hacking time :D 2) UI/UX Design 📐, the home screen, results page, and our animations make for a much more user friendly experience. What we learned 📚 Half of our team consists of novice hackers, so we gained valuable insights into front-end design, particularly with React. We also learned how to implement APIs, such as Deepgram and Auth0, and how to optimize them to produce more accurate results. The entire team dedicated substantial effort to brainstorming innovative tools for users, gaining valuable experience in user-centric design. We take pride in our rapid progress, successfully developing a functional and meaningful product despite our limited prior exposure to hackathons. What's next for Umless 🚀 1) More public speaking tools! Particularly, we hope to integrate computer vision with openCV to analyze a user's eye contact, body language, and facial expressions. We also wish to implement more audio analysis tools that helps to identify awkward pauses, monotone speech, or awkward phrasing. 2) Collaborative features, that continues to gamify the experience by allowing users to compete against their friends for the best speeches."
  },
  {
    "title": "Dropby",
    "link": "https://devpost.com/software/dropby",
    "text": "Try it now at https://dropby-htn24.netlify.app Inspiration: The idea for DropBy was born out of a simple, everyday frustration. One day, I was craving Tim Hortons coffee, but I didn't want to waste time if the line at the SLC location was too long. I thought, \"There must be someone there right now who could tell me about the line, if only I had a way to ask them.\" This made me realize that there's a wealth of real-time, hyper-local information all around us, locked away in the minds of people nearby. DropBy aims to unlock this potential, connecting people who need immediate, local information with those who have it, creating a network of real-time, community-driven intelligence. What it does DropBy connects users with nearby helpers who can provide instant assistance or information. Whether you need a quick errand run, real-time local information, or help with a task, you can reply on DropBy's users' immediate help. How we built it We built our frontend with React + Typescript. For backend, Convex's realtime native API proved to be invaluable, allowing us to create a responsive and scalable system, which is especially useful when handling instant updates of real time information. Additionally, we used Cohere and OpenAI's APIs for AI features such giving a task a short and concise title and content moderation. Challenges we ran into Ensuring user safety and trust was our biggest challenge. We want to make sure the tasks on DropBy is safe and appropriate. To solve this challenge, we leveraged Cohere and OpenAI's moderation APIs to implement multiple safeguard against inappropriate user input and content generation. We also implemented a downvote feature so the community can remove any inappropriate content by downvoting. Accomplishments that we're proud of We've built a responsive platform using Convex's realtime capabilities and created a user-friendly interface. Our AI-powered system automatically generates concise task titles, categories, and time estimates from user descriptions, streamlining the experience and ensuring clarity across all tasks. We also successfully developed a user-friendly interface that simplifies complex interactions, making it easy for users to request and offer help. What we learned Diving into real-time data was totally new territory for us, and thanks to Convex, it was a breeze. We also got a real sense of how important it is to keep things clean and appropriate when you're inviting the public to join in. Plus, we tried out using more structured responses from AI language models for the first time, and it's a game-changer—they're way more reliable now. What's next for DropBy Moving forward, we plan to roll out user authentication for added security. We're also designing a points system to motivate and reward users for their contributions. An exciting feature in development is an AR companion that displays quest locations in real life and on a mini-map, similar to open-world RPG games. While we couldn't complete this due to time constraints and limited Unity experience, it remains a key goal for our next iteration."
  },
  {
    "title": "FoodSense",
    "link": "https://devpost.com/software/foodsense-4etkb1",
    "text": "Inspiration In today's busy world, we recognized a crucial need: leveraging technology to empower people in managing their nutrition effectively and efficiently, especially for homemade meals. Our health is our most valuable asset, yet it's often challenging to keep track of what we eat, particularly when we're cooking at home, eating at a restaurant, and/or when don't have access to nutritional labels. We set out to solve two common problems faced by many, especially fitness enthusiasts and home cooks: 1-Accurately tracking the nutritional content of homemade meals quickly and conveniently 2-Understanding proper portion sizes without the hassle of weighing every ingredient Calculating macros and estimating portions for every meal can be time-consuming and impractical. That's where FoodSense comes in—an innovative web app designed for speed and convenience. It scans your food, provides an instant nutritional breakdown, and helps you understand appropriate portion sizes, all without interrupting your routine. What sets FoodSense apart is its convenience. Being a web app, you can simply open your laptop or desktop computer and scan what you're eating without needing to get up or reach for your phone. This is perfect for those working from home or having meals at their desk. What it does FoodSense uses an OpenAI Vision Model and your phone or webcam to scan the food you're preparing, providing real-time feedback on the macronutrients (protein, carbs, fats) and caloric content. It warns you if you're exceeding your macros and helps you stay on track with your fitness goals. After each scan, you receive personalized feedback. The app has the potential to be a complete fitness planner and tracker powered by AI. How we built it We built FoodSense using a Next.js frontend connected to a custom backend through WebSocket technology, prioritizing user experience and performance. The frontend captures a video stream from the user’s device and sends it to the backend. From there, the backend intelligently selects and processes specific frames before passing them to an OpenAI Vision Model for analysis. Once the model processes the frames, it returns a JSON response with detailed information on macronutrients and calories, which is then displayed in real-time on the frontend. To further enhance the user experience, we added a feature where, upon completing a scan, users can receive personalized guidance from their AI coach, providing detailed ingredient information and suggestions based on what they’ve consumed. Challenges we ran into Integrating a performant WebSocket video stream connection was challenging at first. We encountered difficulties with parallel processing and thread management of the video streams, but through persistent problem-solving, we were able to resolve these issues. A major challenge was ensuring the Vision model could accurately recognize various foods and ingredients under different lighting conditions and angles. Additionally, designing a native app-like interface for the web platform across different browsers, while ensuring smooth access to the camera on multiple devices, proved difficult. However, we overcame these obstacles, and the end result is a cross-platform solution that combines the benefits of accessibility with a user-friendly design. Accomplishments that we're proud of We're incredibly proud of getting the app to function smoothly within the tight constraints of the hackathon. The real-time macro detection works with great performance, and we’re excited about how quickly we were able to integrate various technologies to create a seamless user experience. We're also proud of our teamwork—despite the fact that all team members met on Slack without prior personal connections, we quickly adapted to each other’s strengths and work styles, allowing us to collaborate effectively and bring the project to life. What we learned We learned a great deal about building fast, user-friendly apps while improving our understanding of networking, WebSocket technology, and parallel processing. Integrating AI through vision models gave us valuable experience in applying advanced technology to real-world scenarios. We also honed our teamwork and communication skills, quickly adapting to each other's strengths and collaborating efficiently, despite having just met online. Additionally, we focused on balancing functionality with user experience, ensuring the app is both efficient and intuitive. Hosting and deploying real-time applications further enhanced our ability to manage performance across platforms. What's next for FoodSense We believe that FoodSense has the potential to be a game changer. The platform can be greatly expanded to include tracking features and provide valuable insights into how society consumes food. The vast number of images captured by the app can also contribute to improving machine vision models through continuous training. We genuinely believe that with a few more weeks of work, this product is shipable and could have a significant impact on the world, especially in making calorie counting more accurate and accessible. Given its access to real-time video, FoodSense could even evolve to assist with creating recipes and guiding users through the cooking process with its machine vision capabilities. While we successfully delivered the core functionality during the hackathon, future improvements could include offering personalized meal plans and generating weekly nutrition summaries to help users stay on track with their fitness goals."
  },
  {
    "title": "matches.io",
    "link": "https://devpost.com/software/matches-io",
    "text": "💡 Inspiration 💡 We all know the struggle of searching for jobs that truly align with our skills. We wanted to create something that could make this easier, ESPECIALLY for non-software engineering roles. Matches.io was created out of the desire to not just find job listings, but to ensure we find relevant ones that are aligned to your resume. 🤳 What it does 🤳 Matches uses semantic search powered by the Cohere API to compare your resume with job descriptions. This allows the platform to determine how well-suited you are for various positions, eliminating the guesswork in applying for jobs. You upload your resume, and we handle the rest! The system ranks available job listings based on a similarity score, offering you the best matches to focus on. It also allows users to add reviews for companies, search by role and skill set, and even sort listings by the most popular or highest-paying options. 🛠 How we built it 🛠 Our frontend was built using React.js. We used a MySQL database to store job listings and embeddings via MySQL Workbench. The backend is developed in Python using Flask. It routes user data, handles API requests, and manages the communication between the frontend and database. We used the Cohere API for semantic analysis. ⚠️ Challenges we ran into ⚠️ One of the biggest challenges was connecting the backend with the frontend - CORS the goat. We also faced a few hiccups with GitHub since everyone was working on the codebase, we ran into a lot of merge conflicts. Debugging API integration, particularly with Cohere, was also tricky, especially when dealing with structured outputs for semantic analysis. 🏅 Accomplishments that we're proud of 🏅 We successfully implemented semantic search using Cohere’s cosine similarity algorithm, which calculates how well a job description matches a resume. Most of our team had a backend focus, so we were proud of how quickly we adapted to frontend development using React.js. We developed a seamless user experience and backend logic. 🧠 What we learned 🧠 We learned a lot about semantic search and how to leverage AI to compare job listings with resumes. We also became more proficient in connecting a frontend with a backend and using SQL databases for managing complex queries and API calls. Plus, we learned about creating scalable solutions for job seekers that can easily be extended to new features like salary comparisons and user reviews. 🚀 What's next for Matches.io 🚀 Next steps for matches.io includes improving the user experience on the frontend, allowing for even more roles to be posted. Adding a upload salaries option so users can get an idea of what the salaries are for companies that don't post them, as well as add an AI to improve user's resume depending on the jobs they want."
  },
  {
    "title": "Chameleon",
    "link": "https://devpost.com/software/chameleon-tskl4r",
    "text": "Inspiration At first, we focused on the e-commerce industry. There are many online vendors who could benefit from an aesthetic and themed website that matches their products and brands. We thought that we could build a tool for Shopify that would allow their customers to input some images of their products and have AI-generated frontend code that creates a pretty online shop for them that presents their products in the best way. We then realized that this could be made more customizable for users if we allowed inputting images of design ideas rather than products and generating a frontend page that matches the user’s design aesthetics. Our final goal: Make designing aesthetic websites/online shops more accessible to everyone and not only those who possess technical skills. What it does Chameleon simplifies the process of designing aesthetic websites by eliminating the need for technical skills. Users can submit images of their design ideas, and Chameleon generates beautiful, customized frontend pages that match their design aesthetics. How we built it The application has four high-level steps: image upload, JSON generation, JSON to React component translation, and customization. In the image upload step, the user provides a sketch or screenshot of the design that they desire. During JSON generation, the image is translated to a JSON structure with a well-defined schema that describes the layout of the view. The backend returns this JSON to the frontend, where the JSON is converted into a tree of React components and rendered for the user. Finally, in the customization step, the user can iteratively adjust the design using natural language. FastAPI was used for the backend, with the OpenAI SDK and GPT 4o used for image and text ingestion, as well as Structured Outputs. React was used for the UI. Challenges we ran into ChatGPT struggled to interpret the true size and format of images in design as well as reading in whitespace which impacted its ability to render visual elements accurately. Additionally, ChatGPT faced difficulties in generating JSON that could be effectively converted into React components even though it has no problem generating React code… Accomplishments that we're proud of We are proud of ourselves for developing a fully integrated product that brings our MVP features—and more—to life: What we learned We learned the importance of task delegation. Dividing tasks among team members allowed us to tackle different aspects of the project more efficiently and leverage each team member's strengths. Additionally, we now know that integrating components early in the development process is extremely recommended. We discovered that unforeseen issues often arise, and resolving these can be more time-consuming than anticipated. What's next for Chameleon"
  },
  {
    "title": "Braillingo",
    "link": "https://devpost.com/software/voicebridge",
    "text": "Built With"
  },
  {
    "title": "CheatBot",
    "link": "https://devpost.com/software/cheatbot",
    "text": "Inspiration I love to play board games, but I often can't get a big enough group together to play. This led to my original idea: an AI-powered opponent in Tabletop Simulator who could play virtual board games against you after reading their rules books. This proved to be too ambitious, so I settled on a simplified case for my project: a bot for a modified version of the card game Cheat, built in Python. Features The project features a graphical implementation of Cheat in Pygame and a bot integrated with the OpenAI API that plays against the user. Challenges Towards the end of the hackathon, I had some struggles integrating the OpenAI API and the features it brought into the Pygame piece of my project, but I ultimately found a solution and got the bot working. Future Plans I feel this project is still incomplete, that the bot could be improved to have a longer memory, or that maybe it would be better if I tried a reinforcement learning approach instead. Nevertheless, I enjoyed learning to work with APIs and hope to continue learning with this project, however that may be!"
  },
  {
    "title": "connect.py",
    "link": "https://devpost.com/software/connect-y6khjl",
    "text": "Table of Contents Demo 4 minute video of the tool in action - https://youtu.be/lrKfHAbEC_E Authors Summary Our project is \"Secure Remote Access into a Jetson Nano in an air gapped environment without exposing ports.\" In this hack, we offer features ranging from allowing you to recover from a loss of Internet connection whilst remaining a remote access connection to your device without that device having any open ports (at the level of a 192.168 address). How It Works Our hack involves a Python tool that utilizes Atsign's NoPorts as a core dependency for secure remote access into devices without having to expose any ports. Atsign's NoPorts is a great solution because it is fully open source and allows for things like pulling their docker image so that we can locally run our very own NoPorts relay point, fully enabling us to utilize thier tech even in an air gapped setting. Atsign is also great because all encryption keys are generated at the edge and all traffic is encrypted with them. So no central server can see any of the very top secret things that a person may be doing while remote accessing another machine. Features tool Description This is the tool downloaded on the client side of things where the tunnel is initiated. The tool is a Python script that uses Atsign's NoPorts' NPT binary to establish a tunnel between the client and the server through a relay point. In our use case, the relay point is either 1. @rv_am which is hosted on the cloud by Atsign, if Internet is available, or 2. @alice which is hosted on a Raspberry Pi and available on the local network, if Internet is not available. ~/.local/bin/npt is where most of the heavy encryption and tunnel establishing technology happens and that can be found here. -h command This is the help command that shows you all of the options you have when using the tool. --cli command The CLI (command-line interface) command allows for a rich user experience when first establishing a tunnel. There are so many settings that it may be daunting at first, but the CLI allows for the user to easily input the settings that they want. --override-with-no-internet command By default, the tool will actually check for you if the Internet is available. If it is, then it is assuming that you prefer to connect via Internet. But in some scenarios, if you are both connected to the Internet and connected locally, it would be faster to connect locally. The --override-with-no-internet command allows for the user to force the tool to use the local method of establishing a tunnel. --override-with-internet command The --override-with-internet command allows for the user to force the tool to use the Internet method of establishing a tunnel. This is useful when the user is on the same network as the Jetson Nano and the Raspberry Pi, but they want to use the Internet method of establishing a tunnel. -p command Specify the remote port that you want to connect to on the Jetson Nano. The default is 22 because you probably want to establish a tunnel using SSH. -l command Specify the local port that you want to connect to on the Raspberry Pi. The default is 12332 because it is a random port that we chose. This is typically an ephemeral port that is used to establish the tunnel. --rh command Specify the remote host that you want to connect to on the Jetson Nano. The default is localhost because you probably want to connect to the Jetson Nano's services. But you can also use this as a jump box. -v command Enable verbose output for the npt command. This is useful for debugging purposes. jetson_nano The Jetson Nano in our hack is the device that we will be remote accessing to (either with SSH on port 22 or RDP on port 3389) startsshnpd.sh The startsshnpd.sh script is a simple script that will start the daemon process and run it again in case it crashes. The daemon processs will be listening . The daemon process binary can be found on Atsign's NoPorts releases (the daemon process was not written by us). The daemon encryption keys would be located at ~/.atsign/keys/ Closing Ports sudo nano /etc/ssh/sshd_config sudo service ssh restart sudo nano /etc/xrdp/xrdp.ini sudo systemctl daemon-reload sudo systemctl restart xrdp Sample Logs Example of the daemon process starting up, listening for requests, then setting up a tunnel session. Overall crontab Hopefully this is what the overall crontab on the Jetson Nano looks like, so that you can get a better idea of what these scripts are doing exactly. pi The pi set up consists of two parts: The keys for the srvd to use would be located at ~/.atsign/keys/ 1. venv This is a virtual environment that we use to run several things like: 3 atServers (alice, colin, and barbara) and an atDirectory (also known as the root server) that acts as a DNS server for these tinier atServers. This docker hub image was fully provided by Atsign and can be found here. We simply pulled the image and ran it. We also had to run pkamLoad to load each atServer with the appropriate PKAM public keys so that they could be authenticated to. But in our case, we are using docker-compose.yaml to spin up the image and manage the image/container. The startvenv.sh script is used to start the venv @reboot via crontab. 2. srvd This is a simple socket connector server that Atsign provides as part of their releases. Essentially, when a NPT request is made by our client, the SRVD process will bind to two ports and simply relay the encrypted traffic between the two ports. Since it is encrypted with keys that are on the device, it is unable to decrypt the traffic. You can kind of think about it as passing on \"garbage\" that it can't do anything useful with. This is important because it means that although the raspberry pi is an attackable device, there would be nothing useful on it that a malicious person could do with it. For this hack, we are using v5.6.1 of the SRVD binary that can be found in their releases. Below are some sample logs of a successful NPT tunnel being created. You can see by this line \"2024-09-15 01:57:41.030420 | serverToServer | Bound ports A: 42283, B: 45235\" that two ephemeral ports are bound to the two sides of the tunnel. The client will then use these ports to connect to the tunnel. The startsrvd.sh script is used to start the srvd @reboot via crontab. Sample Logs Tunnel tear down example Overall crontab This is what the overall crontab on the raspberry pi looks like, so that you can get a better idea of what these scripts are doing exactly."
  },
  {
    "title": "LipTrack",
    "link": "https://devpost.com/software/liptrack",
    "text": "Inspiration Communication for deaf individuals occurs through lipreading. However, lipreading is not a foolproof method of communication, as not all sounds can be easily distinguished from one another through lipreading alone. Much of it depends on audio quality, speaking speed, and clarity of speech. Enter LipTrack with Automatic Speech Recognition through CTC (Connectionist Temporal Classification) Keras. Bad audio? No problem. LipTrack is designed to generate captions just by reading lips. Currently, more than 1.5 billion people live with hearing loss. 67% of them express frustration with not feeling like there are enough programs that aid communication. We aim to make a genuine human impact by closing the gap for disabled people and addressing sustainability for future generations through innovative use of technology. What it does LipTrack allows users to upload a video of the desired speaker and generates captions based on what they are saying. It allows disabled people to read audio with the power of AI and ML. How we built it Using JupyterLab I created a deep neural network with 90 epochs yielding an 84% accuracy rate. Ithen employed the Tensorflow Sequential model for the input and output tensor. This loads the LipNet database videos, converts each frame to grayscale, crops the specific region of interest, normalizes the pixel values, and returns a list of frames as a tensor. OpenCV captures the frames of a person speaking, and an automatic speech text model is used to transcribe what they are saying. That dataset is subbed into the deep learning model training pipeline which loads the aligned transcriptions. Here, the annotations are preprocessed and I decoded this from purely the video, no audio. For the front end, I set up the streamlit framework, loaded in the videos and displayed output tokens and the raw tokens being converted into a set of words. I used 3D convolutions to pass the videos and condense them down to a classification-dense layer which predicts characters - single letters at a time. For this, a loss function called CTC (Connectionist Temporal Classification) handles this output. This function works great for word transcriptions that aren’t specifically aligned to frames. Given the structure of our model, it is likely to repeat the same letter or word multiple times if we use the standard cross entropy loss function. (CTC reduces duplicates by using a special token.) Then I used the greedy algorithm to take the most probable prediction when it comes to generating the output. Keras implementation of the method described is in the paper 'LipNet: End-to-End Sentence-level Lipreading' by Yannis M. Assael, Brendan Shillingford, Shimon Whiteson, and Nando de Freitas (https://arxiv.org/abs/1611.01599). Challenges we ran into Configuring GStreamer plugins to compile and parse OpenCV arrays Transcoding String Tensor of type objects from a source encoding to a destination encoding Accomplishments that we're proud of Successfully implementing Automatic Speech recognition using CTC loss that builds an ASR with Keras, which keeps everything nice and clean. What's next for LipTrack Making it into a Chrome extension for video calling caption generating feature Using a DLib face detector to isolate the mouth instead of doing it statically"
  },
  {
    "title": "PhobiaPhixer",
    "link": "https://devpost.com/software/phobiaphixer",
    "text": "pho·bi·a (/ˈfōbēə/); an extreme or irrational fear of or aversion to something. Inspiration Starting off with an interest in VR/AR applications, our group pondered how we could solve real-world problems with a virtual solution. As we got to know other, we all connected over our shared fear of spiders. We set out to overcome our arachnophobia, which inspired us to create an exposure therapy project aimed at helping users \"face their fears.\" What it does Exposure therapy is a type of therapy in which you're gradually exposed to the things, situations and activities you fear. It can help treat several conditions, like phobias, post-traumatic stress disorder (PTSD) and panic disorder. An example of exposure therapy would be baseball players training against a high-speed pitching machine. They crank the speed up to the highest setting, forcing themselves to adjust and become desensitized to the fast-moving ball. After seeing enough pitches, they can track and hit the ball more effectively. The same principle applies to exposure therapy: when users are repeatedly exposed to their phobia, they gradually become desensitized, and over time, their fear lessens until it’s no longer overwhelming. How we built it To build our project, we first leveraged Glitch's simple hosting capabilities and its support for WebXR, which allowed us to seamlessly integrate A-Frame, a web framework for building VR scenes. Using A-Frame, we created immersive environments and dynamic entities for our VR experience. Next, we integrated Pulsoid, a heartbeat monitor app for Android watches, to gather real-time heartbeat data through API calls, which we displayed in the VR world. We sourced spider assets from a library, animated them ourselves, and programmed them to move randomly around the virtual space, always facing the user. The speed and size of the spiders are dynamically adjusted based on the user's heartbeat, adding an interactive and personal element to the experience. Challenges we ran into Accomplishments that we're proud of What we learned What's next for PhobiaPhixer"
  },
  {
    "title": "Actually",
    "link": "https://devpost.com/software/aktually",
    "text": "Inspiration Staying well-informed is difficult, especially in a world that demands our attention in a myriad of complex and pressing issues like pandemics, climate change, and AI—each demanding a nuanced understanding to make a truly representative decision. It doesn't help that we not only have to seek the truth but discard what's false. What it does Actually helps you stay informed by fact checking your content in real time. Give it a video or livestream of a debate, and Actually will transcribe what's being said, then use AI to provide more context on the topic, verify how true the statements being made are, and list supporting citations. How we built it Actually uses Gemini 1.5 Pro to fact check and provide context on audio transcriptions created with Azure Speech Services. The UI was built using Tailwind, React, and Vite, and the backend is made using Python Flask. Challenges we ran into Accomplishments that we're proud of What we learned What's next for Actually"
  },
  {
    "title": "Hangouts",
    "link": "https://devpost.com/software/hangouts",
    "text": "Inspiration The inspiration for Hangout came during my trip to Toronto for a hackathon. I(Rishi) spent three days feeling bored, believing I didn’t have any friends nearby. After leaving, I realized I had several friends in the area and could have had a much more enjoyable time if I had known. This experience sparked the idea for Hangout, an app that connects friends who are close by. What it does Hangout notifies users when their friends are within a 1 km radius, giving them the opportunity to meet up and hang out if they choose. It’s a simple way to stay connected with friends in real-time and avoid missing out on spontaneous hangouts. How we built it We built Hangout using Flutter for the frontend to ensure a smooth and cross-platform experience. Firebase was used for real-time data, managing friend lists and location updates. The app monitors friends' locations and triggers a notification if they are nearby, giving users the option to connect. Challenges we ran into One of the major challenges was optimizing the app’s location tracking without heavily draining the battery. It was tricky to balance the need for accurate, real-time updates with maintaining a light resource footprint. Another challenge was safeguarding users’ privacy, particularly when handling location data, which required encryption and strict permission handling. Accomplishments that we're proud of We’re proud of building a fully functional and user-friendly mobile app that solves a real-world problem. The optimization of geolocation services to balance performance and accuracy is a key accomplishment, as well as successfully integrating real-time notifications and privacy protections. What we learned We learned a lot about geolocation services, notifications, and real-time database interactions. Working with Flutter and Firebase taught us valuable lessons in efficient app performance and creating a seamless user experience. Additionally, understanding the importance of privacy when dealing with location data was crucial. What's next for Hangouts Next, we plan to implement additional features like customizable radius settings, group notifications for multiple friends nearby, and better integration with social media to enhance the user experience. We’re also considering ways to enhance user privacy even further and improve battery optimization for long-term use."
  },
  {
    "title": "PomoGoose",
    "link": "https://devpost.com/software/pomogoose",
    "text": "🐤 Inspiration 🕒 As students, it's difficult for us to get into the flow state when studying, working, or anything in-between. In other times, we've been too focused/locked into the flow state and forget to take breaks. It's important to maintain a good balance between working and taking breaks. Why not make an extension that helps us do both? 🍅 What Does PomoGoose Do? 🍅 PomoGoose focuses on 3 main things: 1) A Pomodoro technique based timer 2) A goose that covers your screen when you break so that you ACTUALLY take a break 3) A currency/shop system that rewards users for properly doing Pomodoro by allowing them to exchange coins for cute skins :) 🔧 How We Built It 🎨 We built a Chrome extension in Typescript with native HTML/CSS. Most of this project makes heavy use of the Chrome Extension API's. As for design, the shop layout was designed in Figma as a low and high fidelity prototype. 👀 Challenges We Ran Into 👀 Originally, we wanted our goose to eat the cursor and disable the user from using their cursor. Unfortunately, due to security concerns (boring!), Chrome doesn't allow extensions to have that level of control over the user's activity. 🎈 What We Learned🎈 This was the first time we worked with chrome developer tools to create an extension so there was a big learning curve with trying to figure out how to implement what we needed. This was also the first time that anyone on our team had worked with creating wireframes in practice. Going into the project, we didn't know how to take advantage of components, instances, and variants on Figma. We were able to learn about those tools which they made up a large part of our prototypes. ✨ Accomplishments ✨ Some of our proudest achievements: ‼ What's next for PomoGoose ‼️ There's always room to build when setting up a shop whether it's: Our Assets and Repo GitHub Repo Figma Design File + Assets"
  },
  {
    "title": "Mixify",
    "link": "https://devpost.com/software/mixify-esk167",
    "text": "Inspiration I was getting pretty tired of hopping back and forth between Spotify, SoundCloud, and YouTube Music just to listen to unreleased Frank Ocean tracks. 😩 I knew there had to be a better way to manage all my music without constantly switching apps. That’s what led me to create Mixify – a solution to bring together songs from all these platforms into one playlist. 🎶😊 What It Does Mixify makes it super easy to create and manage playlists that pull songs from different services like Spotify and YouTube Music. Instead of juggling between apps, you get everything in one place. It’s all about making your music experience smoother and more enjoyable! 😃 How We Built It Building Mixify was quite a ride! Here’s how it came together: Challenges We Ran Into One big challenge was dealing with the SoundCloud API. I had originally planned to include SoundCloud, but the API isn’t available anymore. That threw a wrench in the works, and I had to focus more on integrating with YouTube Music. But hey, it all worked out in the end! 😊 Accomplishments That We’re Proud Of One of the coolest things is that you can now listen to songs from different platforms all within one playlist. 🎉 It’s a huge win for anyone who wants a seamless music experience without hopping between apps. We’re pretty proud of that! What We Learned I learned a lot about integrating APIs and managing user data securely, including handling SDKs and dealing with token expiry. Synchronizing playlists across different platforms presented its own set of challenges, especially when ensuring a seamless user experience. It also highlighted the need to be flexible and adaptable, especially when unexpected issues arise. What’s Next for Mixify There’s still a lot to do! I’m planning to add more features like advanced music recommendations, better playlist syncing, and integration with even more music platforms. Plus, I want to make the user interface even more intuitive and fun. Stay tuned for what’s coming next! 😃🎵"
  },
  {
    "title": "Rewire",
    "link": "https://devpost.com/software/rewire-4cyf0o",
    "text": "Inspiration As high school students, we relate to the issue of procrastination, especially when mobile devices are in close reach. Rather than deleting social media or downloading app blockers to limit distractions, our team was inspired to solve this issue at the root by rewiring students' relationship with tech. What it does Our product is designed to let YOU take charge of your relationship with your mobile devices. With Rewire, you keep your phone in front of a sensor that detects its presence. By removing your phone away from our product, loud noises and a red light will prompt you to place your device back and stay focused without distractions. How we built it We used an Arduino Uno and SparkFun Inventor's Kit to power our project. The ultrasonic sensor detects when your phone is placed or removed in front of the sensor. Rewire alerts the user through a Piezo buzzer and colorful LEDs that are activated when specific distance conditions are met. Challenges we ran into As a team of beginner hackers, we wanted to step outside our comfort zone and create a project using hardware. After receiving parts from the hardware hub, we ran into the challenge of finding a problem to solve in this open-ended hackathon. We started to tackle this ideation challenge by speaking with mentors, who provided us with great advice. Ultimately, we overcame this challenge by choosing an issue that was relevant to us as high school students. Accomplishments that we're proud of We are proud of our perseverance throughout this hackathon that led us to successfully complete this project. Although we dealt with roadblocks such as miswired electrical components and bugs in our code, we remained persistent and worked together to achieve our goal. Also, we're so proud and excited to be a part of HTN 2024! What we learned We developed skills in C++ and refined our skills of wiring breadboards, overall combining software and hardware to create a final product. What's next for Rewire We plan to utilize buttons so users can input their required study hours, and the LCD Display from SparkFun Inventor’s Kit can be used to display remaining hours as well as encouraging messages for users. In addition, we can implement productivity methods such as Pomodoro technique to take strategic breaks to boost productivity."
  },
  {
    "title": "Verifai",
    "link": "https://devpost.com/software/verifai-nxcqgy",
    "text": "Inspiration With the US Presidential Debate being fresh in our minds, we truly realized the magnitude of the pandemic of misinformation. With so many false claims being thrown around, we found it most important to call the issue out from its roots. We recently read an article about the state of modern journalism, stating that the most effective piece of news is actually local news networks. However, these networks are often dominated by large corporations, often causing skewed truths, intense biases, and the rapid spread of misinformation. We knew that thorough research performed by intelligent software would be the solution to such a major problem in today’s society. What it does Verifai is the solution to the rapidly increasing spread of misinformation in digital media. By simply pasting a YouTube video link, which scrapes the entire transcript of the video. This is then annotated with key details about each major “chunk” of the video. Verifai will indicate whether the chunk is a fact, fake, or if there is not enough information to come to a conclusion. Additionally, the tool also gives a brief description about the chunk in order to provide context or to criticize the validity of the speaker. Verifai also gives up to 3 sources in MLA format to prove the credibility of the claim. How we built it We started off by deciding our tech stack.For the frontend we chose React and for the backend we chose Flask.The choice of the stack was simple.React is excellent for building front-end interfaces and our backend needed something in Python because we planned upon using the Cohere API which is highly documented with community support for its Python API.We leveraged the youtube’s transcript api to generate the transcripts for the given video on Youtube.Then we feed the transcripts as text corpus to cohere’s chat api endpoint to classify a given text corpus as factual,non-factual or not having enough information.We leveraged the web connector to generate citations as well. Challenges we ran into Our initial tech stack was solely based on AWS infrastructure leveraging Sagemaker,CodeWhisper,AWS transcribe and Cohere’s API.But we did not get any credits for the hackathon.So we had to make do with what we had.We simply used Youtube’s auto-generated transcripts and then classify the text.We also had the idea of using next.js as our stack and expose the deployed model on AWS as an API.But those plans were let go off and we settled upon React and Flask. Accomplishments that we're proud of A working project :) for starters. Going through the learning curve of getting unstructured text data from the video and converting it into useful data for the cohere model by some unconventional data preprocessing methods. Lastly, we feel this can be a starter project of something big in the field of fact checking social media information in real time. What we learned Diving deep into the world of misinformation, we learned that the situation needs more attention as more and more users become active on social media. Overall, our group learned a lot of new technical skills in the entire tech stack, ranging from frontend to backend. Since everyone had their own choice of tools, it was often difficult to merge our respective parts. What's next for Verifai We want to allow users to fact-check Instagram reels.We plan to build an extension for that.What we have is a proof-of-concept working but our plan is to work on the AWS infrastructure and create a industry ready solution."
  },
  {
    "title": "BrandReach",
    "link": "https://devpost.com/software/brandbreach",
    "text": "Inspiration💡 Imagine a world where every brand’s social media is effortlessly vibrant and engaging, capturing attention with visually stunning posts and compelling captions. Our application, BrandReach, was inspired by the idea of streamlining and automating this creative process, allowing businesses to focus on their core operations while maintaining a dynamic online presence. We envisioned a tool that not only simplifies content creation but also ensures that every post resonates with its target audience, boosting engagement and brand visibility. What it does🤔 Our application creates appealing marketing content for companies and automatically posts them on several of their social media accounts! How we built it🤖 We used a Cohere API to generate a caption, as well as a Stable Diffusion API for a captivating post. We also used Tweepy, a Twitter API that lets us post tweets despite not being on Twitter 👀 Challenges we ran into🐛 • Lack of access to many APIs, such as OpenAI (DALL-E), Meta, and Youtube • Authorization is required for many social media platforms, which is a complicated process • Putting all the code together with the components of text & image generations with the posting to Twitter • staying awake ... ☕ Accomplishments that we're proud of⭐ • Finding an alternative API to OpenAI DALL-E that works well • Successful posting of content to Twitter What we learned📑 • Learned about how APIs work and how to implement them!! What's next for BrandBreach🔮 • Next, we plan to expand to include options for brands to post promotional content on other social media platforms, such as Instagram, Linkedin, Facebook, and Youtube! • Want to include a feature of scraping content from the business's social media page that provides the LLM with more context, and also reduces the need of providing additional content"
  },
  {
    "title": "Help, we're locked in the DP basement!",
    "link": "https://devpost.com/software/help-i-m-trapped-in-the-dana-porter-library",
    "text": "Inspiration Dana Porter Library soars above the rest of campus, being the tallest building within Ring Road and a network hub for thousands of students everyday. However, with ten floors filled books and tables and people, getting lost within the bookshelves is a universal experience for any visitor. The wayfinding signs in the library are unclear and confusing, floor maps hidden away from plain sight. We knew that with MappedIn, we could create not just a plain old floor plan, but an interactive mobile application that could weave around the labyrinth known as the campus's Sugar Cube What it does By using our program, students and visitors are able to search and select the place of any room or location in any one of the ten floors and immediately receive a direct path to their desired location! How we built it We utilized MappedIn's AI features and platform to construct the base of the project, while SDK and React proved helpful in building the front-end. Challenges we ran into A major challenge that we ran into was the crunch on time. Having to learn so many different components and programming through the night was an experience unlike any other. Using MappedIn's features was completely new to all of us, so just figuring out different components and ideas completed the back log. Accomplishments that we're proud of Ultimately, we are proud of the work that we did in as short a time as there was. Learning a new software and language proved to be an uphill challenge. However, the experience had affected all of us and it was impressive to see the program running and working in real time. What we learned As a team, we sharpen our design skills and became familiar with the new languages and technologies. It was an amazing experience to work on this project and contribution to the final main product. What's next?? For future development, we want to incorporate real-life tracking of individuals onto the map, in order to ease the searching process even more. Additionally, given it's setting as a library, a book catalogue would be an extremely useful tool for the application. Expansions of the design could be used to simplify other poorly constructed buildings, such as PAS, Toronto Union Station, or Pearson Airport Terminal Three."
  },
  {
    "title": "Artificial Intelligence",
    "link": "https://devpost.com/software/artificial-intelligence-bazu82",
    "text": "Inspiration The growing technology What it does making the time less to complete the task How we built it using technologies Challenges we ran into spending too much time in a single task Accomplishments that we're proud of we can able to complete the task in minutes What we learned we about the smartness of the human What's next for Artificial Intelligence Narrow AI: Also known as weak AI, it is designed and trained for a specific task. Examples include voice assistants like Siri or Alexa, and recommendation systems on platforms like Netflix or Amazon."
  },
  {
    "title": "Tag-Along",
    "link": "https://devpost.com/software/tag-along-e0ldv4",
    "text": "Inspiration University life involves much more freedom and Tag-Along doubles as a tool for users to not only share their ambitious plans for hangouts, but also: What it does How we built it Tag-Along is made using the React framework, but is programmed in TypeScript instead of the traditional JavaScript to facilitate stricter type-checking. To style the front-end, we used Tailwind CSS for speed and convenience. In terms of our backend, Firebase was used for easy Google Authentication and for their cloud database, Firestore. Additionally, to generate and display maps, we used MappedIn's API and SDK. Their indoor maps were very detailed, shown by their intricate floor plans of E7 which we used. Challenges we ran into Accomplishments that we're proud of What we learned Later on in the Hackathon we encountered some design and architecture issues that could've gone a lot smoother had we planned them out earlier. Next time we'll try to avoid last-minute decisions by spending more time at the beginning discussing them. What's next for Tag-Along"
  },
  {
    "title": "autoM8",
    "link": "https://devpost.com/software/autom8",
    "text": "Inspiration Our inspiration for developing this tool stemmed from the challenges we faced with handling large files. Whether it was struggling to process extensive datasets for model training, grappling with unwieldy CSVs during data cleaning without a concise summary, or uploading files to the cloud for basic image processing—these tasks were time-consuming and often overlooked. Editing these files manually was both labor-intensive and inefficient. This is why we decided to enhance the already impressive MASV tool with a new extension designed to streamline these processes. What it does With the help of MASV, users are able to apply edits to large amounts of image and video files, while guaranteeing the speed, efficiency, and consistency that MASV provides. While focused on images and videos in this setting, the concepts are easily extended to virtually any file processing text you can imagine. How we built it The web app was built and deployed with Defang and the file-saving system was created using MASV API. The image and video processing use python OpenCV library and cloud integration is done on Google Drive. Challenges we ran into One of our biggest challenges was switching projects with less than 24 hours left during the hackathon. With this, it was difficult for us to catch up to the level that other teams were ahead in their projects. For this reason, we were unable to finish the project to the point that we envisioned. It lacks the fully automated features it could have with more work, but the components are there for further polishing. Accomplishments that we're proud of Still managing to get a product together despite the rough start. Managed to stay persistent and not lose our desire to build something cool. Ultimately we are really proud of the idea and implementation we have put together and see it as something we can build our own workflows on top of. What we learned Learned how important it can be to remain open to new ideas and to pivot quickly. If we hadn't made the quick decision to create a different project, who knows what kind of product we would have now. We're taking away this idea of flexibility and looking to apply it creatively in our future projects. What's next for autoM8 Completely automate the data processing pipeline. Provide more support for file formats and processing tasks, making it more applicable across domains. Integrate unique processing featues (LLM analysis, more sophisticated classification of images, etc.)"
  },
  {
    "title": "BetterCommits",
    "link": "https://devpost.com/software/bettercommits",
    "text": "Inspiration As developers, we often find ourselves building a habit of creating poor commit message descriptions when building personal projects. As this habit carries over when doing internships, co-ops or jobs, we wanted to build a solution to the habit. What it does BetterCommits is a VSCode extension which gives real-time feedback to user’s commit messages based on a chosen commit message template convention. It highlights parts of the user’s commit message which lack detail or are “weak”. BetterCommits then creates a tooltip on the highlighted section which the user can hover over with their cursor to see the suggestions. How we built it We built BetterCommits using TypeScript, Python, and Cohere. We used TypeScript for the extension part (communicating with VS-code) and Python and Cohere’s SDK to handle the real-time feedback logic. Challenges we ran into Initially, we wanted to have our project be purely TypeScript based. This turned out to be a mistake as we ran into difficulties setting up the Cohere SDK for TypeScript. Compared to its counterpart written in Python, the TypeScript version was much more complex and difficult to work with despite having the same results. Because of this and the Cohere representatives suggesting we migrate, we had to move a large portion of our TypeScript codebase to Python. This migration took a long time because of how our logic was set up initially. Setting up the Python libraries was also a mini struggle as everyone’s machine was on different versions of Python which made it harder to work on it together due to library version mismatch. Accomplishments that we're proud of We’re proud to have built a product we find helpful in our day to day lives. Although our project doesn’t solve world hunger, cure cancer or something unimaginable, we’re still proud to have built something that helps people by just a little, even if it's just saving a couple minutes of their time. What we learned During the development of BetterCommits, we learned how to combine both TypeScript and Python code, utilizing the strengths of both languages. We also integrated the Cohere SDK and API in order to achieve our goal. Through this process, we learnt about the range of possibilities available through the use of language models to produce efficient solutions. What's next for BetterCommits Some ideas for the future we have for BetterCommits are: User signup/signin and customize their preferences PR templates the user can choose in their settings or make their own Storing good PR’s in a database Can be used as training data for cohere, a way for users to upload examples of good PR’s that follow their specific conventions and train cohere’s feedback to mimic said examples"
  },
  {
    "title": "StreamGuard",
    "link": "https://devpost.com/software/streamguard",
    "text": "Intro Have you struggled with tongue slips? Done either by your favorite streamer or perhaps even yourself? Happens occasionally but when it does, it calls for disaster for the creator. One wrong word could mean losing portions of your audience, or worse, a long developed career.\nThis is where StreamGuard come to save the day... StreamGuard is an innovative content moderation tool designed to enhance the live streaming experience by filtering out unwanted language in real-time. Whether you’re a Twitch streamer aiming to maintain a family-friendly channel or a broadcaster ensuring professional standards during live interviews, StreamGuard has you covered. Why StreamGuard? In the fast-paced world of live streaming, maintaining a clean and professional broadcast can be challenging. StreamGuard provides a reliable solution to prevent unintended slip-ups from reaching your audience, protecting your brand and ensuring a positive viewing experience for all. Inspiration StreamGuard was inspired by the need for live streamers to protect their livelihoods, as well as the desire of certain consumers to moderate the content they listen to. How we built it We built StreamGuard's frontend using Next.js and Tailwind CSS for smooth transitions, creating a visually appealing user interface, all deployed on Vercel for easy updates. The backend, powered by Flask, handles HTTP requests, while PyDub and PyAudio manage audio processing. We leveraged PyTorch for machine learning alongside with OpenAI’s Whisper providing real-time speech recognition for accurate transcription and filtering. Real-time communication is facilitated by Socket, ensuring minimal latency. Finally, our final goal is for StreamGuard to be deployed on Google Cloud Platform, where it can thrive in larger clusters, taking advantage of the larger AI models via more resources. How to Real-Time As you might imagine, with one big difficult problem, you must first break it down into many manageable chunks. So that is exactly what we did! Our software breaks down the live feed into 1s to sub 1s segments which are processed individually then put together to create the final transcript you see. The system is intelligent and hence the chunk cuts happen when you finish the word you are speaking, i.e. the instant before you say the next word in your sentence. This opens many gateways and possibilities for us, concurrent-programming/parallel-computing, ability for APIs and faster response times are a few to mention. But for this project we are using concurrent programming at small scale to get the job done. Challenges we ran into Creating a Real-Time-System is easier said then done. Here's a peek into issues we faced and challenges that almost made us scrap the idea: Challenge1 - Difficulty Moving Data\nMost of the systems you interact with on the internet run on traditional API calls with fixed amount of information. You cannot send a stream of data over http. Therefore, you are very limited in terms of frameworks and communication methods you can use. Challenge2 - Unfamiliarity Although we had a member familiar with real time systems, creating a working application from the ground up created many new ways one can mess up. We ran into countless errors we had to debug, libraries and frameworks we were unfamiliar with, new technologies, and many other challenges during the weekend. Accomplishments that we're proud of We are proud of creating a unique idea with a legitimate business use case, and all the work we put in to create a proof of concept. What we learned We learned more about our respective tech stacks, as well as about the design, collaboration and development processes and of course; about real time systems. What's next for StreamGuard We do have a Roadmap for StreamGuard on our git repo which we hope to someday accomplish. But here's a quick summery: Customized alerts - maybe you don't want to have a beep sound effect but want to mute or play your own sound effects!\nDashboard - A web-based dashboard for configurations and monitoring.\nLive Translation - In addition to live transcription and live moderating, our system allows for development of live translation removing the language barrier for audiences speaking different language"
  },
  {
    "title": "GoosePeak",
    "link": "https://devpost.com/software/goosepeak-hixzd8",
    "text": "💡Inspiration Your professor assigns a YouTube lecture to review, but you end up watching Naruto or a Mr. Beast video instead. GEESE WHAT, you missed the deadline! We've all been there, and it's not a great feeling ... that's where GoosePeak steps in to help 💪. GoosePeak is a chrome extension that's dedicated towards helping you reach your \"peak\" productivity, by \"peeking\" at your screen! With GoosePeak, you find your own goose buddy 🪿that grows more and more frustrated as you neglect your tasks and absorb irrelevant content! 🧐What it does GoosePeak first asks for your task list. Once your tasks are confirmed, you are free to browse the web. As long as you stay on sites that align with your tasks, your virtual goose buddy remains calm and content. Very demure, very mindful 😌! However, if you stray off course, be prepared to face the wrath of your goose buddy as their frustration grows 😡. Not only that, you'll be faced with an annoying pop-up that will steer you right back to being productive and navigating the right links. ❓How we built it In developing GOOSEPEAK, we used the following technologies: Design: Played around with Figma to create the different pages and UI elements. Front-end: We built our interface and pop-ups using React, HTML, TailwindCSS, and JS Back-end: Our back-end is powered by Node.js, and we also leveraged the Cohere API. APIs: We have used Cohere's API to extract information from the links and identify whether they include keywords that are relevant to the task list. 💢Challenges we ran into Like any hacker, we encountered our own issues with merge conflicts. We also struggled with connecting to the Cohere API in a chrome extension environment. It even took all of us quite some time in understanding everyone’s code and building our code based on each other’s outputs. 🎖️Accomplishments that we're proud of Although our team has little to no experience in developing chrome extensions, we are very proud that we have come this far to create a working MVP in a fast-paced environment. We are also excited to see our adorable geese come to life, transforming from a simple idea into an engaging feature on our Chrome extension 🖊️ 😎What we learned 👀What's next for GoosePeak"
  },
  {
    "title": "DaVinci Solve",
    "link": "https://devpost.com/software/davinci-solve",
    "text": "Inspiration Both of us are students who hope to enter the now deflated computer science market, and we shared similar experiences in the mass LeetCode grind. However, we also understood that simply completing LeetCode questions weren't enough; often, potential candidates are met with an unpleasant surprise when they are asked to walk through their thought process. We believed that not being able to communicate your algorithmic ideas was a missed problem, and we hoped to eliminate this with our solution. What it does DaVinci Solve is a way for users to practice communicating their thought process in approaching LeetCode problems (and any other competitive programming problems). This website is an AI-simulated interview scenario which prompts users to outline their solutions to various LeetCode problems through speech and provides feedback on the algorithmic approach they provide. How we built it We used Gradio as a quick front-end and back-end solution. We implemented both Groq and OpenAI APIs for speech-to-text, text-to-speech, and LLM generation. We implemented Leetscrape to scrape problems off of LeetCode, then used Beautiful-Soup to format the problem for display. We started by mapping out our idea in a flow-chart that we could incrementally complete in order to keep our progress on track. Then we organized our APIs and created a console-based version using various calls from our APIs. Then, we used Gradio to wrap up the functionality in a MVP localhost website. Challenges we ran into There was a good amount of bugs that kept surfacing as we wrote more code, and it was also hard to figure out how to move on with the lacking documentation and flexibility from Gradio. However, eventually we pulled out the bug spray. With a good amount of perseverance, we finally weeded out every single bug from our code, which provided a good amount of relief. Accomplishments that we're proud of We're pretty proud of being able to go all out on our first hackathon project while spending lots of time for other activities and enjoying the hackathon experience. We even managed to avoid consuming any caffeine and steered away from all-nighters in favour for a good amount of sleep. But definitely the most satisfying thing was getting our project to work in the end. What we learned This was our first hackathon, and it was an eye-opening experience to see how much could happen within 36 hours. We developed skills on building and testing as fast as possible. Lastly, we learned how to cooperate on an exciting project while squeezing out as much fun out of Hack the North as we could. What's next for DaVinci Solve We plan on implementing a better front-end, a memory system for the feedback-giving LLM, and an in-built IDE for users to test their actual code after illustrating their approach."
  },
  {
    "title": "Coffee 2.0",
    "link": "https://devpost.com/software/coffee-2-0",
    "text": "Inspiration We've all experienced it- heavy eye lids and unwillingly nodding off to sleep. Whether you're a hacker trying to make it through the night, a student at a boring lecture, or a sleepy driver, Coffee 2.0 can help you stay alert and awake. What it does When a camera detects that you're falling asleep, it will notify an Arduino to sound a buzzer and spray water at you. This will help keep users wide awake without the need for caffeine. How we built it We built Coffee 2.0 using Arduino for the hardware and Python for sleep detection. We used the Dlib library to obtain coordinates that map out the user's face. These coordinates were then used to determine whether the user's eyes were drooping or awake. Challenges we ran into Connecting the hardware to the backend. We spent a long time trying to send data to the Raspberry Pi Pico, but realized it wasn't possible to send messages to the GPIO pins from our laptop. We had to switch to transferring data through the Arduino by using a light sensor. When the backend detects that you're sleeping, it will turn a box white, notifying the Arduino to turn on the buzzer. Accomplishments that we're proud of Despite facing issues with the Raspberry Pi Pico, we are proud that we found an alternative way to send data to the hardware. We are also proud of having a working project that can successfully detect when you're falling asleep. What we learned As it was our first time using the Raspberry Pi, we learned a lot the capabilities and functionalities of the Pico. We learned that going forward we should definitely use a Raspberry Pi 3 or similar model because the Pico has limited functionalities when it comes to sending and receiving data."
  },
  {
    "title": "Wisewear",
    "link": "https://devpost.com/software/ace-a",
    "text": "Built With"
  },
  {
    "title": "RISC-V 32 bit Program Emulator",
    "link": "https://devpost.com/software/risc-v-32-bit-program-emulator",
    "text": "Built With"
  },
  {
    "title": "Polaris",
    "link": "https://devpost.com/software/polaris-j9pcs2",
    "text": "Inspiration As both attendees and organizers of school and social gatherings, we’ve experienced the frustration of trying to keep everyone engaged. It’s a lofty goal when faced with an overwhelming number of sub-events, the challenge of finding mentors and support, and the constant hunt for nearby food options. These struggles inspired us to create a solution that simplifies event navigation and enhances the experience for everyone involved. What it does Introducing Polaris—a one-stop event companion app powered by the Mappedin API. Polaris is designed to solve the common pain points of event attendees and organizers by offering real-time navigation, personalized event recommendations, mentor tracking, and even snack truck locations. Whether it’s optimizing schedules, finding mentors, or knowing where others are in real-time, Polaris ensures a seamless and immersive experience at any large-scale event. How we built it We built Polaris using Next.js for the front-end, ensuring a smooth and intuitive user experience. The Mappedin API powers our detailed indoor navigation, helping users find their way around complex event venues. MongoDB serves as our database, storing event data, user profiles, and preferences. Node.js-backed backend handles real-time event updates, mentor tracking, and user interactions. Additionally, we integrated an AI chatbot for personalized event recommendations and instant user support. Challenges we ran into One of the key challenges was integrating the Mappedin API seamlessly with our app to provide real-time, accurate navigation and event data. We also faced some difficulties optimizing the mentor tracking feature, which required precise location data and efficient algorithms. Accomplishments that we're proud of We’re proud of creating a fully functional proof of concept that integrates multiple technologies, from the Mappedin API to AI chatbots. Our real-time mentor tracker and snack truck locator are particularly exciting features that add real value for attendees. Additionally, we’re proud of the user-friendly interface and the overall design, which make Polaris an intuitive and enjoyable app to use. What we learned Throughout this project, we gained a deeper understanding of working with complex APIs like Mappedin and integrating them into an app with multiple real-time features. Moreover, we learned the importance of balancing the technical side with user experience design to create a product that not only works but also resonates with its audience. What's next for Polaris In the future, we plan to enhance Polaris by adding more advanced features such as real-time feedback collection from attendees, social media integration to increase event visibility, and even more personalized event suggestions. We’re also considering expanding Polaris to include voice-activated commands for hands-free navigation, making the app even more user-friendly. Lastly, we aim to scale the app for use in larger events, conferences, and festivals."
  },
  {
    "title": "Enhance",
    "link": "https://devpost.com/software/enhance-tlrcdz",
    "text": "Inspiration Lucy Guo's Passes story inspired us to implement the CRM functionality she was seeking. What it does Our hackathon integrates with any CRM via an api endpoint, and requires nothing but a string to be stored for the functionality. It will update user learnings via the CRM inbox, and when mass messages are sent out these learnings will be added when appropriate to make the messages feel more heartfelt. How we built it We built on top of Convex in Next.js for our mock CRM, and our functionality is offered by calls to AI models. We have a TypeScript backend. Challenges we ran into Accomplishments that we're proud of What we learned What's next for Enhance"
  },
  {
    "title": "IntelligentTutor",
    "link": "https://devpost.com/software/intelligenttutor",
    "text": "Inspiration The inspiration for the IntelligentTutor project stems from a challenge familiar to many students—needing assistance with course material when professors or TAs aren’t readily available. As students, we often have urgent questions that can lead to stress, and waiting for answers can disrupt our learning process. IntelligentTutor was designed to address these doubts instantly, offering timely support to students whenever they need it, helping them feel more confident and less anxious about their studies. Additionally, with over 2.19 million university students in Canada alone grinding through coursework year-round (and 200 million worldwide), many still miss out on key topics due to time constraints. IntelligentTutor aims to provide them with quick and easy access to essential learning materials, ensuring no one falls behind. What it does IntelligentTutor 📚 is here to make studying easier and smarter. After signing up with your email, GitHub, or Google account, you can add all your courses to a personalized dashboard. For each course, upload your textbooks, notes, or any other materials in PDF format—just once. From then on, IntelligentTutor does the heavy lifting. Our AI-powered chatbot 🤖 is trained specifically on your course content. Whenever you have an exam, test, or quiz coming up, just ask questions like “Explain Topic A” or “Summarize Topic B” and get immediate, detailed responses based on your uploaded materials—no need to search through files or specify file names - bot knows exactly where to pull the information from. It's that simple! ✨ But that’s not all! Once you've reviewed a topic, IntelligentTutor lets you test yourself instantly 📝 by generating quizzes on the fly. This way, you can check your knowledge and get real-time feedback, helping you stay on top of your studies. With IntelligentTutor, you get personalized support 24/7, saving time and reducing stress 🎯, so you can focus on mastering your courses and boosting your confidence. Get instant explanations, quiz yourself, and study any topic you need—without the hassle of digging through countless PDFs. It’s all about making your learning experience stress-free and efficient 📚. How we built it We built IntelligentTutor using a combination of cutting-edge technologies to ensure a seamless and personalized learning experience. Frontend: The user interface is built with Next.js and TailwindCSS providing a responsive and interactive dashboard where students can easily manage their courses and materials. Backend: Our backend is powered by Typescript and Python, and manages user authentication with Clerk. AI & NLP: To provide intelligent responses, we integrated LangChain and OpenAI’s GPT model. To create high-quality questions from answers, we are using Groq which leverages the Llama3 model. These tools enable the chatbot to understand questions and generate answers based on the course-specific PDFs uploaded by students. Database: We leveraged ChromaDB, a vector database, and Prisma to store the embedded representations of the course content and ensure that the chatbot only responds with material from the correct documents. This was made possible with Cohere embeddings which we were able to use due to the $150 credit (we embedded really big files). Challenges we ran into Accomplishments that we're proud of -User-Friendly Frontend: We created a highly intuitive and responsive interface, making it easy for students to navigate and manage their courses. What we learned What's next for IntelligentTutor"
  },
  {
    "title": "Octochat",
    "link": "https://devpost.com/software/octobot-w6zkmh",
    "text": "Inspiration When approaching a large repository, it is often difficult to parse through the whole codebase. The average time expected for a new developer (not intern) to become fully productive is about 7 months. That's why we made Octochat: a personal assistant that answers any question you have about any codebase, big or small, with the potential to onboard developers (including ourselves) faster and dive into what's important. What it does Octochat helps developers understand a new code file by allowing them to ask questions about how code snippets or functions work, search for where something has been implemented, how to debug an error, help generate better documentation if needed, and overall, more quickly understand the layers of code. How we built it We integrated Voiceflow's API in a wxt-based extension, leveraging complex AI agents to navigate equally complex codebases. The front end was created with React and Typescript Challenges we ran into We struggled with injecting the pop up using an extension, and barriers using VoiceFlow's chatbot design, which led to the need for a custom design. We also ran into difficulty with the frontend making a post request and connecting the request from the user. Accomplishments that we're proud of We are proud of getting the backend working. What we learned We learned how to use the WXT framework. What's next for Octochat Next, Some other features brainstormed include:"
  },
  {
    "title": "REMI",
    "link": "https://devpost.com/software/remi-m10a54",
    "text": "Inspiration At Hack the North 11, dreaming big is a key theme. I wanted to play off of this theme, in combination with my growing interest in AI, to create a chill platform to help people realize and achieve their best literal dreams! I enjoy listening to ambient music to wind down before I go to sleep, and I'm sure I'm not the only one who does, so I wanted to create something where people could, with the help of AI, make their own custom ambient tracks to listen to before or as they're falling asleep, overall enriching their pre-sleep routine and (hopefully) having better dreams. What it does The user, when making a custom track, enters up to three keywords (or \"ambiances\") that are then provided to the AI music generating platform soundraw.io to create ambient music that closely aligns with their desired sound. There are also pre-made \"templates\" of ambient tracks that a user can easily click play and listen to at their leisure. How we built it AI ambient tracks were generated using soundraw.io. The web app interface was built using the Vue.js framework, with additional JavaScript, HTML, and CSS code as needed. Challenges we ran into In all honesty, this project was mostly to push my own limits and expand my knowledge in front-end development. I had very little experience in that area of development coming into HTN 11, so I faced a number of challenges in building an interface from scratch, such as playing around with HTML and CSS for the first time and properly creating transitions between different pages based on which icons I clicked. Accomplishments that we're proud of I have never done a solo hack before, nor have I ever built a web app interface myself, so these were some major personal milestones that I am very proud of and I did a lot of learning when it came to front end development! What we learned How to create a web app interface from scratch, more about AI text-to-sound. What's next for REMI Future improvements could include (but are not limited to):"
  },
  {
    "title": "VoiceLens",
    "link": "https://devpost.com/software/voicelens",
    "text": "Inspiration People who are mute struggle to communicate with people in their day-to-day lives as most people do not understand American Sign Language. We were inspired by the desire to empower mute individuals by bridging communication gaps and fostering global connections. What it does VoiceLens uses advanced lip-reading technology to transcribe speech and translate it in real-time, allowing users to choose their preferred output language for seamless interaction. How we built it We used ReactJS for the frontend and Symphonic Labs API to silently transcribe and read the lips of the user. We also then used Groq's llama3 allow for translation between various languages and used Google's Text-to-speech API to voice the sentences. Challenges We faced challenges in accurately capturing and interpreting lip movements, as well as ensuring fast and reliable translations across diverse languages. Accomplishments that we're proud of We're proud of achieving high accuracy in lip-reading and successfully providing real-time translations that facilitate meaningful communication. What we learned We learned the importance of collaboration between technology and accessibility, and how innovative solutions can make a real difference in people's lives. What's next for VoiceLens We plan to enhance our language offerings, improve speed and accuracy, and explore partnerships to expand the app's reach and impact globally."
  },
  {
    "title": "LockedIn",
    "link": "https://devpost.com/software/lockedin-bcn2re",
    "text": "Inspiration As UW engineering students, studying takes up the majority of our day (sadly). Especially during the exam season, finding a place to study in popular places like the library or student centre is impossible. However we quickly discovered that the UW Portal app lists free classrooms with no lectures going on for the rest of the day! We found that these rooms are completely silent, isolated from other students, and have no distractions. However, there was one issue: these classrooms are scattered across campus and the rooms can be difficult to find. After learning about MappedIn, we knew it was the missing piece of the puzzle. MappedIn can create 3D designs of buildings across campus, allowing us to find to available empty classrooms easily. Combined with the UW Portal API, we were inspired to create a tool that drastically cuts down on finding open classrooms, and lets us get to studying faster! What it does Users can navigate to https://www.lockedin.study/ to see a 3D rendering of buildings that have available empty classrooms. They can select any of the available buildings to access, as well as a room number and the floor of the building that they are on. MappedIn will then create a straightforward path to the room, traversing floors if necessary. How we built it We used React with TypeScript to build the front end of the project, using the MappedIn SDK to create navigation routes. We also using MappedIn's platform to create a complete indoor map of select buildings on campus. We integrated the UW Portal API to dynamically generate available study rooms in buildings. For the backend, we used Express.js to store basic info about user likes and study room availability. Challenges we ran into Working with MappedIn's SDK and mapping editor was a new experience for all three of us. In particular, creating navigation routes spanning multiple floors was a challenge, but we were quickly able to figure out how to use MappedIn's React hooks and map objects. Accomplishments that we're proud of We're proud of successfully implementing our map with the UI while keeping the interface accessible and clean. Working on solving a problem that all three members experience daily was very enjoyable since we got to solve a problem that directly influences our day-to-day life in university. What we learned Building LockedIn inspired all three of us to go out of our comfort zones with new technologies and skills. We all came out of this with greatly improved frontend, backend, design, and mapping skills. What's next for LockedIn In the future, we aim to implement geolocation to make the user experience even simpler by tracking their movements in real-time. Geolocation can also be used to detect if a classroom is in use."
  },
  {
    "title": "Calm",
    "link": "https://devpost.com/software/calm-j5kenq",
    "text": "Built With"
  },
  {
    "title": "AutoMate",
    "link": "https://devpost.com/software/automate-u6okwb",
    "text": "Inspiration The inspiration for autoMate came from the desire to simplify how we interact with the web. The idea was to create an AI-powered assistant that could handle everyday web tasks effortlessly, from finding the latest news to writing emails. Our main goal is to help the elderly access the internet by communicating in natural language, something they are much more familiar with. By combining natural language processing (NLP) and web automation, we aimed to reduce the time spent on manual searches and repetitive actions. The vision was to build an assistant that could work seamlessly with text and voice commands, offering a hands-free and more accessible experience for all users. What it does autoMate is an AI-powered web-browsing assistant that allows users to perform tasks through natural language input or voice commands. Users can ask autoMate to perform tasks like: With the integration of Cohort for AI generation and Groq for voice recognition, autoMate also generates content dynamically and transcribes voice commands in real-time. How we built it autoMate was built using a combination of web development tools and AI frameworks: Challenges we ran into Integrating real-time voice recognition using Groq posed challenges around maintaining low latency and ensuring accurate transcription. Fine-tuning the NLP models to understand various natural language inputs while also responding correctly to voice commands was another hurdle. Lastly, designing a user interface that was both simple and powerful enough to cater to diverse tasks required several iterations. Accomplishments that we're proud of We’re proud of successfully integrating voice recognition and AI content generation into autoMate, making it both user-friendly and highly functional. Another highlight is the step-by-step transparency feature, which ensures users know exactly what the AI is doing, building trust in automation. We also achieved significant accuracy in interpreting user intent, even from spoken commands. What we learned Through this project, we learned a lot about the intricacies of combining AI technologies with real-time systems like voice recognition. We gained deeper insights into optimizing AI models for both speed and accuracy, especially when handling NLP tasks in both text and speech. Additionally, working on automation workflows highlighted the need to balance between user control and efficiency. What's next for autoMate The future of autoMate lies in further enhancing its learning capabilities, allowing it to personalize responses based on individual user preferences. We also plan to introduce more integrations, such as shopping assistance and automated document generation. Expanding multi-lingual support for both text and voice input is another priority, making autoMate accessible to a global audience. Lastly, improving the AI’s decision-making transparency to offer even more detailed action breakdowns will continue to be a core focus."
  },
  {
    "title": "HeadSpace",
    "link": "https://devpost.com/software/headspace-bd1ytu",
    "text": "Inspiration The idea was born from the concept of bringing a journal to life. Journaling is a powerful tool for self-reflection and emotional processing, but what if you could immerse yourself in those emotions? What if the world around you could change based on how you feel or what you've written? We wanted to create an experience where players can explore their inner world in a more interactive and dynamic way, amplifying their emotions through an evolving environment. What it Does HeadSpace takes your journal entries and turns them into a living, breathing world. As you write or speak your thoughts, the game uses AI to interpret your words and generate a 3D environment that reflects your emotions. Whether it's a serene forest when you're feeling calm or a stormy sea when you're overwhelmed, the environment changes to match your mood. The goal is to provide a unique and immersive way for players to explore their emotions and gain insight into their mental state. How We Built It Challenges We Ran Into Accomplishments That We're Proud Of What We Learned What's Next for HeadSpace HeadSpace is just getting started, and we can't wait to see how players interact with and benefit from this immersive journaling experience!"
  },
  {
    "title": "RoboGenie",
    "link": "https://devpost.com/software/robogenie",
    "text": "Inspiration I often find myself building simple web scrapers which send me updates via email. But most non-hackers will never even try doing this. With RoboGenie, they can create notification conditions in plain english! What it does RoboGenie can notify you about anything. For example: “Monitor the weather forecast and when it's going to be below 20F, send me an email with the prices to fly to Cancun” How we built it When you make a new request, an LLM agent makes a plan and asks you clarification questions. Then, the task is scheduled to execute at a specific time. Upon execution, a new agent runs which has access to a web browser and the ability to send you an email. It can also take persistent notes on a scratchpad and reschedule itself to run at a later time, enabling infinite flexibility with ongoing tasks. Challenges we ran into Getting the LLM to understand the content of a web-page and how to interact with it is quite challenging. I tried visual models like GPT-4o, and I tried feeding the entire DOM to the LLM, before eventually building a custom DOM-to-LLM-readable-tree conversion process. Accomplishments that we're proud of This is probably the smoothest product experience I've ever built at a hackathon. What we learned I learned how to build more complicated agent workflows and what works and doesn't work in terms of LLM function calling. For example, Claude struggles with deeply-nested input especially when there are many tools available. What's next for RoboGenie It's pretty much production-ready so I plan to promote it on socials and see what happens!"
  },
  {
    "title": "Apex Moderation System",
    "link": "https://devpost.com/software/apex-moderation-system",
    "text": "Built With"
  },
  {
    "title": "The Voices",
    "link": "https://devpost.com/software/the-voices",
    "text": "Inspiration The complexity of navigating opportunities across borders due to pre-existing language barriers sparked our inspiration. Whether it’s missing out on a global business deal or struggling with personal connections when travelling, language differences often become a hurdle. We sought to create a solution to combat this challenge, and that’s how we landed on 'The Voice', a tool designed to bridge these gaps and make international communication seamless. What it does \"The Voice\" enables real-time translation by converting spoken language into text, translating it on the fly, and delivering instant voice output in the desired language. Whether you're in a business meeting or travelling abroad, the app allows users to break language barriers effortlessly, making global communication accessible for everyone. How we built it Backend: We used Python, leveraging OpenAI’s Whisper library for initial speech-to-text functionality. Recognizing its limitations in translation quality, we integrated the DeepL API to provide more accurate and natural translations. Frontend: For the UI, we used React with Tailwind CSS to create a clean, intuitive user interface that simplifies interaction. Electron: To run the app natively on the desktop and integrate voice input, we built it with Electron, making the app platform-independent. Challenges we ran into Compiling audio inputs: Capturing and processing audio from the laptop’s microphone required working with tools like Voiceflow for user interaction and managing inter-process communication. Hijacking computer audio: We faced complications in rerouting system audio for the app to capture and process it in real-time, especially integrating this within Electron’s framework. Accomplishments that we're proud of We are proud to have overcome several technical challenges, including successfully hijacking the computer's audio for real-time processing, which enabled seamless voice translation. Additionally, we designed a pleasant, intuitive UI that not only enhances the user experience but also showcases our proficiency in React and Tailwind CSS. This allowed us to create a visually appealing and functional interface, making cross-language communication both easy and enjoyable. Combining these backend and frontend accomplishments is a major highlight of our project. What we learned Beyond improving our teamwork, communication, and collaboration, we gained technical insights in: What's next for The Voices Looking ahead, we aim to expand The Voice by integrating support for all major languages, making it a truly global solution. Additionally, we plan to implement advanced text-to-speech capabilities, allowing users not only to receive translated text but also to hear the translation in real-time, further enhancing accessibility. We’re also exploring strategic partnerships with telecommunications enterprises to embed our app into their services, enabling smoother communication experiences for international calls and meetings. These future developments will make The Voice a powerful tool for breaking language barriers worldwide."
  },
  {
    "title": "MoonWalk",
    "link": "https://devpost.com/software/walkback",
    "text": "Inspiration After a brutal Achilles tear and weeks in bed, one of our teammate’s moms re-tore her Achilles after a fall from a mobility scooter, similar to our walker. Luckily, she was at home when the re-injury occurred and could get help from family. Not everyone is so lucky, and this incident got us thinking about accessibility tools such as scooters, wheelchairs, and walkers and how they could be improved to assist when no one else is around. When someone who is physically impaired falls, they are often unable or severely restricted in their ability to get back to their physical aid. Surely, it would be helpful if their walker came back for them? What it does This walker is equipped with a camera, a large motor for power, and two smaller motors to pull the brakes. Using a computer vision algorithm that we trained using YOLOv8, the walker detects when someone has fallen and raised their hand, requesting assistance. The walker then motors toward the fallen user to help them. How we built it This project combined complex hardware, mechanical design, and software components, Challenges we ran into After training our model, we found that it wasn't running as we wanted on the Raspberry Pi due to power limitations. We had to quickly pivot and upload the model to a web server, developing our own API for the Raspberry Pi to upload images and receive the necessary information. UART was really finicky to work with, after writing code that worked literally hours before, our team was disappointed to find that it not longer worked and that we had to fix it. None of our team had worked with a Raspberry Pi before, making setup and configuration quite challenging. We didn’t get all the crucial hardware we requested, but our team adapted and altered the project accordingly. Accomplishments that we're proud of Training the model in such a short time frame was a huge win, especially with the help of roboflow. Writing the UART was difficult but extremely satisfying once completed. Our team maintained a resourceful and positive attitude, overcoming every obstacle. What we learned We learned a lot of new technical skills: new Python libraries, Linux commands, CAD designs, 3D printing settings, and hardware techniques. Working with a Raspberry Pi was an eye-opening experience, especially with UART communication. Additionally, we gained valuable teamwork lessons, including how much sleep deprivation can affect problem-solving, but also the extent to which persistence can lead to solutions! What's next for Moonwalk? Who knows? We may continue working on this after the weekend. It’s too early to say on this very sleep-deprived Sunday morning..."
  },
  {
    "title": "Common Ground",
    "link": "https://devpost.com/software/lockedin-pnvb26",
    "text": "Inspiration Have you ever connected with someone over a mutual interest? Our team set out to recreate that feeling for thousands of potential users; using an algorithm to identify common interests, and harnessing the LLM capabilities of Groq to synthesize and communicate this information to users. What it does Common Ground is an easy-to-use social network graph visualization that allows you to discover your common interests with other people. You can see which mutual friends you have, which communities you are both a part of, and which groups you could join to achieve more common ground! How we built it Challenges we ran into Accomplishments that we're proud of What's next for Common Ground We look forward to scaling our systems and integrating automatic Twitter scraping, further enhancing the user experience!"
  },
  {
    "title": "Fangler",
    "link": "https://devpost.com/software/fangler",
    "text": "Inspiration Fangler was inspired by: What it does Fangler is a multi-functional, battery-powered, portable headband that includes lighting and cooling purposes. The light turns on with a simple switch, whereas the fan is automated, activating at a certain temperature. This project allows the user to enjoy their time outdoors to the fullest extent without worrying about being too hot during the dog days of summer, and being able to adapt to temperature changes. How we built it 1) Ideation: 2) Developing basis of model (Arduino-controlled fan): testing with different motors and propellers, until a suitable power one was 3) CAD software 4) Assemble each function separately, brought together at end 5) Develop code Challenges we ran into Many times we wondered about the possibility of securing all the bits to the headpiece, as well as the concern of securing the headpiece to the head. Fangler appeals to the exercise-loving, constantly moving people, and so the stability of our project is quite crucial. Accomplishments that we're proud of What we learned What's next for Fangler A more lightweight and balanced design Specialized PCBs to maximize efficiency Better fan placement/quality of materials for structural integrity and comfortability Better cable management New feature: different exercise modes & everyday life modes, fan speed varies by different factors (humidity/sweat, temperature change factor)"
  },
  {
    "title": "Go Fish",
    "link": "https://devpost.com/software/go-fish-jxs6iq",
    "text": "Welcome to Go Fish: Information in Reel Time Motivation Large language models and search engines often find non-relevant information, making them less useful in isolated environments (i.e. for individual companies). What if you could find more useful information, faster? How It Works Go Fish combines API calls to relevant platforms with Groq’s fine-tuned LLM - instead of trying to find information on the entire internet, limit your search to information pulled directly from Shopify, StackOverflow, and more - you can even display code snippets pulled directly from Github! After retrieving information via API calls, data is passed to Groq's LLM, where it filters and retrieves the information most relevant to your query. Go Fish also emphasizes important terms and ideas within your search - making your search more relevant, more accurate, and more efficient. Cool Features Code snippet and specific linedisplay during Github search,\nResults summary display,\nShopify search,\nProvides links to additional resources,\nIn-line reddit post display,\nAnd more! How It's Better Go Fish avoids some of the biggest problem with LLMs today. Many LLMs, including OpenAI (ChatGPT) both crawl the entire internet - leading to spider problems - and retrieve previously AI-generated information, leading to \"AI inbreeding\". By restricting Go Fish's information to data retrieved via API call from human sources, your information is more relevant and more accurate, without falling victim to these common problems. In addition, Go Fish can be expanded to store several of your last searches so you can quickly and easily return to previous information in air-gapped or internet-less environments. Go Fish's concept can also be applied to closed networks or information systems without global internet access for developers or employees. Uses and Applications Easy communication between software developers and those outside of programming spaces - relevant summaries, images, and linked resources give you the communication tools you need to make sure everyone is on board with your project.\nBug fixing made easy - search your Github and return code snippets and specific lines.\nNeed outside help? Interface with StackOverflow or Reddit to get answers you need - when in doubt, turn to outside experts.\nAdapt Go Fish to any search domain - here, we're demo-ing an expanded search space across Shopify, Reddit, and more, but it's easy to modify Go Fish to any interface - even local networks or employer-specific applications. How We Built It Go Fish was created using Javascript, React, and Python, in combination with the following services and platforms:\nGroq (LLM) Shopify (API)\nSlack (API)\nGithub (API)\nStackOverflow (API)\nGoDaddy (Domain) Future Applications and Considerations Go Fish can be modified and applied to individual systems, e.g. for employees to use in workspaces - retrieving only useful information, and making work more efficient. Or: to any finite domain! Go Fish is compatible with air-gapped (internet-less) practices of holding several recent searches in a queue, depending on data storage and retrieval needs, as well as required system size."
  },
  {
    "title": "crosswalk of shame...",
    "link": "https://devpost.com/software/crosswalk-of-shame",
    "text": "Inspiration This idea stemmed from our team’s interest in urban planning. As university students who mainly get around the city by walking, we’ve noticed that many people (especially students) look at their phones while crossing intersections. Although seemingly harmless, distracted walking at intersections has resulted in over 60 pedestrian fatalities per year in Canada source, on average. In the United States, texting and walking has resulted in over 10,000 injuries a year source. Furthermore, in areas of high population density, such as downtown Toronto, drivers can have unpredictable behaviour and roads can be especially congested at times. This makes distracted walking a dangerous decision, and thus we saw a need for improvement. Introducing, Crosswalk of Shame..., What it does Crosswalk of Shame... is a website that displays a gallery of pedestrian photos who cross intersections in an unsafe manner. Each photo is taken when a pedestrian looks at their phone, while also playing a sound (“get off your phone!”). The goal of this is to discourage distracted walking in a comedic manner; specifically, through public shaming. The website also displays crossing statistics data (number of crossings, number of dangerous crossings, percentage of dangerous crossings, and average time to cross for multiple time frame options). How we built it The frontend interface was developed using React and is hosted through Vercel. Convex was used for backend; specifically to create API endpoints and for data storage. PyTorch and YOLOv8 were used to achieve pedestrian recognition and to identify dangerous pedestrian crossings. Challenges we ran into Our biggest challenge was hardware integration. Originally, we planned to create a hardware system with a camera and LED light integrated with Raspberry Pi to signal dangerous pedestrian behavior. The schema was to pass the tags labeled by the YOLOv8 outputs into the Raspberry Pi OS to control the light switch. However, the hardware equipment made this task difficult. The ethernet cables purchased had unstable connections and had to be plugged in and unplugged every 2 minutes, the Pi ran into network issues, and the operating system had to be accessed through SSH instead of a nice GUI provided by the Pi. Many pieces of hardware like SD readers and cables were unavailable and had to be sourced from outside of HTN. All of these issues made the hardware integration time-consuming and resulted in the final integration being unsuccessful. Accomplishments that we're proud of With most of us having little or moderate experience in coding, a lot of learning and research was completed on the way. We are very proud to see our project through to the end with most major features functioning properly. We are proud of how well we collaborated under the time pressure of the hackathon, supporting each other throughout the process. In terms of technical accomplishments, we are very pleased with the accuracy of our pedestrian identification program. We are also proud of implementing Convex as a backend framework given that no one on the team had prior experience with it. Finally, we are happy that we were able to successfully host our website onto Vercel. What we learned Through Hack the North, we learned the importance of flexibility and problem-solving when dealing with hardware integration challenges. Despite our original plan to use Raspberry Pi for hardware control, we encountered numerous setbacks, such as unstable connections and faulty equipment, which required us to adapt quickly. This experience taught us how to pivot our focus when necessary. Additionally, we gained valuable experience using Convex for backend development, which none of us had prior knowledge of. We also learned to navigate machine learning frameworks like PyTorch and YOLOv8, enhancing our skills in pedestrian identification and computer vision applications. Finally, collaborating under tight deadlines sharpened our teamwork and communication skills. What's next for Crosswalk of Shame… Crosswalk of Shame… is not yet able to be implemented on a large scale (i.e. on actual streets). We acknowledge that the current trained CV model sometimes crashes when overflowing with data, and cannot detect dangerous crossings of pedestrians looking at their phones when they are not facing the camera. Furthermore, we initially also wanted to incorporate identifying children, the elderly, and other vulnerable populations. However, the lack of training data and the ability to train on site made us pivot to focus on a more general approach of improving pedestrian crossing safety. We also wanted to incorporate tracking pedestrian crossing lights to identify any crossings that occur when lights are red or if insufficient time is given for pedestrians to cross an intersection. Additionally, since our goal is to allow mass installment of similar systems at crossings, the Raspberry Pi electric signal component may be insufficient and should be upgraded to larger warning lights to raise the driver’s attention to pedestrians. Finally, if our system is installed at most intersections, the collected camera data would also allow city planners to analyze where high volumes of unsafe pedestrian crossings occur and thus invest in infrastructure to reduce potential accidents."
  },
  {
    "title": "Finmary report",
    "link": "https://devpost.com/software/summarcial-report",
    "text": "Project Story Inspiration The project was inspired by the increasing complexity of corporate reports and financial documents. These reports often contain valuable insights but are challenging to digest due to their length and technicality. I wanted to create a tool that could transform these dense documents into easy-to-understand summaries and structured data, enabling anyone—from analysts to casual readers—to extract useful insights without wading through hundreds of pages. I also sought to harness the power of AI to streamline and automate this process, making analysis faster and more accessible. What I Learned Through building this project, I deepened my understanding of AI, particularly OpenAI's GPT-3.5 API. I gained experience in handling PDFs programmatically using the PyMuPDF library (fitz), and working with text extraction. The process also taught me the importance of optimizing AI prompts to ensure accurate and useful responses. Additionally, I learned how to manipulate large chunks of text, ensuring the AI model could handle the data efficiently and return useful insights. Finally, structuring the output in JSON allowed me to practice organizing data for easy analysis and machine readability. How I Built It I started by identifying the core task: extracting data from PDF reports and processing it in segments, so each part could be handled efficiently by the AI. The project is built with Python and uses several key libraries: The program works by dividing a PDF into segments (in this case, 48 pages), feeding each segment into the OpenAI model, and generating a response for each segment. Once all segments are processed, the program generates a comprehensive summary based on the individual responses. Finally, the results are saved into a JSON file. Challenges I Faced One of the main challenges was managing the large amounts of text extracted from PDF reports. The AI model has token limits, so I had to carefully split the PDF into manageable segments to ensure that each prompt remained within the model’s processing capabilities. Another challenge was designing prompts that would generate accurate and detailed responses from the AI. I also encountered issues with formatting the final output into JSON, especially when responses included unexpected characters that needed to be cleaned. Additionally, dealing with varied formatting in PDFs (tables, charts, and images) presented challenges in terms of text extraction, as not all elements were easily interpreted by the AI."
  },
  {
    "title": "serve ay",
    "link": "https://devpost.com/software/serve-ay",
    "text": "Inspiration We hate surveys, you prolly do too. We made this to prove that long surveys just. don't. work. What it does tracks how attentive the participant is throughout the survey. How we built it React Challenges we ran into Building in the playback video into the end result Accomplishments that we're proud of having lots of fun at Hack the North!!!! What we learned We learned how to incorporate a video into our project What's next for serve ay big things!"
  },
  {
    "title": "Elestrals Worlde",
    "link": "https://devpost.com/software/elestrals-worlde",
    "text": "Built With"
  },
  {
    "title": "deb(AI)te",
    "link": "https://devpost.com/software/deb-ai-te",
    "text": "deb(AI)te Project for Hack the North 2024. 💡 Inspiration The idea for deb(AI)te was born from the desire to help people improve their presentation, critical thinking, and debating skills in a fun and interactive way. By integrating fun AI-driven characters with distinct personalities and expertise, users can engage in dynamic, real-time debates in a low-pressure environment. The aim is to make learning these essential skills more engaging and accessible for real-world purposes. ❓ What it does deb(AI)te allows users to participate in structured debates against AI opponents. Users choose a debate topic, and the AI, customized with different difficulty levels, responds as an expert in the field. The user gets 30 seconds to prepare an argument and then present using their speech. After the user is done, the AI will provide their argument as to why their stance is superior. Then, the system uses a rating mechanism to assess both performances and provides constructive feedback on their arguments, helping the user to improve over time. The application leverages speech-to-text functionality for seamless interaction and AWS Polly for high-quality text-to-speech responses from the AI characters. 🧰 How we built it We built deb(AI)te using JavaScript, React, and Tailwind CSS. We integrated AWS Transcribe and Polly for speech-to-text and text-to-speech functionality. The debate engine is powered by Cohere. Each of the characters' personalities and debate tactics are fed into Cohere giving the user a unique debate experience. As well the judging is also processed by Cohere giving feedback on the user's debate on three different categories - Creativity, Logic, and Flow. Cohere analyzes the data given by the user through the speech-to-text functionality to give meaningful feedback. The more confident and factual the user's speech, the higher the rating they will achieve. 🤔 Challenges we ran into One challenge was creating AI personalities that were both engaging and realistic without straying into sensitive or controversial topics. As well, balancing the AI's difficulty levels in a way that challenges the user while providing valuable learning opportunities took significant fine-tuning. 🏆 Accomplishments that we're proud of We’re proud to have created a fully functional debate platform within such a limited time period. Making each of the AI personalities was very fun and exciting as well as seeing our ideas come to fruition. We are also proud of how the UI/UX turned out including the artwork that was created to support the project. 🧠 What we learned Through this project, we learned how to effectively integrate voice-based AI functionalities, refine prompt engineering, and handle real-time user interactions in a web-based application. 👉 What's next for deb(AI)te"
  },
  {
    "title": "memora.",
    "link": "https://devpost.com/software/memora-3yr2wv",
    "text": "memora. memora is a journaling app that knows you better than yourself. It works as a voice diary, allowing you to record spoken diary entries. memora summarizes and analyzes these entries to understand your personality, traits, and values. With this info, you can ask memora any question such as \"do I overwork myself?\" or \"Am I improving at collaboration?\" and it can give you thorough answers based on your entries, allowing you to more deeply understand yourself. the inspiration. My phone contains over 2GB of voice memos, featuring countless moments of meaningful introspection, happy moments, and forgotten stories, but none of them are ever replayed once they've been recorded. In the search of figuring out a more efficient and helpful way for people to record and store voice diaries, the team realized that data could be used rather than just stored. From that thought, they came up with memora, which summarizes recordings to make them easy to look back at, while also helping people understand themselves on a deeper level. How we built it The frontend is built with React and Vite, with AntDesign for styling. The backend, built with FastAPI, handles the summarization of transcribed text using Cohere and stores it in MongoDB. Our RAG-based search engine leverages ChromaDB to store and retrieve vectorized journal entries, while Cohere is used to generate responses. what's next for memora. lots :)"
  },
  {
    "title": "GooseMeet",
    "link": "https://devpost.com/software/goosemeet",
    "text": "Inspiration 🪿 The dreaded morning scrum. Endless meetings, forgotten action items, and the constant repetition of questions that were answered yesterday or the week before. As co-op students ourselves, we understand the struggle all too well. That’s why we created a way to simplify meetings and keep everyone on track. What it does 🦆 Introducing GooseMeet, a wholesome intelligent little goose 🪿 that accompanies your team through all your meetings. As the perfect meeting companion, Mr. Goose 🪿tracks every meeting, remembers past meetings, and has the intelligence to provide insights based on past discussions. Unlike other meeting assistants which provide you with insights/data after the meeting has ended, Mr. Goose 🪿 will talk to you in real-time using state-of-the-art voice models, and help make your meetings more fruitful! He can answer any questions you may have, such as: “what was discussed in yesterday’s meeting,” “which intern accidentally pushed to main last Friday,” or even, \"help us brainstorm marketing ideas!\" Helping your team stay organized, Mr. Goose ensures no detail slips through the cracks, and that your meetings stay efficient and productive. How we built it 🔧 Challenges we ran into 🏁 Accomplishments that we're proud of 🌟 What we learned 📖 What's next for GooseMeet 📈"
  },
  {
    "title": "GOOSEAI",
    "link": "https://devpost.com/software/gooseai",
    "text": "Inspiration GOOSEAI was inspired by the need to explore how artificial intelligence can facilitate nuanced and engaging discussions between diverse perspectives. The idea stemmed from the desire to see AI not only mediate but also enhance debates, much like a conversation between two insightful individuals. What it Does GOOSEAI enables intelligent, dynamic debates between AI-driven avatars of two geese. By leveraging advanced AI, it simulates an engaging dialogue on various topics, showcasing how different viewpoints can be explored and articulated effectively. How We Built It We built GOOSEAI by integrating natural language processing models with GROQ AI’s advanced analytics capabilities. The development process involved training the AI on diverse datasets to ensure it could handle a wide range of topics and perspectives. We also designed a user-friendly interface for seamless interaction and real-time debate simulation. Challenges We Ran Into One of the main challenges was ensuring that the AI could generate coherent and contextually relevant responses throughout the debate. Balancing the conversational flow while maintaining the accuracy of the information was also difficult. Additionally, we faced challenges in fine-tuning the AI's ability to handle complex arguments and diverse viewpoints. Accomplishments That We’re Proud Of We are proud of successfully creating an AI system that can simulate meaningful debates between two geese, demonstrating the potential for AI in enhancing interactive discussions. Achieving a high level of conversational coherence and relevance in real-time interactions was a significant milestone. We are also pleased with the user-friendly design that allows easy engagement with the AI debate. What We Learned Through the development of GOOSEAI, we learned valuable lessons about the intricacies of natural language understanding and the importance of context in generating meaningful dialogue. We also gained insights into the challenges of AI moderation and the need for continuous refinement to ensure the quality of interactions. What’s Next for GOOSEAI Moving forward, we plan to expand GOOSEAI’s capabilities by incorporating more complex debate topics and improving its ability to handle nuanced arguments. We also aim to enhance the user experience with additional features and integrations, potentially exploring applications in educational and training environments to facilitate learning through dynamic discussions."
  },
  {
    "title": "Tabinator",
    "link": "https://devpost.com/software/tabulator",
    "text": "Inspiration Have you ever been working on a project and had your desktop tabs juuust right? Have you ever been playing a video game and had the perfect wiki tabs open. Have you ever then had to shutdown your computer for the night and lose all those sweet, sweet windows? Tabinator is the program for you! What it does Tabinator allows you to save and load your desktop \"layouts\"! Just open our desktop app, save your current programs as a layout, and voila! You're programs - including size, position, and program information like Chrome tabs - are saved to the cloud. Now, when you're ready to dive back into your project, simply apply your saved config, and everything will fit back into it's right place. How we built it Tabinator consists of a few primary components: Our flow is as follows: A user visits our website, signs up with Github, and is prompted to download Tabinator for their OS. Upon downloading, our Desktop app grabs your JWT token from our web-app in order to authenticate with the API. Now, when the user goes to back up their layout, they trigger the relevant script for their operating system. That script grabs the window information, converts it into standardized JSON, passes it back to our React desktop app, which sends it to our Convex database. Challenges we ran into Not only was this our first time writing Powershell, Swift, and Rust, but it was also our first time gluing all these pieces together! Although this was an unfamiliar tech stack for us all, the experience of diving head-first into it made for an incredible weekend Accomplishments that we're proud of Being able to see our application work - really work - was an incredible feeling. Unlike other project we have all worked on previously, this project has such a tactile aspect, and is so cool to watch work. Apart from the technical accomplishments, this is all our first times working with total strangers at a hackathon! Although a tad scary walking into an event with close to no idea who we were working with, we're all pretty happy to say we're leaving this weekend as friends. What's next for Tabinator Tabinator currently only supports a few programs, and is only functional on Linux distributions running X11. Our next few steps would be to diversify the apps you're able to add to Tabinator, flesh out our Linux support, and introduce a pricing plan in order to begin making a profit."
  },
  {
    "title": "Photosynthespace",
    "link": "https://devpost.com/software/photosynthispace",
    "text": "🫘 Inspiration For many of us, our most impactful work happens in the most understated spaces. Libraries, study halls, our bedrooms. That's not necessarily a bad thing, but we wanted to see blooming plants bring life and energy to these spaces-- without the real life responsibility of plant parenting. Just like the the natural process this app was named after, we realized our MR garden can bring life and energy to our goals as well if it gives us a chance to document, visualize, and celebrate our accomplishments while feeling like a game! 🌱 What it does Photosynthespace is a Mixed Reality (MR) gardening simulation that allows you to identify surfaces in a room and cultivate virtual plants on it as a tool for goal setting and progress tracking. Each plant sowed by you represents a goal or habit of your description. Then, you have the option to input your own steps or to have the plant generate a step-by-step plan. You can view these roadmaps as you select the plant and watering these plants is the equivalent of checking off a step on the list. Watch these lil guys grow as you water them more and more! Next, if you encounter a problem with completing any steps of your 'plants' (synonymous with 'plans' in our app😼), you can talk to them directly via a voice-to-text conversation and have a tailored rubber ducky plant-y to navigate your issues with. Finally, bring your headset anywhere and you can take care of your garden and your progress in any space! Meditate, cultivate, relax, and enjoy the cute visuals and music. 🥽 Why Mixed Reality? We believe in retaining the essence of your space and simply reveal the progress that's often hidden. Hopefully, it fortifies the vital skill of appreciating your achievements no matter how big or small. At the end of the day, you grew them with care, as you do with a plant. Traditionally, XR is used to transport you to a new environment, but Photosynthespace will transform your environment into a representation of you and your efforts! 🪻 How we built it Photosynthespace is built with Unity connected to a Meta Quest 3. Voiceflow interprets the user's requests and concerns to generate conversation responses and the steps to achieving a given goal. AWS 🌷 Challenges we ran into 🌻 Accomplishments that we're proud of 🪴 What we learned 🌳 What's next for Photosynthespace"
  },
  {
    "title": "IngredientAI",
    "link": "https://devpost.com/software/connections-search",
    "text": "Inspiration The idea for IngredientAI came from my personal frustration with navigating complex and often misleading ingredient labels on beauty and personal care products. I realized that many consumers struggle to understand what these ingredients actually do and whether they are safe. The lack of accessible information often leaves people in the dark about what they are using daily. I wanted to create a tool that brings transparency to this process, empowering users to make healthier and more informed choices about the products they use. What it does The idea for IngredientAI was born out of the frustration of navigating complex and often misleading ingredient labels on beauty and personal care products. I realized that many consumers struggle to understand what these ingredients actually do and whether they are safe. The lack of accessible information often leaves people in the dark about what they are using daily. I wanted to create a tool that brings transparency to this process, empowering users to make healthier and more informed choices about the products they use. How I built It The frontend is built using React Native and ran using Expo. Users interact with a FlatList component that accesses a backend database powered by Convex. Text extracted from images as well as generates ingredient descriptions is all done through OpenAI's gpt-4o-mini large-language model. Challenges I ran into A big challenge that I come across was figuring out how to extract text from images. Originally, I planned on setting up a server-side script that makes use of Tesseract.js's OCR capabilities. However, after some testing, I realized that I did not have enough time to fine tune Tesseract so that it extracts text from images under a variety of different lighting. For IngredientAI to be used by consumers, it must be able to work under a wide variety of circumstances. To solve this issue, I decided it would be best for me to use OpenAI's new Vision capabilities. I did not go with this originally because I wanted to minimize the amount of OpenAI API calls I made. However, under time constraints, this was the best option. Accomplishments that I'm proud of I am extremely proud of how far my App Development has come. At a previous hackathon in March, I had used React Native for the very first time. At that hackathon, I was completely clueless with the technology. A lot of my code was copy/pasted from ChatGPT and I did not have a proper understanding of how it worked. Now, this weekend, I was able to create a fully functional mobile application that has organized (enough) code that allows me to expand on this project in the future. What I learned Every hackathon, my goal is to learn at least one new technology. This weekend, I decided to use Convex for the very first time. I really appreciated the amount of resources that Convex provides for learning their technology. It was especially convenient that they had a dedicated page for hackathon projects. It made setting up my database extremely fast and convenient, and as we know, speed is key in a hackathon. What's next for IngredientAI My aim is to eventually bring IngredientAI to app stores. This is an app I would personally find use for, and I would like to share that with others. Future improvements and features include: I hope you all get the chance to try out IngredientAI in the near future!"
  }
]